% Jinja Template for Latex file
% Copyright François Pacaud, 2023

\documentclass{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{xcolor,bm,url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tikz}
\usepackage{cleveref}
\usepackage{xspace}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\nullspace}{Ker}
\DeclareMathOperator{\rangespace}{range}
\newcommand{\ldlt}{$\mathrm{LDL^T}$\xspace}
\newcommand{\lblt}{$\mathrm{LBL^T}$\xspace}
\newcommand{\llt}{$\mathrm{LL^T}$\xspace}
\newcommand{\cond}{\kappa_2}
\newcommand{\epstol}{\mathbf{u}}
\newcommand{\cactive}{\mathcal{B}}
\newcommand{\cinactive}{\mathcal{N}}

% \title{Nonlinear programming on GPU: current state-of-the-art}
\title{Approaches to nonlinear programming on GPU architectures}
% \title{Approaches to nonlinear optimization on GPU architectures} ?
\author{François Pacaud \and
Sungho Shin \and
Alexis Montoison \and
Michel Schanen \and
Mihai Anitescu
}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
  Solving nonlinear optimization problems requires the successive solutions of large symmetric indefinite linear systems, a task known to pose computational challenges.
  In particular, sparse factorization of such matrices relies on expensive numerical pivoting operations, which are difficult to parallelize on GPUs.
  A workaround is to reformulate the successive Karush-Kuhn-Tucker systems by condensing it into a symmetric positive-definite system suitable for an efficient Cholesky factorization on GPU.
  In this paper, we analyze the performance of two different condensed-space methods within an interior-point solver.
  We prove that the condensed matrices exhibit structured ill-conditioning, limiting the loss of accuracy when computing a descent direction.
  We implement the two methods on GPU, using the NVIDIA library cuDSS for solving sparse linear systems.
  We illustrate their strengths and weaknesses on large-scale instances taken from the PGLIB and the COPS benchmarks.
  We find that GPUs excel at computing results rapidly down to a medium convergence tolerance, showcasing a 10x speed-up compared to a state-of-the-art CPU version, yet demonstrating limited robustness.
\end{abstract}


% \tableofcontents


\input{sections/introduction.tex}
\input{sections/ipm.tex}
\input{sections/kkt_systems.tex}
\input{sections/conditioning.tex}
\input{sections/numerics.tex}

\section{Conclusion}
This article moves one step further in the solution of generic nonlinear
programs on GPU architectures. We have compared two approaches
to solve the KKT systems arising at each interior-point iteration, both
based on a condensation procedure.
Despite the formation of an ill-conditioned matrix, our theoretical analysis shows that the loss of accuracy is benign in floating-point arithmetic, thanks to the specific properties of the interior-point method.
Our numerical results show that both methods are competitive to solve large-scale
nonlinear programs.
Compared to the state-of-the-art HSL linear solvers, we achieve a 10x speed-up on large-scale OPF instances and quasi-dense instances (\texttt{elec}). While the results are more varied across the instances of the COPS benchmark, our performance consistently remains competitive with HSL.

Looking ahead, our future plans involve enhancing the robustness of the two condensed KKT methods, particularly focusing on stabilizing convergence for small tolerances (below $10^{-8}$).
It's worth noting that the sparse Cholesky solver can be further customized to meet the specific requirements of the interior-point method~\cite{wright1999modified}.
% Add a sentence about NCL on GPU?
Enhancing the two methods on the GPU would enable the resolution of large-scale problems that are currently intractable on classical CPU architectures.
Examples include multiperiod and security-constrained OPF problems, particularly when combined with a Schur-complement approach.

\small
\bibliographystyle{siam}
\bibliography{biblio.bib}
\normalsize

\end{document}
