% Jinja Template for Latex file
% Copyright François Pacaud, 2023

\documentclass{article}

\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{xcolor,bm,url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tikz}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\nullspace}{null}
\DeclareMathOperator{\rangespace}{range}
\newcommand{\cond}{\kappa_2}
\newcommand{\epstol}{\mathbf{u}}
\newcommand{\cactive}{\mathcal{B}}
\newcommand{\cinactive}{\mathcal{N}}

\title{Nonlinear programming on GPU: current state-of-the-art}
\author{François Pacaud \and
Sungho Shin \and
Alexis Montoison \and
Michel Schanen \and
Mihai Anitescu
}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
  Solving nonlinear programs requires the
  successive solutions of large-scale indefinite symmetric matrices
  Sparse factorization routines rely
  on expensive numerical pivoting operations
  which are notoriously hard to parallelize on a GPU.
  A workaround is to reformulate the successive Karush-Kuhn-Tucker (KKT) systems
  in a form more amenable for the GPU,
  by reducing it down to a symmetric positive-definite condensed
  system whose factorization using Cholesky runs efficiently on the GPU.
  In this paper, we analyze the performance of two different condensed-space methods
  when used inside an interior-point solver.
  We prove that the condensed matrices exhibit structured ill-conditioning,
  limiting the loss of accuracy when computing a descent direction.
  We implement the two methods on the GPU, using the newly
  released cUDSS solver. We illustrate their strengths and weaknesses
  on large-scale instances taken from the PGLIB and the COPS benchmarks.
  We find that GPUs shine to compute quick and dirty results down to a medium convergence tolerance
  --- with a 10x speed-up compared to a state-of-the-art CPU routine --- but
  with a limited robustness.
\end{abstract}


% \tableofcontents


\input{sections/introduction.tex}
\input{sections/ipm.tex}
\input{sections/kkt_systems.tex}
\input{sections/conditioning.tex}
\input{sections/numerics.tex}


\section{Conclusion}
Future extensions:
\begin{itemize}
  \item Interplay with first-order method (PDHG for LP)
  \item Solving LP and QP problems on the GPU with IPM
  \item From Cholesky to LDL (regularization can
    ensure the KKT matrix is SQD, c.f. Orban, Saunders).
\end{itemize}

\bibliographystyle{siam}
\bibliography{biblio.bib}


% \appendix
% \input{sections/argos.tex}

\end{document}
