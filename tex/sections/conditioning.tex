\section{Conditioning of the condensed KKT system}
The condensed matrix $K$ appearing in \eqref{eq:kkt:condensed} is known to be ill-conditioned close to a local optimum solution.
This behavior is amplified for the matrices $K_\gamma$ (for large $\gamma$) and $K_\tau$ (for small $\tau$).
For a generic linear system $Mx = b$, the relative error after perturbing the right-hand side by $\Delta b$ is bounded by
\begin{subequations}
  \label{eq:cond:defaultbond}
\begin{equation}
  \| \Delta x \| \leq \| M^{-1} \| \| \Delta b \| \;, \quad
  \frac{\| \Delta x \| }{\| x \|} \leq \cond(M) \, \frac{\| \Delta b \|}{\|b \|} \;.
\end{equation}
When the matrix is perturbed by $\Delta M$, the perturbed solution
$\widehat{x}$ satisfies $\Delta x = \widehat{x}- x =  - (M + \Delta M)^{-1} \Delta M \widehat{x}$.
If $\cond(M) \approx \cond(M + \Delta M)$, we have $M \Delta x \approx -\Delta M x$ (neglecting second-order terms),
giving the bounds
\begin{equation}
  \| \Delta x \| \leq \|M^{-1}\| \|\Delta M \| \|x \| \; , \quad
  \frac{\| \Delta x \|}{\|x\|} \leq \cond(M)\frac{\|\Delta M \|}{\|M\|} \; .
\end{equation}
\end{subequations}
The relative errors are bounded above by the conditioning $\cond(M)$,
indicating that without further assumptions the condensed matrix $K$ amplifies
the errors made throughout the algorithm.
Hence, it is legitimate to investigate the impact of the ill-conditioning
when solving the condensed system~\eqref{eq:kkt:condensed} with LiftedKKT or with HyKKT.
We will see that we can tighten the bounds in \eqref{eq:cond:defaultbond}
by exploiting the structured ill-conditioning of the condensed matrix $K$.
We base our analysis on \cite{wright1998ill}, where
the author has put a particular emphasis on the condensed KKT
system~\eqref{eq:kkt:condensed}, but without equality constraints. We generalize her results to the
matrix $K_\gamma$, which incorporates both equality and inequality
constraints. The results extend directly to $K_\tau$ (by setting $\gamma = 0$).

To alleviate the notations, we suppose that the primal variable
$x$ is unconstrained, leaving only the slack $s$ with bounded values in \eqref{eq:slack_problem}.
This is equivalent to include the bounds on the variable $x$ in the inequality constraints,
as $\bar{h}(x) \leq 0$ with $\bar{h}(x) := (h(x), -x)$.

\subsection{Centrality conditions}
We start the discussion by recalling important results porting
on the iterates of the interior-point algorithm~\cite{wright2001effects}.
Let $p = (x, s, y, z)$ the current primal-dual iterate (the multiplier $v$ is considered apart),
and $p^\star$ a solution of the KKT conditions~\eqref{eq:kktconditions}.
We denote $\delta(p, v) = \| (p, v) - (p^\star, v^\star) \|$ the Euclidean distance to the
primal-dual stationary point $p^\star$.
From \cite[Theorem 2.2]{wright2001effects}, if Assumption~\ref{hyp:ipm}
hold at $p^\star$ and $v^\star > 0$,
\begin{equation}
  \delta(p, v) = \Theta\left( \left\Vert \begin{bmatrix}
      \nabla_p L(p, v) \\ \min(v, s)
  \end{bmatrix}
  \right\Vert \right) \; .
\end{equation}
For $(s, v) > 0$, we define the \emph{duality measure} $\Xi(s, v)$ as the mapping
\begin{equation}
  \Xi(s, v) = s^\top v / m_i \; . % Sungho: m_i not defined here. 
\end{equation}
We suppose the iterates $(p, v)$ satisfy the \emph{centrality conditions}
\begin{subequations}
  \label{eq:centralitycond}
  \begin{align}
    & \| \nabla_p \mathcal{L}(p, v) \| \leq C \; \Xi(s, v) \;,  \\
    \label{eq:centralitycond:complement}
    & (s, v) > 0 \;,\quad s_i v_i \geq \alpha \, \Xi(s, v) \quad \forall i =1, \cdots, m_i \; , % Sungho: is this condition correct? Shouldn't s_iv_i be lower-bounded by a constant?
  \end{align}
\end{subequations}
for some constant $C > 0$ and $\alpha \in (0, 1)$.
Conditions~\eqref{eq:centralitycond:complement} ensures that the products
$s_i v_i$ are not too disparate in the diagonal term $D_s$.
This condition is satisfied in the solver Ipopt (see \cite[Equation (16)]{wachter2006implementation}).
We denote $\cactive = \cactive(x^\star)$ the active-set at the optimal solution $x^\star$,
and $\cinactive = \cinactive(x^\star)$ the inactive set.

\begin{proposition}[\cite{wright2001effects}, Lemma 3.2]
  \label{prop:cond:boundslack}
  Suppose $p^\star$ satisfies Assumption~\ref{hyp:ipm}.
  If the current primal-dual iterate $(p, v)$ satisfies the centrality
  conditions~\eqref{eq:centralitycond}, then
  \begin{subequations}
    \begin{align}
      i \in \mathcal{B} \implies s_i = \Theta(\Xi) \, , \quad v_i = \Theta(1) \;, \\
      i \in \mathcal{N} \implies s_i = \Theta(1) \, , \quad v_i = \Theta(\Xi) \; .
    \end{align}
  \end{subequations}
\end{proposition}
Using the centrality conditions \eqref{eq:centralitycond}, we can bound
the distance to the solution $\delta(p, v)$ with the duality measure $\Xi$.
\begin{theorem}[\cite{wright2001effects}, Theorem 3.3]
  Suppose $p^\star$ satisfies Assumption~\ref{hyp:ipm}.
  If the current primal-dual iterate $(p, v)$ satisfies the centrality
  conditions~\eqref{eq:centralitycond}, then
  \begin{equation}
    \delta(p, v) = O(\Xi) \; .
  \end{equation}
\end{theorem}

\subsection{Structured ill-conditioning}
We show that if the iterates $(p, v)$ satisfy
the centrality conditions~\eqref{eq:centralitycond}, then the
condensed matrix $K_\gamma$ has a structured ill-conditioning.

\subsubsection{Invariant subspaces in $K_\gamma$}
First, we follow the analysis presented in \cite{wright1998ill}
and show that the condensed matrix $K_\gamma$ can be decomposed as
\begin{equation}
  \label{eq:cond:svd}
  K_\gamma = \begin{bmatrix} U_L & U_S \end{bmatrix}
  \begin{bmatrix}
    \Sigma_L & 0 \\ 0 & \Sigma_S
  \end{bmatrix}
  \begin{bmatrix}
    U_L^\top \\ U_S^\top
  \end{bmatrix}
  \; ,
\end{equation}
with $U$ an orthogonal matrix and $\Sigma$ diagonal, where
the two diagonal matrices $\Sigma_L$ and $\Sigma_S$ are both better conditioned than $K_\gamma$.
The result is a consequence of the SVD decomposition,
provided that the singular values satisfy $\frac{\sigma_1}{\sigma_{p}} \leq \frac{\sigma_1}{\sigma_n}$
and $\frac{\sigma_{p+1}}{\sigma_{n}} \leq \frac{\sigma_1}{\sigma_n}$.

We recall that $K_\gamma = W + H^\top D_s H + \gamma G^\top G$, with
the diagonal matrix $D_s = S^{-1} V$.
In addition, we denote $m_a$ the cardinal of $\mathcal{B}$,
and $p := m_a + m_e$. We denote by
$H_{\cactive}$ the Jacobian of active inequality constraints, $H_{\cinactive}$ the
Jacobian of inactive inequality constraints and
$A := \begin{bmatrix} G^\top & H_{\cactive}^\top \end{bmatrix}^\top$ the active Jacobian.
We define the minimum and maximum active slack values as
\begin{equation}
  s_{min} = \min_{i \in \cactive} s_i \; , \quad
  s_{max} = \max_{i \in \cactive} s_i \; .
\end{equation}

To establish the decomposition of $K_\gamma$ as presented in \eqref{eq:cond:svd},
we modify the approach outlined in \cite[Theorem 3.2]{wright1998ill} to account for the additional
term $\gamma G^\top G$ arising from the equality constraints.
\begin{theorem}[Properties of $K_\gamma$]
  \label{thm:cond}
  Suppose the condensed matrix is evaluated at a primal-dual
  point $(p, \nu)$ satisfying~\eqref{eq:centralitycond},
  for sufficiently small $\Xi$.
  Let $\lambda_1, \cdots, \lambda_n$ be the $n$ eigenvalues of
  $K_\gamma$, ordered as $|\lambda_1| \geq  \cdots \geq |\lambda_n|$.
  Let $\begin{bmatrix} Y & Z \end{bmatrix}$ be an orthogonal
  matrix, where the columns of $Z$ span the null-space of
  $A$. Let $\underline{\sigma} =\min\{\frac{1}{\Xi}, \gamma\}$
  and $\overline{\sigma} = \max\{\frac{1}{s_{min}}, \gamma\}$.
  Then,
  \begin{enumerate}
    \item[(i)] The $p$ largest-magnitude eigenvalues of $K_\gamma$ are positive,
      with $\lambda_1 = \Theta(\overline{\sigma})$ and $\lambda_p = \Omega(\underline{\sigma})$.
    \item[(ii)] The $n-p$ smallest-magnitude eigenvalues of $K_\gamma$
      are $\Theta(1)$.
    \item[(iii)] If $0 < p < n$, then $\cond(K_\gamma) = \Theta(\overline{\sigma})$.
    \item[(iv)] There are orthonormal matrices $\widetilde{Y}$ and $\widetilde{Z}$ for
      simple invariant subspaces of $K_\gamma$, such that $Y - \widetilde{Y} = O(\underline{\sigma}^{-1})$
      and $Z - \widetilde{Z} = O(\underline{\sigma}^{-1})$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We start the proof by setting apart the inactive constraints from the active constraints in $K_\gamma$:
  \begin{equation}
    K_\gamma = W + H_{\cinactive}^\top S_{\cinactive}^{-1} V_{\cinactive} H_{\cinactive}
    + A^\top D_\gamma A \, ,
    \quad
    \text{with} \quad D_\gamma = \begin{bmatrix} S_{\cactive}^{-1} V_{\cactive} & 0 \\ 0 & \gamma I \end{bmatrix} \; .
  \end{equation}
  Using Assumption~\ref{hyp:ipm}, Lipschitz
  continuity implies that the Hessian and the inactive Jacobian
  are bounded: $W = O(1)$, $H_{\cinactive} = O(1)$.
  Proposition~\ref{prop:cond:boundslack} implies that
  $s_{\cinactive} = \Theta(1)$ and $v_{\cinactive} = \Theta(\Xi)$. We deduce:
  \begin{equation}
    \label{eq:cond:inactiveblock}
    H_{\cinactive}^\top S_{\cinactive}^{-1} V_{\cinactive} H_{\cinactive} = O(\Xi) \; .
  \end{equation}
  Hence, for small enough $\Xi$,
  the condensed matrix $K_\gamma$ is dominated by the block of active constraints:
  \begin{equation}
    K_\gamma = A^\top D_\gamma A + O(1) \; .
  \end{equation}
  Sufficiently close to the optimum $p^\star$, the constraints qualification
  in Assumption~\ref{hyp:ipm} implies that $A = \Theta(1)$ and has rank $p$.
  The eigenvalues $\{\eta_i\}_{i =1,\cdots,n}$ of $A^\top D_\gamma A$
  satisfy $\eta_i > 0$ for $i = 1,\cdots,p$ and $\eta_i = 0$ for $i = p+1, \cdots, n$.
  As $s_{\cactive} = \Theta(\Xi)$ and $v_{\cactive} = \Theta(1)$
  (Proposition~\ref{prop:cond:boundslack}), the smallest diagonal
  element in $D_\gamma$ is $\Omega(\min\{\frac{1}{\Xi}, \gamma\})$
  and the largest diagonal element is $\Theta(\max\{\frac{1}{s_{min}}, \gamma\})$.
  Hence,
  \begin{equation}
    \eta_1 = \Theta(\overline{\sigma}) \; , \quad
    \eta_p = \Omega(\underline{\sigma}) \; .
  \end{equation}
  Using \cite[Lemma 3.1]{wright1998ill}, we deduce $\lambda_1 = \Theta(\overline{\sigma})$
  and $\lambda_p = \Omega(\underline{\sigma})$, proving the first result (i).

  Let $L_\gamma = A^\top D_\gamma A$.
  We have that
  \begin{equation}
    \begin{bmatrix}
      Z^\top \\ Y^\top
    \end{bmatrix}
    L_\gamma \begin{bmatrix}Z & Y \end{bmatrix}
    = \begin{bmatrix}
      L_1 & 0 \\
      0 & L_2
    \end{bmatrix} \; ,
  \end{equation}
  with $L_1 = 0$ and $L_2 = Y^\top L_\gamma Y$.
  The smallest eigenvalue of $L_2$ is $\Omega(\underline{\sigma})$
  and the matrix $E := K_\gamma - L_\gamma$ is $O(1)$.
  By applying \cite[Theorem 3.1, (ii)]{wright1998ill},
  the $n - p$ smallest eigenvalues in $K_\gamma$ differ by
  $\Omega(\underline{\sigma}^{-1})$ from those of the reduced Hessian $Z^\top K_\gamma Z$.
  In addition, \eqref{eq:cond:inactiveblock} implies
  that $Z^\top K_\gamma Z - Z^\top W Z = O(\Xi)$. Using SOSC,
  $Z^\top W Z$ is positive definite for small enough $\Xi$, implying
  all its eigenvalues are $\Theta(1)$. Using again \cite[Lemma 3.1]{wright1998ill},
  we get that the $n-p$ smallest eigenvalues in $K_\gamma$ are $\Theta(1)$,
  proving (ii). The third point (iii) is proved by combining
  (i) and (ii) (provided $0 < p < n$).
  Eventually, point (iv) is proven by using \cite[Theorem 3.1 (i)]{wright1998ill}.
\end{proof}

\begin{corollary}
  The condensed matrix $K_\gamma$ can be decomposed as
  \eqref{eq:cond:svd}, with $\Sigma_L \in \mathbb{R}^{p \times p}$ and $\Sigma_S \in \mathbb{R}^{(n-p) \times (n-p)}$.
  In addition, for suitably chosen $Y$ and $Z$,
  \begin{equation}
    \label{eq:cond:invariantsubpsace}
    U_L - Y = O(\underline{\sigma}^{-1}) \; , \quad
    U_S - Z = O(\underline{\sigma}^{-1}) \; .
  \end{equation}
\end{corollary}
\begin{proof}
  According to Theorem~\ref{thm:cond}, the $p$ largest eigenvalues of $K_\gamma$ are large
  and well separated from the $n - p$ smallest eigenvalues. Using
  the spectral theorem, we obtain the decomposition as \eqref{eq:cond:svd}.
  Using Theorem \ref{thm:cond}, part (iv), we obtain the result
  in \eqref{eq:cond:invariantsubpsace}.
\end{proof}
Theorem~\ref{thm:cond} gives us a deeper insight into the structure
of the condensed matrix $K_\gamma$.
Using equation~\eqref{eq:cond:invariantsubpsace}, we observe
we can assimilate the large-space of $K_\gamma$ with $\rangespace(A^\top)$
and the small space with $\nullspace(A)$.
The decomposition~\eqref{eq:cond:svd} leads to the following relations
\begin{equation}
  \label{eq:cond:boundinvariantsubspace}
  \begin{aligned}
    & \| K_\gamma \| = \| \Sigma_L \| = \Theta(\overline{\sigma}) \; , &
    \Sigma_L^{-1} = O(\underline{\sigma}^{-1})  \;, \\
    & \| K_\gamma^{-1} \| = \| \Sigma_S^{-1} \| = \Theta(1) \, , &
  \Sigma_S = \Theta(1) \, .
  \end{aligned}
\end{equation}
The condition of $\Sigma_L$ depends on $\cond(A)$, $\cond(V)$
and the ratio $\frac{s_{max}}{s_{min}} = O(\Xi \overline{\sigma})$.
The condition of $\Sigma_S$ reflects the condition of the reduced Hessian $Z^\top W Z$.

\begin{remark}
  Theorem~\ref{thm:cond} (iii) tells us that $\cond(K_\gamma) = \Theta(\overline{\sigma})$,
  meaning that if $\gamma$ is large-enough to satisfy $\gamma \geq \frac{1}{s_{min}}$, then
  the conditioning $\cond(K_\gamma)$ increases linearly with $\gamma$. In
  other words, the value $\frac{1}{s_{min}}$ gives us a lower-bound for $\gamma_{max}$
  in Proposition~\ref{prop:kkt:hykkt:cond}.
\end{remark}

\subsubsection{Numerical accuracy of the condensed matrix $K_\gamma$}
We are now interested in bounding the error made in
the computation of the condensed matrix $K_\gamma$ in floating
point arithmetic.

In direct contrast with \cite[Section 4.1]{wright1998ill}, the slack variable
$s$ appearing in the diagonal matrix $S^{-1} V$ is not subject to cancellation:
the interior-point algorithm is passing exactly the value of the slack
variables to the linear solver (this would not be the case if the slack
$s_i$ was replaced by the nonlinear function $h_i(x)$). Hence,
the error arising from the multiplication by $S^{-1}$ is only $O(\epstol)$,
which is significantly better than the error in $O(\epstol / s_{min})$ we would obtain
in the cancellation case.

In floating-point arithmetic, the condensed matrix $K_\gamma$ is evaluated as
\begin{multline*}
  \widehat{K}_\gamma = W + E + (A + \Delta A)^\top (D_\gamma + \Delta D_\gamma) (A + \Delta A) \\
  + (H_{\cinactive} + \Delta H_{\cinactive})^\top S_{\cinactive}^{-1} V_{\cinactive} (H_{\cinactive} + \Delta H_{\cinactive}) \; ,
\end{multline*}
with $\| \Delta H_{\cinactive} \| = O(\epstol \| H_{\cinactive}\|)$, $\| \Delta A \| = O(\epstol \| A\|)$
and $E$ symmetric with $\|E \| = O(\epstol \overline{\sigma})$.
We have
\begin{equation}
A^\top D_\gamma A = \Theta(\overline{\sigma}) \; , \quad
A^\top \Delta D_\gamma A = O(\epstol \overline{\sigma}) \; .
\end{equation}
We deduce that the perturbation $\Delta K_\gamma$ is bounded by $O(\epstol \overline{\sigma})$.
\begin{proposition}
  \label{prop:cond:boundcondensedmatrix}
  In floating-point arithmetic, the perturbation
  of the condensed matrix $\Delta K_\gamma$ satisfies
  $\Delta K_\gamma := \widehat{K_\gamma} - K_\gamma  = O(\epstol \overline{\sigma})$.
\end{proposition}
The perturbation $\Delta K_\gamma$ displays no specific structure.
Hence, we should determine under which
conditions the perturbed matrix $\widehat{K}_\gamma$
keeps the structure highlighted in \eqref{eq:cond:svd}.
We know that the smallest eigenvalue $\eta_p$ of $A^\top D_\gamma A$
is $\Omega(\underline{\sigma})$. As mentioned in
\cite[Section 3.4.2]{wright1998ill}, the perturbed matrix
$\widehat{K}_\gamma$ has $p$ large eigenvalues
bounded below by $\underline{\sigma}$ if the perturbation
is much smaller than the eigenvalue $\eta_p$:
\begin{equation}
  \label{eq:cond:perturbationbound}
  \| \Delta K_\gamma \| \ll \eta_p  \; .
\end{equation}
However, the bound in Proposition~\ref{prop:cond:boundcondensedmatrix} is too loose
for \eqref{eq:cond:perturbationbound} to hold without any further assumption.
As $\eta_p = \Omega(\underline{\sigma})$, we deduce $\frac{\|K_\gamma \|}{\eta_p}
= O(\overline{\sigma}\underline{\sigma}^{-1})$.
If we suppose in addition the ratio $\overline{\sigma}\underline{\sigma}^{-1}$ is close to $1$,
then $\|\Delta K_\gamma\| = O(\epstol \overline{\sigma})$ can be replaced by
$\| \Delta K_\gamma\|= O(\epstol \underline{\sigma})$ which in turns implies \eqref{eq:cond:perturbationbound}.

\subsubsection{Numerical solution of the condensed system}
We are interested in estimating the relative error
made when solving the system $K_\gamma x = b$ in floating
point arithmetic. We suppose $K_\gamma$ is factorized using
a backward-stable Cholesky decomposition. The computed
solution $\widehat{x}$ is solution of a perturbed system
$\widetilde{K}_\gamma \widehat{x} = b$, with $\widetilde{K}_\gamma
= K_\gamma + \Delta_s K_\gamma$ and $\Delta_s K_\gamma$ a symmetric matrix satisfying,
\begin{equation}
  \label{eq:cond:backwardstable}
  \|\Delta_s K_\gamma\| \leq \epstol \varepsilon_n \|K_\gamma\| \;,
\end{equation}
for $\epsilon_n$ a small constant depending on the dimension $n$.
We need the following additional assumptions to
ensure (a) the Cholesky factorization runs to completion
and (b) we can incorporate the backward-stable perturbation $\Delta_s K_\gamma$
in the generic perturbation $\Delta K_\gamma$ introduced in
Proposition~\ref{prop:cond:boundcondensedmatrix}.
\begin{assumption} Let $(p, v)$ be the current primal-dual iterate. We assume:
  \begin{itemize}
    \item[(a)] $(p, v)$ satisfies the centrality conditions~\eqref{eq:centralitycond}.
    \item[(c)] The parameter $\gamma$ satisfies $\gamma = \Theta(\Xi^{-1})$.
    \item[(c)] The duality measure is large enough relative to the precision $\epstol$: $\epstol \ll \Xi$.
    \item[(d)] The primal step $\widehat{x}$ is computed using a backward
      stable method satisfying \eqref{eq:cond:backwardstable} for a small constant
      $\varepsilon_n$.
  \end{itemize}
  \label{hyp:cond:wellcond}
\end{assumption}
Condition (a) implies that
$s_{min} = \Theta(\Xi)$ and $s_{max} = \Theta(\Xi)$ (Proposition \ref{prop:cond:boundslack}).
Condition (b) supposes in addition $\gamma = \Theta(\Xi^{-1})$, making
the matrix $\Sigma_L$ well-conditioned with
$\underline{\sigma} = \Theta(\Xi^{-1})$,
$\overline{\sigma} = \Theta(\Xi^{-1})$ and $\overline{\sigma}/\underline{\sigma} = \Theta(1)$.
Condition (c) ensures that the conditioning of $\cond(K_\gamma) = \Theta(\overline{\sigma})$
satisfies $\cond(K_\gamma) \epstol \ll 1$,
ensuring the Cholesky factorization runs to completion.
Condition (d) tells us that the perturbation caused by the Cholesky
factorization is $\Delta_s K_\gamma = O(\epstol \| K_\gamma\|)$. As
\eqref{eq:cond:boundinvariantsubspace} implies $\|K_\gamma \| = \Theta(\Xi^{-1})$,
we can incorporate $\Delta_s K_\gamma$ in the perturbation
$\Delta K_\gamma$ given in Proposition~\ref{prop:cond:boundcondensedmatrix}.

We denote $x$ the solution of the linear system $K_\gamma x = b$
in exact arithmetic, and $\widehat{x}$ the solution of
the perturbed system $\widehat{K}_\gamma \widehat{x} = \widehat{b}$
in floating-point arithmetic. We are interested in bounding
the error $\Delta x = \widehat{x} - x$. We
recall that every vector $x \in \mathbb{R}^n$ decomposes as
\begin{equation}
  x = U_L x_L + U_S x_S = Y x_Y + Z x_Z \; .
\end{equation}

\paragraph{Impact of right-hand-side perturbation.}
Using \eqref{eq:cond:svd}, the inverse of
$K_\gamma$ satisfies
\begin{equation}
  \label{eq:cond:inversecondensed}
  K_\gamma^{-1}  = \begin{bmatrix} U_L & U_S \end{bmatrix}
  \begin{bmatrix}
    \Sigma_L^{-1} & 0 \\ 0 & \Sigma_S^{-1}
  \end{bmatrix}
  \begin{bmatrix}
    U_L^\top \\ U_S^\top
  \end{bmatrix}
  \; .
\end{equation}
Hence, if we solve the system for $\widehat{b} := b + \Delta b$,
$\Delta x = K_\gamma^{-1} \Delta b$ decomposes as
\begin{equation}
  \begin{bmatrix}
    \Delta x_L \\ \Delta x_S
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Sigma_L^{-1} & 0 \\ 0 & \Sigma_S^{-1}
  \end{bmatrix}
  \begin{bmatrix}
    \Delta b_L \\ \Delta b_S
  \end{bmatrix}
  \; ,
\end{equation}
which in turn implies the following bounds:
\begin{equation}
  \label{eq:cond:rhserror}
     \| \Delta x_L \| \leq \| \Sigma_L^{-1} \| \| \Delta b_L \| \; ,\quad
    \| \Delta x_S \| \leq \| \Sigma_S^{-1} \| \| \Delta b_S \| \; .
\end{equation}
As $\Sigma_L^{-1} = O(\Xi)$ and $\Sigma_S^{-1} = \Theta(1)$,
we deduce that the error $\Delta x_L$ is smaller by a factor
of $\Xi$ than the error $\Delta x_S$. The total error
$\Delta x = U_L \Delta x_L + U_S \Delta x_S$ is bounded by
\begin{equation}
  \label{eq:cond:rhserrorfull}
  \| \Delta x \|
  \leq  \| \Sigma_L^{-1} \| \| \Delta b_L \| + \| \Sigma_S^{-1} \| \| \Delta b_S \| =
  O(\|\Delta b \|) \; .
\end{equation}

\paragraph{Impact of matrix perturbation.}
As $\|\Delta K_\gamma\| \ll \|K_\gamma\|$, we have that
\begin{equation}
  \label{eq:cond:invperturbed}
  \begin{aligned}
    (K_\gamma + \Delta K_\gamma)^{-1} &= (I + K_\gamma^{-1} \Delta K_\gamma)^{-1} K_\gamma^{-1} \\
                                      &= K_\gamma^{-1} - K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1} + O(\|\Delta K_\gamma\|^2) \; .
  \end{aligned}
\end{equation}
Using \eqref{eq:cond:inversecondensed} and noting $\Delta K_\gamma = \begin{bmatrix}
  \Gamma_L \\ \Gamma _S
\end{bmatrix}$, the first-order error is given by
\begin{equation}
  \label{eq:cond:inversecondensederror}
  K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1}  =
U_L \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}U_L^\top  +
  U_S \Sigma_S^{-1} \Gamma_S \Sigma_S^{-1}U_S^\top  \;.
\end{equation}
Using \eqref{eq:cond:perturbationbound} and $(\Gamma_L, \Gamma_S)= O( \Xi^{-1}\epstol)$,
we deduce that the error made in the large-space is $O(\Xi\epstol)$ whereas
the error in the small-space is $O(\Xi^{-1}\epstol )$.

\subsection{Solution of the condensed KKT system}
We use the relations~\eqref{eq:cond:rhserror} and \eqref{eq:cond:inversecondensederror}
to bound the error made when solving the condensed KKT system~\eqref{eq:kkt:condensed}
in floating-point arithmetic.
In all this section, we assume that
the primal-dual iterate $(p,v)$ satisfies Assumption~\ref{hyp:cond:wellcond}.
Using \cite[Corollary 3.3]{wright2001effects}, the solution $(d_x, d_y)$ of the
condensed KKT system \eqref{eq:kkt:condensed} in exact arithmetic satisfies
$(d_x, d_y) = O(\Xi)$.
In \eqref{eq:kkt:condensed}, the RHS $\bar{r}_1$ and $\bar{r}_2$
evaluate in floating-point arithmetic as
\begin{equation}
  \label{eq:cond:condensedrhs}
  \left\{
  \begin{aligned}
    \bar{r}_1 &= - \widehat{r}_1 + \widehat{H}^\top\big(\widehat{D}_{s} \widehat{r}_{4} - \widehat{r}_{2} \big) \;, \\
     \bar{r}_2 &= -\widehat{r}_3 \; .
  \end{aligned}
  \right.
\end{equation}
Using basic floating-point arithmetic, we get
$\widehat{r}_1 = r_1 + O(\epstol)$,
$\widehat{r}_3 = r_3 + O(\epstol)$,
$\widehat{r}_4 = r_4 + O(\epstol)$.
The error in the right-hand-side $r_2$ is impacted by the term $\mu S^{-1}e$:
under Assumption~\ref{hyp:cond:wellcond}, it impacts differently
the active and inactive components:
$\widehat{r}_{2,\cactive}= r_{2,\cactive} + O(\epstol)$ and
$\widehat{r}_{2,\cinactive}= r_{2,\cinactive} + O(\Xi \epstol)$.
Similarly, the diagonal matrix $\widehat{D}_s$ retains full accuracy only
w.r.t. the inactive components: $\widehat{D}_{s,\cactive} = D_{s,\cactive} + O(\Xi^{-1} \epstol)$
and $\widehat{D}_{s,\cinactive} = D_{s,\cinactive} + O(\Xi \epstol)$.

\subsubsection{Solution with HyKKT}
We analyze the accuracy achieved when we solve the condensed system~\eqref{eq:kkt:condensed}
using HyKKT,
and show that the error remains reasonable even for large values of
the regularization parameter $\gamma$.

\paragraph{Initial right-hand-side.}
In floating-point arithmetic, the initial right-hand side in \eqref{eq:kkt:schurcomplhykkt}
is evaluated as
$\widehat{r}_\gamma :=\widehat{G} \widehat{K}_\gamma^{-1} (\bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2) - \bar{r}_2$. Using \eqref{eq:cond:condensedrhs}, we have
\begin{equation}
  \label{eq:cond:boundderivationhykkt}
  \begin{aligned}
  \bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2 &=
- \widehat{r}_1 + \gamma \widehat{G}^\top \widehat{r}_3+ \widehat{H}^\top\big(\widehat{D}_{s} \widehat{r}_{4} - \widehat{r}_{2} \big) \\
&=  -
\underbrace{\widehat{r}_1}_{O(\epstol)} +
\underbrace{
\widehat{H}_{\cinactive}^\top\big(\widehat{D}_{s,\cinactive} \widehat{r}_{4,\cinactive} - \widehat{r}_{2,\cinactive} \big)}_{O(\Xi \epstol)}
+ \underbrace{\widehat{A}^\top \begin{bmatrix}
  \widehat{D}_{s,\cactive} \widehat{r}_{4,\cactive} - \widehat{r}_{2,\cactive}  \\
  \gamma \widehat{r}_3
\end{bmatrix}}_{O(\Xi^{-1}\epstol)} \; .
  \end{aligned}
\end{equation}
We denote $\widehat{s}_\gamma = \bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2 =
Y \widehat{s}_Y + Z \widehat{s}_Z$.
The error decomposes as $\Delta s_\gamma = Y \Delta s_Y Y + Z \Delta s_Z
= U_L \Delta s_L + U_S \Delta s_S$.
Using \eqref{eq:cond:boundderivationhykkt},
we have $\Delta s_Y = O(\Xi^{-1} \epstol)$ and $\Delta s_Z = O(\epstol)$.
Using \eqref{eq:cond:invariantsubpsace}, we deduce
$\Delta s_L = U_L^\top \Delta s_\gamma = O(\Xi^{-1} \epstol)$ and
$\Delta s_S = U_S^\top \Delta s_\gamma = O(\epstol)$.
We obtain using \eqref{eq:cond:boundinvariantsubspace} and \eqref{eq:cond:inversecondensed}
that the error in the large space $\Delta s_L$ annihilates in the backsolve:
\begin{equation}
  \label{eq:cond:boundhykkt1}
  K_\gamma^{-1} \Delta s_\gamma = U_L \Sigma_L^{-1} \Delta s_L + U_S \Sigma_S^{-1} \Delta s_S  = O(\epstol)
  \; .
\end{equation}
Using \eqref{eq:cond:invperturbed}, we get
\begin{equation}
  \widehat{G} \widehat{K}_\gamma^{-1} \Delta s_\gamma \approx
  \widehat{G} (I - K_\gamma^{-1}\Delta K_\gamma) K_\gamma^{-1} \Delta s_\gamma \; .
\end{equation}
Using \eqref{eq:cond:boundhykkt1}, the first term is $\widehat{G} K_\gamma^{-1} \Delta s_\gamma = O(\epstol)$.
We have in addition
\begin{equation}
  G K_\gamma^{-1}\Delta K_\gamma (K_\gamma^{-1} \Delta s_\gamma)  =
  \big[ G U_L \Sigma_L^{-1} \Gamma_L + G U_S \Sigma_S^{-1} \Gamma_S \big] (K_\gamma^{-1} \Delta s_\gamma) \; .
\end{equation}
Using again \eqref{eq:cond:invariantsubpsace}:
$G U_L = G Y + O(\Xi)$ and $G U_S = GZ + O(\Xi) = O(\Xi)$. Hence
$G U_L \Sigma_L^{-1} \Gamma_L = O(1)$ and $G U_S \Sigma_S^{-1} \Gamma_S = O(1)$.
Using \eqref{eq:cond:rhserrorfull}, we have $K_\gamma^{-1} \Delta G^\top = O(\epstol)$,
implying $\Delta G K_\gamma^{-1} \Delta K_\gamma (K_\gamma^{-1} \Delta s_\gamma) = O(\Xi^{-1} \epstol^{2})$.
Assumption~\ref{hyp:cond:wellcond} implies that $\Xi^{-1} \epstol^2 \ll \epstol$.
All in all, the error in the right-hand-side $\Delta \widehat{r}_\gamma$ satisfies:
\begin{equation}
  \label{eq:cond:errorrgamma}
  \Delta \widehat{r}_\gamma = -\Delta \bar{r}_2 + \widehat{G} \widehat{K}_\gamma^{-1} \Delta s_\gamma = O(\epstol) \;.
\end{equation}
The result is remarkable: despite an expression involving the inverse
of the ill-conditioned condensed matrix $K_\gamma$, the error made in $r_\gamma$
is bounded only by the machine precision $\epstol$.

\paragraph{Schur-complement operator.}
The solution of the system~\eqref{eq:kkt:schurcomplhykkt}
involves the Schur complement $S_\gamma = G K_\gamma^{-1} G^\top$.
We show that the Schur complement
has a specific structure that limits the loss of accuracy
in the conjugate gradient algorithm.

\begin{proposition}
  Suppose the current primal-dual iterate $(p, v)$ satisfies Assumption~\ref{hyp:cond:wellcond}.
  In exact arithmetic,
  \begin{equation}
    S_\gamma = GY \, \Sigma_L^{-1} \, Y^\top G^\top + O(\Xi^2) \; .
  \end{equation}
\end{proposition}
\begin{proof}
  Using \eqref{eq:cond:inversecondensed}, we have
  \begin{equation}
    G K_\gamma^{-1} G^\top =
    G U_L \Sigma_L^{-1} U_L^\top G^\top + G U_S \Sigma_S^{-1} U_S^\top G^\top \;.
  \end{equation}
  Using \eqref{eq:cond:invariantsubpsace}, we have $G U_L = GY + O(\Xi)$,
  and $G = O(1)$, implying
  \begin{equation}
    G U_L \Sigma_L^{-1} U_L^\top G^\top = G Y  \Sigma_L^{-1} Y^\top G^\top + O(\Xi^2) \; .
  \end{equation}
  Using again \eqref{eq:cond:invariantsubpsace}, we have $G U_S = GZ + O(\Xi) = O(\Xi)$.
  Hence, $G U_S \Sigma_S^{-1} U_S^\top G^\top = O(\Xi^2)$,
  concluding the proof.
\end{proof}
We adapt the previous proposition to bound the error made when evaluating
$\widehat{S}_\gamma$ in floating-point arithmetic.
\begin{proposition}
  Suppose the current primal-dual iterate $(p, v)$ satisfies Assumption~\ref{hyp:cond:wellcond}.
  In floating-point arithmetic,
  \begin{equation}
    \label{eq:cond:errorSgamma}
    \widehat{S}_\gamma = S_\gamma + O(\epstol) \; .
  \end{equation}
\end{proposition}
\begin{proof}
  We denote $\widehat{G} = G + \Delta G$ (with $G = O(\epstol)$). Then
  \begin{equation}
    \begin{aligned}
      \widehat{S}_\gamma &= \widehat{G} \widehat{K}_\gamma^{-1} \widehat{G}^\top \; , \\
                         &\approx (G + \Delta G)\big(K_\gamma^{-1} - K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1}\big)(G + \Delta G)^\top \;, \\
                    &\approx S_\gamma - G \big(K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1} \big)G^\top
                    + K_\gamma^{-1} \Delta G^\top + \Delta G K_\gamma^{-1} \; .
    \end{aligned}
  \end{equation}
  The second line is given by \eqref{eq:cond:invperturbed},
  the third by neglecting the second-order errors.
  Using \eqref{eq:cond:rhserrorfull}, we get $K_\gamma^{-1} \Delta G^\top = O(\epstol)$
  and $\Delta G K_\gamma^{-1} = O(\epstol)$.
  Using \eqref{eq:cond:inversecondensederror}, we have
  \begin{equation*}
    G \big(K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1} \big)G^\top =
G U_L \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}U_L^\top G^\top  +
G U_S \Sigma_S^{-1} \Gamma_S \Sigma_S^{-1}U_S^\top  G^\top \;.
  \end{equation*}
  Using \eqref{eq:cond:invariantsubpsace}, we have $G U_S = O(\Xi)$,
  and as $\Sigma_S^{-1} = \Theta(1)$ and $\Gamma_S = O(\Xi^{-1} \epstol)$, we
  get
  $G U_S \Sigma_S^{-1} \Gamma_S \Sigma_S^{-1}U_S^\top  G^\top = O(\Xi \epstol)$.
  Finally, as $\Sigma_L^{-1} = \Theta(\Xi)$ and $G U_L = GY + O(\Xi)$,
  we have
  \begin{equation}
    G U_L \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}U_L^\top G^\top =
    G Y \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}Y^\top G^\top + O(\Xi^2 \epstol) \; .
  \end{equation}
  We conclude the proof by using
  $G Y \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}Y^\top G^\top = O(\Xi \epstol)$.
\end{proof}
The two error bounds \eqref{eq:cond:errorrgamma} and
\eqref{eq:cond:errorSgamma} ensure that we can solve
\eqref{eq:kkt:schurcomplhykkt} using a conjugate gradient
algorithm, as the errors remain limited in floating-point
arithmetic.

\subsubsection{Solution with Lifted KKT system}
The sparse-condensed KKT system has removed the equality
constraints from the optimization problems, simplifying
the solution of the condensed KKT system to
\begin{equation}
  \label{eq:cond:sckktstep}
  K d_x = -r_1 + H^\top (D_s r_4 - r_2) \; .
\end{equation}
Hence, the active Jacobian $A$ reduces to the active inequalities $A = H_{\cactive}$.
Using a similar analysis than in \eqref{eq:cond:boundderivationhykkt},
the error in the right-hand-side is $O(\Xi^{-1} \epstol)$ and is in the
range space of the active Jacobian $A$. Using \eqref{eq:cond:inversecondensed},
we can show that the absolute error on $\widehat{d}_x$ is bounded by $O(\|d_x\| \epstol)
= O(\Xi \epstol)$. That implies the descent direction $\widehat{d}_x$ retains
full relative precision close to optimality.
In other words, we can refine the solution returned by the Cholesky solver accurately using
iterative refinement.

\subsubsection{Summary}
Numerically, the primal-dual step $(\widehat{d}_x, \widehat{d}_y)$
is computed only with an (absolute) precision $\varepsilon_{K}$,
greater than the machine precision $\epstol$ (for HyKKT, $\varepsilon_K$
is the absolute tolerance of the CG algorithm, for LiftedKKT the
absolute tolerance of the iterative refinement algorithm $\varepsilon_{K}$).

The errors $\widehat{d}_x - d_x = O(\varepsilon_K)$ and
$\widehat{d}_y - d_y = O(\varepsilon_K)$ propagate further in $(\widehat{d}_s, \widehat{d}_z)$.
According to \eqref{eq:kkt:condensed}, we have $\widehat{d}_s = - \widehat{r}_4 - \widehat{H} \widehat{d}_x$.
By continuity, $\widehat{H} = H + O(\epstol)$ and $\widehat{r}_4 = r_4 + O(\epstol)$, implying
\begin{equation}
  \widehat{d}_s = d_s + O(\varepsilon_K) \; .
\end{equation}
Eventually, we obtain $\widehat{d}_z = - \widehat{r}_2 - \widehat{D}_s \widehat{d}_s$,
giving the following bounds for the errors in the inactive and active components:
\begin{equation}
  \begin{aligned}
    & \widehat{d}_{z,\cactive} &= -\widehat{r}_{2,\cactive} - \widehat{D}_{s,\cactive} \widehat{d}_{s,\cactive}
    &= d_{z,\cactive} + O(\max\{\epstol, \varepsilon_K \Xi^{-1}\}) \;,\\
                             & \widehat{d}_{z,\cinactive} &= -\widehat{r}_{2,\cinactive} - \widehat{D}_{s,\cinactive} \widehat{d}_{s,\cinactive}
                               &= d_{z,\cinactive} + O(\varepsilon_K \Xi) \; .
  \end{aligned}
\end{equation}
The error arises mostly in the active components $\widehat{d}_{z,\cactive}$.
We want to set $\varepsilon_K \ll \Xi$ to limit the loss of accuracy, but doing
so is not trivial as we are approaching the optimum. For example,
solving the condensed system with a very good tolerance $\varepsilon_K = 10^{-12}$
would only ensure that the absolute error is bounded by $10^{-4}$ if $\Xi = 10^{-8}$
(as it is typical in primal-dual IPM). The impact remains limited if we
have only a few active inequality constraints.


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
