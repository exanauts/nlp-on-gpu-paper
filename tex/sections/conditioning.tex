\section{Conditioning of the condensed KKT system}
The condensed matrix $K$ is known to be ill-conditioned
close to local optimum solution. That behavior is amplified
for the matrix $K_\gamma$ as the parameter $\gamma$ can be large.
Let's look at a generic system $Mx = b$. Without
further assumption the relative error after a perturbation
$\Delta b$ on the right-hand-side is bounded by
\begin{subequations}
  \label{eq:cond:defaultbond}
\begin{equation}
  \| \Delta x \| \leq \| M^{-1} \| \| \Delta b \| \;, \quad
  \frac{\| \Delta x \| }{\| x \|} \leq \cond(M) \, \frac{\| \Delta b \|}{\|b \|} \;.
\end{equation}
When the matrix is perturbed by $\Delta M$, the perturbed solution
$\tilde{x}$ satisfies $\Delta x = \tilde{x}- x =  - (M + \Delta M)^{-1} \Delta M \tilde{x}$.
If $\cond(M) \approx \cond(M + \Delta M)$, the error $\Delta x$ satisfies
$M \Delta x \approx -\Delta M x$ (neglecting second-order terms), giving the bounds
\begin{equation}
  \| \Delta x \| \leq \|M^{-1}\| \|\Delta M \| \|x \| \; , \quad
  \frac{\| \Delta x \|}{\|x\|} \leq \cond(M)\frac{\|\Delta M \|}{\|M\|} \; .
\end{equation}
\end{subequations}
Hence, it is legitimate to investigate the impact of the ill-conditioning
when solving the condensed system~\eqref{eq:kkt:condensed} using
either the equality-relaxation strategy or the HyKKT strategy.
We will see that the we can found tighter bounds than \eqref{eq:cond:defaultbond}
for the relative error, using the structured ill-conditioning of the condensed matrix $K$.
We base our following analysis on \cite{wright1998ill}, where
the author has put a particular emphasis on the condensed KKT
system~\eqref{eq:kkt:condensed} we are interested in. We generalize her results to the
matrix $K_\gamma$, which incorporates both equality and inequality
constraints.

To clarify the notations, we suppose that the primal variable
$x$ is unconstrained, leaving only the slack $s$ with bounded values.
This is equivalent to redefine the inequality constraints
in \eqref{eq:problem} so as to include
the bounds on the variable $x$: $\bar{h}(x) \leq 0$ with $\bar{h}(x) = (h(x), -x)$.

\subsection{Centrality conditions}
We start this paragraph by recalling important results from
\cite{wright2001effects}.
Let $p = (x, s, y, z)$ the current primal-dual iterate,
and $p^\star$ a solution of the KKT conditions~\eqref{eq:kktconditions}.
We note $\delta(p, v) = \| (p, v) - (p^\star, v^\star) \|$ the Euclidean distance to the
primal-dual stationary point $p^\star$.
From \cite[Theorem 2.2]{wright2001effects}, we have the following bounds
if $p^\star$ satisfies the Assumptions~\ref{hyp:ipm} and $v > 0$:
\begin{equation}
  \delta(p, v) = \Theta\left( \left\Vert \begin{bmatrix}
      \nabla_p L(p, v) \\ \min(v, s)
  \end{bmatrix}
  \right\Vert \right) \; .
\end{equation}
For $(s, v) > 0$, we define the \emph{duality measure} $\Xi(s, v)$ as the mapping
\begin{equation}
  \Xi(s, v) = s^\top v / m_i \; .
\end{equation}
We suppose the iterates $(p, v)$ satisfies the \emph{centrality conditions}
\begin{subequations}
  \label{eq:centralitycond}
  \begin{align}
    & \| \nabla_p \mathcal{L}(p, v) \| \leq C \; \Xi(s, v) \;,  \\
    \label{eq:centralitycond:complement}
    & (s, v) > 0 \;,\quad s_i v_i \geq \alpha \, \Xi(s, v) \quad \forall i =1, \cdots, m_i \; ,
  \end{align}
\end{subequations}
for some constant $C > 0$ and $\alpha \in (0, 1)$.
Conditions~\eqref{eq:centralitycond:complement} ensures that the products
$s_i v_i$ are not too disparate in the diagonal term $D_s$.
We note that this condition is satisfied in Ipopt (see \cite[Equation (16)]{wachter2006implementation}).

\begin{proposition}[\cite{wright2001effects}, Lemma 3.2]
  \label{prop:cond:boundslack}
  Suppose $p^\star$ satisfies the assumptions~\ref{hyp:ipm},
  and the current primal-dual iterate $(p, v)$ satisfies the centrality
  conditions~\eqref{eq:centralitycond}. Then, we have that
  \begin{subequations}
    \begin{align}
      i \in \mathcal{B} \implies s_i = \Theta(\Xi) \, , \quad v_i = \Theta(1) \;, \\
      i \in \mathcal{N} \implies s_i = \Theta(1) \, , \quad v_i = \Theta(\Xi) \; .
    \end{align}
  \end{subequations}
\end{proposition}
Using the centrality conditions \eqref{eq:centralitycond}, we can bound
the distance to the solution $\delta(p, v)$ with the duality measure $\Xi$.
\begin{theorem}[\cite{wright2001effects}, Theorem 3.3]
  Suppose $p^\star$ satisfies the assumptions~\ref{hyp:ipm},
  and the current primal-dual iterate $(p, v)$ satisfies the centrality
  conditions~\eqref{eq:centralitycond}. Then, we have that
  \begin{equation}
    \delta(p, v) = O(\Xi) \; .
  \end{equation}
\end{theorem}

\subsection{Structured ill-conditioning}
We show that if the iterates $(p, v)$ satisfy
the centrality conditions~\eqref{eq:centralitycond}, then the
condensed matrix $K_\gamma$ exhibits a particular structure that limits
the effect of the ill-conditionning.

\subsubsection{Invariant subspaces in $K_\gamma$}
We follow the analysis presented in \cite{wright1998ill},
and show that the condensed matrix $K_\gamma$ can be decomposed as
\begin{equation}
  \label{eq:cond:svd}
  K_\gamma = \begin{bmatrix} U_L & U_S \end{bmatrix}
  \begin{bmatrix}
    \Sigma_L & 0 \\ 0 & \Sigma_S
  \end{bmatrix}
  \begin{bmatrix}
    U_L^\top \\ U_S^\top
  \end{bmatrix}
  \; ,
\end{equation}
with $U$ orthogonal matrix and $\Sigma$ diagonal, where
the two diagonal matrices $\Sigma_L$ and $\Sigma_S$ are much better conditioned than $K_\gamma$.
The result is a consequence of the SVD decomposition,
provided that the singular values satisfy $\frac{\sigma_1}{\sigma_{p}} \leq \frac{\sigma_1}{\sigma_n}$
and $\frac{\sigma_{p+1}}{\sigma_{n}} \leq \frac{\sigma_1}{\sigma_n}$.

We recall that $K_\gamma = W + H^\top D_s H + \gamma G^\top G$, with
the diagonal matrix $\Sigma_s = S^{-1} V$.
We note $\cactive = \cactive(x^\star)$ the active-set at the optimal solution $x^\star$,
and $\cinactive = \cinactive(x^\star)$ the inactive set.
In addition, we note $m_a$ the current number of
active constraints, and $p := m_a + m_e$. We denote by
$H_{\cactive}$ the Jacobian of active inequality constraints, $H_{\cinactive}$ the
Jacobian of inactive inequality constraints and
$A := \begin{bmatrix} G^\top & H_{\cactive}^\top \end{bmatrix}^\top$.
We define the minimum and maximum active slack values as
\begin{equation}
  s_{min} = \min_{i \in \cactive} s_i \; , \quad
  s_{max} = \max_{i \in \cactive} s_i \; .
\end{equation}

To prove $K_\gamma$ decomposes as in \eqref{eq:cond:svd},
we adapt \cite[Theorem 3.2]{wright1998ill} to the
condensed matrix $K_\gamma$, with the additional term
$\gamma G^\top G$ coming from the equality constraints.
\begin{theorem}[Properties of $K_\gamma$]
  \label{thm:cond}
  Suppose the condensed matrix is evaluated at a primal-dual
  point $(p, \nu)$ satisfying~\eqref{eq:centralitycond},
  for sufficiently small $\Xi$.
  Let $\lambda_1, \cdots, \lambda_n$ be the $n$ eigenvalues of
  $K_\gamma$, ordered as $|\lambda_1| \geq  \cdots \geq |\lambda_n|$.
  Let $\begin{bmatrix} Y & Z \end{bmatrix}$ be an orthogonal
  matrix, where the columns of $Z$ span the null-space of
  $A$. Let $\underline{\sigma} =\min\{\frac{1}{\Xi}, \gamma\}$
  and $\overline{\sigma} = \max\{\frac{1}{s_{min}}, \gamma\}$.
  Then,
  \begin{enumerate}
    \item[(i)] The $p$ largest-magnitude eigenvalues of $K_\gamma$ are positive,
      with $\lambda_1 = \Theta(\overline{\sigma})$ and $\lambda_p = \Omega(\underline{\sigma})$.
    \item[(ii)] The $n-p$ smallest-magnitude eigenvalues of $K_\gamma$
      are $\Theta(1)$.
    \item[(iii)] If $0 < p < n$, then $\cond(K_\gamma) = \Theta(\overline{\sigma})$.
    \item[(iv)] There are orthonormal matrices $\widetilde{Y}$ and $\widetilde{Z}$ for
      simple invariant subspaces of $K_\gamma$, such that $Y - \widetilde{Y} = O(\underline{\sigma}^{-1})$
      and $Z - \widetilde{Z} = O(\underline{\sigma}^{-1})$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We start the proof by setting apart the inactive constraints from the active constraints in $K_\gamma$:
  \begin{equation}
    K_\gamma = W + H_{\cinactive}^\top S_{\cinactive}^{-1} V_{\cinactive} H_{\cinactive}
    + A^\top D_\gamma A \, ,
    \quad
    \text{with} \quad D_\gamma = \begin{bmatrix} S_{\cactive}^{-1} V_{\cactive} & 0 \\ 0 & \gamma I \end{bmatrix} \; .
  \end{equation}
  Using Assumption~\ref{hyp:ipm}, the Hessian and the inactive Jacobian
  are bounded by Lipschitz continuity: $W = O(1)$, $H_{\cinactive} = O(1)$.
  Proposition~\ref{prop:cond:boundslack} implies that
  $s_{\cinactive} = \Theta(1)$ and $v_{\cinactive} = \Theta(\Xi)$. We deduce:
  \begin{equation}
    \label{eq:cond:inactiveblock}
    H_{\cinactive}^\top S_{\cinactive}^{-1} V_{\cinactive} H_{\cinactive} = O(\Xi) \; .
  \end{equation}
  Hence, for small enough $\Xi$,
  the condensed matrix $K_\gamma$ is dominated by the block of active constraints:
  \begin{equation}
    K_\gamma = A^\top D_\gamma A + O(1) \; .
  \end{equation}
  Sufficiently close to the optimum, the constraints qualification
  in Assumption~\ref{hyp:ipm} implies that $A = O(1)$ and has rank $p$.
  The eigenvalues $\{\eta_i\}_{i =1,\cdots,n}$ of $A^\top D_\gamma A$
  satisfy $\eta_i > 0$ for $i = 1,\cdots,p$ and $\eta_i = 0$ for $i = p+1, \cdots, n$.
  As $s_{\cactive} = \Theta(\Xi)$ and $v_{\cactive} = \Theta(1)$
  (Proposition~\ref{prop:cond:boundslack}), the smallest diagonal
  element in $D_\gamma$ is $\Omega(\min\{\frac{1}{\Xi}, \gamma\})$
  and the largest diagonal element is $\Theta(\max\{\frac{1}{s_{min}}, \gamma\})$.
  Hence,
  \begin{equation}
    \eta_1 = \Theta(\overline{\sigma}) \; , \quad
    \eta_p = \Omega(\underline{\sigma}) \; .
  \end{equation}
  Using \cite[Lemma 3.1]{wright1998ill}, we get $\lambda_1 = \Theta(\overline{\sigma})$
  and $\lambda_p = \Omega(\underline{\sigma})$, proving the first result (i).

  Let $L_\gamma = A^\top D_\gamma A$.
  We have that
  \begin{equation}
    \begin{bmatrix}
      Z^\top \\ Y^\top
    \end{bmatrix}
    L_\gamma \begin{bmatrix}Z & Y \end{bmatrix}
    = \begin{bmatrix}
      L_1 & 0 \\
      0 & L_2
    \end{bmatrix} \; ,
  \end{equation}
  with $L_1 = 0$ and $L_2 = Y^\top L_\gamma Y$.
  The smallest eigenvalue of $L_2$ is $\Omega(\underline{\sigma})$
  and the matrix $E := K_\gamma - L_\gamma$ is $O(1)$.
  By applying \cite[Theorem 3.1, (ii)]{wright1998ill},
  the $n - p$ smallest eigenvalues in $K_\gamma$ differ by
  $\Omega(\underline{\sigma}^{-1})$ from those of the reduced Hessian $Z^\top K_\gamma Z$.
  In addition, \eqref{eq:cond:inactiveblock} implies
  that $Z^\top K_\gamma Z - Z^\top W Z = O(\Xi)$. Using SOSC,
  $Z^\top W Z$ is positive definite for small enough $\Xi$, implying
  all its eigenvalues are $\Theta(1)$. Using again \cite[Lemma 3.1]{wright1998ill},
  we get that the $n-p$ smallest eigenvalues in $K_\gamma$ are $\Theta(1)$,
  proving (ii). The third point (iii) is proved by combining
  (i) and (ii) (provided $0 < p < n$).

  Eventually, point (iv) is proven by using \cite[Theorem 3.1, (i)]{wright1998ill}.
\end{proof}

\begin{corollary}
  The condensed matrix $K_\gamma$ can be decomposed as
  \eqref{eq:cond:svd}, with $\Sigma_L \in \mathbb{R}^{p \times p}$ and $\Sigma_S \in \mathbb{R}^{(n-p) \times (n-p)}$.
  In addition, for suitably chosen $Y$ and $Z$,
  \begin{equation}
    \label{eq:cond:invariantsubpsace}
    U_L - Y = O(\underline{\sigma}^{-1}) \; , \quad
    U_S - Z = O(\underline{\sigma}^{-1}) \; .
  \end{equation}
\end{corollary}
\begin{proof}
  According to Theorem~\ref{thm:cond}, the $p$ first eigenvalues are large
  and well separated from the $n - p$ small eigenvalues. Using
  the spectral theorem, we obtain the decomposition as \eqref{eq:cond:svd}.
  Using Theorem \ref{thm:cond}, part (iv), we obtain the result
  in \eqref{eq:cond:invariantsubpsace}.
\end{proof}
Theorem~\ref{thm:cond} gives us a deeper insight on the structure
of the condensed matrix $K_\gamma$.
Using equation~\eqref{eq:cond:invariantsubpsace}, we observe
we can assimilate the large-space of $K_\gamma$ with the range space of the
active Jacobian $A^\top$
and the small space with the null space of $A$.
The decomposition~\eqref{eq:cond:svd} leads to the following relations
\begin{equation}
  \label{eq:cond:boundinvariantsubspace}
  \begin{aligned}
    & \| K_\gamma \| = \| \Sigma_L \| = \Theta(\overline{\sigma}) \; , &
    \Sigma_L^{-1} = O(\underline{\sigma}^{-1})  \;, \\
    & \| K_\gamma^{-1} \| = \| \Sigma_S^{-1} \| = \Theta(1) \, , &
  \Sigma_S = \Theta(1) \, .
  \end{aligned}
\end{equation}
The condition of $\Sigma_L$ depends on $\cond(A)$, $\cond(V)$
and the ratio $\frac{s_{max}}{s_{min}} = O(\Xi \overline{\sigma})$.
The condition of $\Sigma_S$ reflects the condition of the reduced Hessian $Z^\top W Z$.

Theorem~\ref{thm:cond} (iii) tells us that $\cond(K_\gamma) = \Theta(\overline{\sigma})$,
meaning that if $\gamma$ is large-enough so that $\gamma \geq \frac{1}{s_{min}}$, then
the conditioning $\cond(K_\gamma)$ increases linearly with $\gamma$. In
other words, the value $\frac{1}{s_{min}}$ gives us a lower-bound for $\gamma_{max}$
in Proposition~\ref{prop:kkt:hykkt:cond}.

\subsubsection{Numerical accuracy of the condensed matrix $K_\gamma$}
We are now interested in bounding the error made in
the computation of the condensed matrix $K_\gamma$ in floating
point arithmetic.

In direct contrast with \cite[Section 4.1]{wright1998ill}, the slack variable
$s$ appearing in the diagonal matrix $S^{-1} V$ is not subject to cancellation:
the interior-point algorithm is passing exactly the value of the slack
variables to the linear solver (this would not be the case if the slack
$s_i$ was replaced by the nonlinear function $h_i(x)$). Hence,
the error arising from the multiplication by $S^{-1}$ is only $O(\epstol)$,
which is significantly better than the error in $O(\epstol / s_{min})$ we would obtain
in the cancellation case.

In floating point arithmetic, the condensed matrix $K_\gamma$ is evaluated as
\begin{multline*}
  \widehat{K}_\gamma = W + E + (A + \Delta A)^\top (D_\gamma + \Delta D_\gamma) (A + \Delta A) \\
  + (H_{\cinactive} + \Delta H_{\cinactive})^\top S_{\cinactive}^{-1} V_{\cinactive} (H_{\cinactive} + \Delta H_{\cinactive}) \; ,
\end{multline*}
with $\| \Delta H \| = O(\epstol \| H\|)$, $\| \Delta G \| = O(\epstol \| G\|)$
and $E$ symmetric with $\|E \| = O(\epstol \overline{\sigma})$.
We have
\begin{equation}
A^\top D_\gamma A = \Theta(\overline{\sigma}) \; , \quad
A^\top \Delta D_\gamma A = O(\epstol \overline{\sigma}) \; .
\end{equation}
We deduce that the perturbation $\Delta K_\gamma$ are bounded
by multiple of $\| K_\gamma \|$.
\begin{proposition}
  \label{prop:cond:boundcondensedmatrix}
  In floating point arithmetic, the perturbation
  of the condensed matrix $\Delta K_\gamma$ is bounded by
  $\Delta K_\gamma := \widehat{K_\gamma} - K_\gamma  = O(\epstol \overline{\sigma})$.
\end{proposition}
The perturbed matrix $\Delta K_\gamma$ has no specific structure
in our case. Hence, we should determine under which
conditions the perturbed matrix $\widehat{K}_\gamma$
keeps the structure highlighted in \eqref{eq:cond:svd}.
We know that the smallest eigenvalue of $A^\top D_\gamma A$
is $\Omega(\underline{\sigma})$. As mentioned in
\cite[Section 3.4.2]{wright1998ill}, the perturbed matrix
$\widehat{K}_\gamma$ continues to have $p$ large eigenvalues
bounded below by $\underline{\sigma}$ if the perturbation
is much smaller than the eigenvalue $\eta_p$:
\begin{equation}
  \label{eq:cond:perturbationbound}
  \| \Delta K_\gamma \| \ll \eta_p  \; .
\end{equation}
If the perturbation satisfies~\eqref{eq:cond:perturbationbound}, then
the conditions of Theorem~\ref{thm:cond} apply. However,
the bound in Proposition~\ref{prop:cond:boundcondensedmatrix} is too loose
to apply \eqref{eq:cond:perturbationbound} without any further assumption.
As $\eta_p = \Omega(\underline{\sigma})$, we have that $\frac{\|K_\gamma \|}{\eta_p}
= O(\overline{\sigma}\underline{\sigma}^{-1})$. In other words,
if $\overline{\sigma}\underline{\sigma}^{-1}$ is not too large,
$\|\Delta K_\gamma\| \ll \| K_\gamma \|$ implies \eqref{eq:cond:perturbationbound},
allowing to keep the structured ill-conditioning in the perturbed
matrix $\widehat{K}_\gamma$.

\subsubsection{Numerical solution of the condensed system}
We are now interested in estimating the relative error
made when solving the system $K_\gamma x = b$ in floating
point arithmetic. We suppose $K_\gamma$ is factorized using
a backward-stable Cholesky decomposition. The computed
solution $\widehat{x}$ is solution of a perturbed system
$\widetilde{K}_\gamma \widehat{x} = b$, with $\widetilde{K}_\gamma
= K_\gamma + \Delta_s K_\gamma$ and $\Delta_s K_\gamma$ a symmetric matrix satisfying
\begin{equation}
  \label{eq:cond:backwardstable}
\|\Delta_s K_\gamma\| \leq \epstol \varepsilon_n \|K_\gamma\| \; ,
\end{equation}
and $\epsilon_n$ a small constant depending on the dimension $n$.

We have to make the following additional assumptions to
ensure (i) the Cholesky factorization runs to completion
and (ii) we can incorporate the backward-stable perturbation $\Delta_s K_\gamma$
in the generic perturbation $\Delta K_\gamma$ introduced in
Proposition~\ref{prop:cond:boundcondensedmatrix}.
\begin{assumption} Let $(p, v)$ be the current primal-dual iterate
  , such that:
  \begin{itemize}
    \item[(a)] $(p, v)$ satisfies the centrality conditions~\eqref{eq:centralitycond}.
    \item[(c)] The parameter $\gamma$ satisfies $\gamma = \Theta(\Xi^{-1})$.
    \item[(c)] The duality measure is large enough relative to $\epstol$: $\epstol \ll \Xi$.
    \item[(d)] The primal step $\widehat{x}$ is computed using a backward
      stable method satisfying \eqref{eq:cond:backwardstable} for a small constant
      $\varepsilon_n$.
  \end{itemize}
  \label{hyp:cond:wellcond}
\end{assumption}
Condition (a) implies that
$s_{min} = \Theta(\Xi)$ and $s_{max} = \Theta(\Xi)$ (Proposition \ref{prop:cond:boundslack}).
Condition (b) supposes in addition $\gamma = \Theta(\Xi^{-1})$, implying
the matrix $\Sigma_L$ is well-conditioned and
the ratio $\overline{\sigma}/\underline{\sigma}$ is not too large.
In particular, we obtain $\underline{\sigma} = \Theta(\Xi^{-1})$,
$\overline{\sigma} = \Theta(\Xi^{-1})$ and $\overline{\sigma}/\underline{\sigma} = \Theta(1)$.
Condition (c) ensures that the conditioning of $\cond(K_\gamma)$
\eqref{eq:cond:boundinvariantsubspace} satisfies $\cond(K_\gamma) \epstol \ll 1$,
making sure the Cholesky factorization runs to completion.
Condition (d) tells us that the perturbation caused by the Cholesky
factorization is $\Delta_s K_\gamma = O(\epstol \| K_\gamma\|)$. The
conditions \eqref{eq:cond:boundinvariantsubspace} imply $\|K_\gamma \| = \Theta(\Xi^{-1})$,
meaning that we can incorporate $\Delta_s K_\gamma$ in the perturbation
$\Delta K_\gamma$ given in Proposition~\ref{prop:cond:boundcondensedmatrix}.

We note $x$ the solution of the linear system $K_\gamma x = b$
in exact arithmetic, and $\widehat{x}$ the solution of
the perturbed system $\widehat{K}_\gamma \widehat{x} = \widehat{b}$
in floating point arithmetic. We are interested in bounding
the error $\Delta x = \widehat{x} - x$. We express the exact solution $x$ as
\begin{equation}
  x = U_L x_L + U_S x_S = Y x_Y + Z x_Z \; .
\end{equation}
We note $\bar{x}$ the exact solution of the system
$K_\gamma \bar{x} = b + \Delta b$. Using \eqref{eq:cond:defaultbond},
we can bound the error with
\begin{equation}
  \| \bar{x} - x\| \leq \| K_\gamma^{-1} \| \|\Delta b \| \; .
\end{equation}
As $\| K_\gamma^{-1}\| = \Theta(1)$, if $\Delta b = O(\epstol)$ we have that
$\| \bar{x} - x \| = O(\epstol)$. However, we can
refine the bound if $\Delta b$ belongs to the range-space
of the active Jacobian $A^\top$.

\begin{proposition}[Right-hand-side perturbation]
  \label{prop:cond:rhsperturb}
  Suppose Assumptions~\ref{hyp:cond:wellcond} holds at
  the current the primal-dual iterate $(p, v)$, and
  $\Delta b \in \rangespace(A^\top)$. Then the
  error satisfies
  $\bar{x} - x = O(\Xi \epstol)$.
\end{proposition}
\begin{proof}
  As $\Delta b \in \rangespace(A^\top)$, we have $\Delta b_z = 0$
  and $\|\Delta b_Y \| = \|\Delta b \|$. We deduce
  \begin{equation}
    \bar{x} - x = K_\gamma^{-1} \Delta b =
    U_L \Sigma_L^{-1} \Delta b_L  +
    U_S \Sigma_S^{-1} \Delta b_S  \; .
  \end{equation}
  In addition, $\| U_L^\top \Delta b \| = \| \Delta b_L \| = O(\epstol)$
and $\| U_S^\top \Delta b \| = \| \Delta b_S \| = O(\Xi \epstol )$.
  Using \eqref{eq:cond:boundinvariantsubspace}, we deduce
  \begin{equation}
    \begin{aligned}
    & \| \bar{x}_L - x_L \| \leq \|\Sigma_L^{-1} \|\|\Delta b_L \| = O(\Xi \epstol) \;, \\
    & \| \bar{x}_S - x_S \| \leq \|\Sigma_S^{-1} \|\|\Delta b_S \| = O(\Xi \epstol) \;,
    \end{aligned}
  \end{equation}
  concluding the proof.
\end{proof}
We now look at the impact of a perturbed matrix.
We note $\widehat{x}$ the solution of $\widehat{K}_\gamma \widehat{x} = \widehat{b}$.
Using \eqref{eq:cond:defaultbond} and Proposition~\ref{prop:cond:boundcondensedmatrix}, we observe
that the upper-bound on the error $\|\widehat{x} - \bar{x}\|$ depends on $\Xi^{-1} \epstol$,
which can be significant close to convergence:
\begin{equation}
  \| \widehat{x} - \bar{x} \| \leq \|K_\gamma^{-1} \| \|\Delta K_\gamma\| \| \bar{x} \|
  = \| \bar{x} \| O(\Xi^{-1} \epstol) \; .
\end{equation}
Exploiting the structure of $K_\gamma$ explicited in \eqref{eq:cond:svd},
we can refine the bound further and show that only the small-space
part of $\widehat{x}$ is impacted by the ill-conditioning of $K_\gamma$.
\begin{proposition}[Matrix perturbation]
  \label{prop:cond:matrixperturb}
  Suppose Assumptions~\ref{hyp:cond:wellcond} holds at
  the current the primal-dual iterate $(p, v)$.
  The respective bound in the large-space error
  and small-space error are
  \begin{equation}
    \begin{aligned}
    & \| \widehat{x}_L - \bar{x}_L  \| \leq \|\Sigma_L^{-1} \|\|\Delta K_\gamma \|\|\bar{x} \| = \| \bar{x} \| O(\epstol) \;, \\
    & \| \widehat{x}_S - \bar{x}_S  \| \leq \|\Sigma_S^{-1} \|\|\Delta K_\gamma \|\|\bar{x} \| = \| \bar{x} \| O(\Xi^{-1}\epstol ) \;.
    \end{aligned}
  \end{equation}

\end{proposition}
\begin{proof}
  This is a direct application of TODO, using $\begin{bmatrix} B_L \\ B_S
  \end{bmatrix} = U^\top \Delta K_\gamma U$.
\end{proof}

Combining together Proposition~\ref{prop:cond:rhsperturb}
with Proposition~\ref{prop:cond:matrixperturb}, we deduce that
if $\Delta b \in \rangespace{A^\top}$, then
\begin{equation}
  \label{eq:cond:boundbacksolve}
  \begin{aligned}
    \| \widehat{x}_L - x_L  \| & \leq \|\widehat{x}_L - \bar{x}_L \|  + \|\bar{x}_L - x_L \|
    & \leq& \| \bar{x} \| O(\epstol) + O(\Xi\epstol) \;, \\
    \| \widehat{x}_S - x_S  \| & \leq \|\widehat{x}_S - \bar{x}_S \|  + \|\bar{x}_S - x_S \|
        & \leq& \| \bar{x} \| O(\Xi^{-1}\epstol) + O(\Xi\epstol) \;, \\
    \| \widehat{x} - x  \| & \leq \|\widehat{x} - \bar{x} \|  + \|\bar{x}- x \|
        & \leq& \| \bar{x} \| O(\Xi^{-1}\epstol) + O(\Xi \epstol) \;.
  \end{aligned}
\end{equation}


\subsection{Solution of the condensed KKT system}
Now that we have bounded the errors made in the the linear system
$K_\gamma x = b$, we have all the elements to bound the accuracy
of the solution of the condensed KKT system~\eqref{eq:kkt:condensed}
in floating point arithmetic. In all this section, we assume that
the primal-dual iterate $(p,v)$ satisfies the assumptions~\ref{hyp:cond:wellcond}.

In \eqref{eq:kkt:condensed}, the RHS $\bar{r}_1$ and $\bar{r}_2$
evaluate in floating point arithmetic as
\begin{equation}
  \label{eq:cond:condensedrhs}
  \left\{
  \begin{aligned}
    \bar{r}_1 &= - \widehat{r}_1 + \widehat{H}^\top\big(\widehat{D}_{s} \widehat{r}_{4} - \widehat{r}_{2} \big) \;, \\
     \bar{r}_2 &= -\widehat{r}_3 \; .
  \end{aligned}
  \right.
\end{equation}
Using basic floating point arithmetic, we get for $i=1,3,4$, $\widehat{r}_i = r_i + O(\epstol)$.
The error in the RHS $r_2$ is impacted by the badly conditioned term $\mu S^{-1}e$:
under Assumption~\ref{hyp:cond:wellcond}, it impacts differently
the the active and inactive components:
$\widehat{r}_{2,\cactive}= r_{2,\cactive} + O(\epstol)$ and
$\widehat{r}_{2,\cinactive}= r_{2,\cinactive} + O(\Xi \epstol)$.

\subsubsection{Solution with HyKKT}
We suppose we solve the condensed system~\eqref{eq:kkt:condensed} using HyKKT
and show that even for large values of $\gamma$ the error remains reasonable.
First, we can factor the matrix $K_\gamma$ using Cholesky if $q_n \cond(K_\gamma) \epstol < 1$
with $q_n$ a constant depending on the dimension $n$. As $\cond(K_\gamma) = \Theta(\Xi^{-1})$,
the condition holds if Assumption~\ref{hyp:cond:wellcond} is satisfied.

In floating point arithmetic, the initial RHS in \eqref{eq:kkt:schurcomplhykkt}
is evaluated as
$\widehat{r}_\gamma :=\widehat{G} \widehat{K}_\gamma^{-1} (\bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2) - \bar{r}_2$. Using \eqref{eq:cond:condensedrhs}, we have
\begin{equation}
  \label{eq:cond:boundderivationhykkt}
  \begin{aligned}
  \bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2 &=
- \widehat{r}_1 + \gamma \widehat{G}^\top \widehat{r}_3+ \widehat{H}^\top\big(\widehat{D}_{s} \widehat{r}_{4} - \widehat{r}_{2} \big) \\
&=  -
\underbrace{\widehat{r}_1}_{O(\epstol)} +
\underbrace{
\widehat{H}_{\cinactive}^\top\big(\widehat{D}_{s,\cinactive} \widehat{r}_{4,\cinactive} - \widehat{r}_{2,\cinactive} \big)}_{O(\Xi \epstol)}
+ \underbrace{\widehat{A}^\top \begin{bmatrix}
  \widehat{D}_{s,\cactive} \widehat{r}_{4,\cactive} - \widehat{r}_{2,\cactive}  \\
  \gamma \widehat{r}_3
\end{bmatrix}}_{O(\Xi^{-1}\epstol)} \; .
  \end{aligned}
\end{equation}
The terms in the underbrace show the bound on the errors. We observe the third terms
has an error increasing as $O(\Xi^{-1} \epstol)$
lying entirely in the range space of $\widehat{A}^\top$.
Hence, using Proposition~\ref{prop:cond:matrixperturb} and the bounds derived in \eqref{eq:cond:boundbacksolve}, we deduce
that the error made in the backsolve $\widehat{K}_\gamma^{-1}(\bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2)$
is bounded by $\|\bar{x}\|O(\epstol)$ in the large-space and $\|\bar{x}\|O(\Xi^{-1}\epstol)$
in the small-space. As the small space can be assimilated to the null-space of $A$ close
to convergence (see \eqref{eq:cond:invariantsubpsace} and we are multiplying the solution
of the backsolve by $G$ to assemble $r_\gamma$, we deduce that
\begin{equation}
  \widehat{r}_\gamma = r_\gamma + O(\| r_\gamma \| \epstol) \; .
\end{equation}

We are now interested into the accuracy of the matrix product
$y = S_\gamma x = (G K_{\gamma}^{-1} G^\top) x$, which implicitly determines
the accuracy we can achieve within the CG algorithm. Using a similar reasoning than with $\widehat{r}_\gamma$, we can
show that in floating point arithmetic, the output of the matrix-product
$\widehat{y} = \widehat{S}_\gamma x$ exhibits an error $\widehat{y} = y + O(\|x\| \epstol)$:
similarly, the large errors made in the backsolve $\widehat{K}_\gamma^{-1}$
are in small-space of $K_\gamma$ and cancel
when we multiply the solution by $\widehat{G}$ afterwards.
If

Eventually, HyKKT recovers the primal descent direction as $\widehat{K}_\gamma \widehat{d}_x = \widehat{r}_\gamma - \widehat{G}^\top \widehat{d}_y$. Using again Proposition~\ref{prop:cond:matrixperturb}, we get
\begin{equation}
  \widehat{d}_x = d_x + O() \; .
\end{equation}


\subsubsection{Solution with sparse-condensed KKT system}
The sparse-condensed KKT system has removed the equality
constraints from the optimization problems, simplifying
the solution of the condensed KKT system to
\begin{equation}
  K d_x = -r_1 + H^\top (D_s r_4 - r_2) \; .
\end{equation}
Hence, the active Jacobian $A$ reduces to the active inequalities $A = H_{\cactive}$.
Using a similar analysis than in \eqref{eq:cond:boundderivationhykkt},
the error in the right-hand-side is $O(\Xi^{-1} \epstol)$ and is in the
range space of the active Jacobian $A$. Using again Proposition~\ref{prop:cond:matrixperturb},
we can show that the error on $\widehat{d}_x$ is bounded by $O(\|\bar{r}_1\| \epstol)$.

\subsubsection{Summary}


