\section{Conditioning of the condensed KKT system}
The condensed matrix $K$ appearing in \eqref{eq:kkt:condensed} is known to be increasingly ill-conditioned as the primal-dual iterates approach to a local solution with active inequalities.
This behavior is amplified for the matrices $K_\gamma$ and $K_\tau$,
as HyKKT and LiftedKKT require the use of
respectively large $\gamma$ and small $\tau$. In this section, we analyze the
numerical error associated with the solution of the condensed KKT
system and discuss how the structured ill-conditioning makes the
adverse effect of ill-conditioning relatively benign.

We first discuss the perturbation bound for a generic linear system $Mx = b$.
The relative error after perturbing the right-hand side by $\Delta b$ is bounded by:
\begin{subequations}
  \label{eq:cond:defaultbond}
\begin{equation}
  \| \Delta x \| \leq \| M^{-1} \| \| \Delta b \| \;, \quad
  \frac{\| \Delta x \| }{\| x \|} \leq \cond(M) \, \frac{\| \Delta b \|}{\|b \|} \;.
\end{equation}
When the matrix is perturbed by $\Delta M$, the perturbed solution
$\widehat{x}$ satisfies $\Delta x = \widehat{x}- x =  - (M + \Delta M)^{-1} \Delta M \widehat{x}$.
If $\cond(M) \approx \cond(M + \Delta M)$, we have $M \Delta x \approx -\Delta M x$ (neglecting second-order terms),
giving the bounds
\begin{equation}
  \| \Delta x \| \lessapprox \|M^{-1}\| \|\Delta M \| \|x \| \; , \quad
  \frac{\| \Delta x \|}{\|x\|} \lessapprox \cond(M)\frac{\|\Delta M \|}{\|M\|} \; .
\end{equation}
\end{subequations}
The relative errors are bounded above by a term depending on the conditioning $\cond(M)$.
Hence, it is legitimate to investigate the impact of the ill-conditioning
when solving the condensed system~\eqref{eq:kkt:condensed} with LiftedKKT or with HyKKT.
We will see that we can tighten the bounds in \eqref{eq:cond:defaultbond}
by exploiting the structured ill-conditioning of the condensed matrix $K$.
We base our analysis on \cite{wright1998ill}, where
the author has put a particular emphasis on the condensed KKT
system~\eqref{eq:kkt:condensed} without equality constraints. We generalize their results to the
matrix $K_\gamma$, which incorporates both equality and inequality
constraints. The results extend directly to $K_\tau$ (by letting the number of equalities to be zero).

\subsection{Centrality conditions}
We start the discussion by recalling important results about the iterates of the interior-point algorithm.
For $p := (x, s, y, z)$, we denote by
$(p, v)$ the current primal-dual iterate,
and $(p^\star, v^\star)$ a solution of the KKT conditions~\eqref{eq:kktconditions}.
We denote $\cactive = \cactive(x^\star)$ the active-set at the optimal solution $x^\star$,
and $\cinactive = \cinactive(x^\star)$ the inactive set.
In this section, we are interested in the \emph{local} convergence behavior of the
primal-dual iterate, and we suppose $(p, v)$ is close enough to the solution
$(p^\star, v^\star)$.

% Sungho: not sure if this should be introduced here.
\begin{assumption}
  \label{hyp:ipm}
  Let $(p^\star, v^\star)$ be a primal-dual solution
  satisfying the KKT conditions~\eqref{eq:kktconditions}. Let the following hold:
  \begin{itemize}
  \item Continuity: The Hessian $\nabla^2_{x x} L(\cdot)$ is Lipschitz continuous
    near $p^\star$;
  \item Linear Independence Constraint Qualification (LICQ): the active Jacobian $A(x^\star)$ is full row-rank;
  \item Strict Complementarity (SCS): for every $i \in \mathcal{B}(x^\star)$, $z_i^\star > 0$.
  \item Second-order sufficiency (SOSC): for every $h \in \nullspace\!\big(A(x^\star)\big)$,
    $h^\top \nabla_{x x}^2 L(p^\star)h > 0$.
  \end{itemize}
\end{assumption}

We denote $\delta(p, v) = \| (p, v) - (p^\star, v^\star) \|$ the Euclidean distance to the
primal-dual stationary point $(p^\star, v^\star)$.
From \cite[Theorem 2.2]{wright2001effects}, if Assumption~\ref{hyp:ipm}
holds at $p^\star$ and $v > 0$,
\begin{equation}
  \delta(p, v) = \Theta\left( \left\Vert \begin{bmatrix}
      \nabla_p L(p, v) \\ \min(v, s)
  \end{bmatrix}
  \right\Vert \right) \; .
\end{equation}
For feasible iterate $(s, v) > 0$, we define the \emph{duality measure} $\Xi(s, v)$ as the mapping
\begin{equation}
  \Xi(s, v) = s^\top v / m_i \; , % Sungho: m_i not defined here. Also, I'd suggest explaining a bit of context on the duality measure.
\end{equation}
where $m_i$ is the number of inequality constraints.
The duality measure encodes the current satisfaction of the complementarity
constraints. For a solution $(p^\star, v^\star)$, we have $\Xi(s^\star, v^\star) = 0$.
The duality measure can be used to define the barrier parameter in IPM.

We suppose the iterates $(p, v)$ satisfy the \emph{centrality conditions}
\begin{subequations}
  \label{eq:centralitycond}
  \begin{align}
    & \| \nabla_p \mathcal{L}(p, v) \| \leq C \; \Xi(s, v) \;,  \\
    \label{eq:centralitycond:complement}
    & (s, v) > 0 \;,\quad s_i v_i \geq \alpha \, \Xi(s, v) \quad \forall i =1, \cdots, m_i \; ,
  \end{align}
\end{subequations}
for some constants $C > 0$ and $\alpha \in (0, 1)$.
Conditions~\eqref{eq:centralitycond:complement} ensure that the products
$s_i v_i$ are not too disparate in the diagonal term $D_s$.
This condition is satisfied (even if rather loosely)
in the solver Ipopt (see \cite[Equation (16)]{wachter2006implementation}).

\begin{proposition}[\cite{wright2001effects}, Lemma 3.2 and Theorem 3.3]
  \label{prop:cond:boundslack}
  Suppose $p^\star$ satisfies Assumption~\ref{hyp:ipm}.
  If the current primal-dual iterate $(p, v)$ satisfies the centrality
  conditions~\eqref{eq:centralitycond}, then
  \begin{subequations}
    \begin{align}
      i \in \mathcal{B} \implies s_i = \Theta(\Xi) \, , \quad v_i = \Theta(1) \;, \\
      i \in \mathcal{N} \implies s_i = \Theta(1) \, , \quad v_i = \Theta(\Xi) \; .
    \end{align}
    and the distance to the solution $\delta(p, v)$ is bounded by the duality measure $\Xi$:
    \begin{equation}
      \delta(p, v) = O(\Xi) \; .
    \end{equation}
  \end{subequations}
\end{proposition}

\subsection{Structured ill-conditioning}
The following subsection looks at the structure
of the condensed matrix $K_\gamma$ in HyKKT. All the results
apply directly to the matrix $K_\tau$ in LiftedKKT, by setting the number
of equality constraints to $m_e = 0$.
First, we show that if the iterates $(p, v)$ satisfy
the centrality conditions~\eqref{eq:centralitycond}, then the
condensed matrix $K_\gamma$ exhibits a structured ill-conditioning.

\subsubsection{Invariant subspaces in $K_\gamma$}
Without regularization we have that $K_\gamma = W + H^\top D_s H + \gamma G^\top G$, with
the diagonal $D_s = S^{-1} V$.
We note by $m_a$ the cardinality of the active set $\mathcal{B}$,
$H_{\cactive}$ the Jacobian of active inequality constraints, $H_{\cinactive}$ the
Jacobian of inactive inequality constraints and by
$A := \begin{bmatrix} H_{\cactive}^\top & G^\top \end{bmatrix}^\top$ the active Jacobian.
We define the minimum and maximum active slack values as
\begin{equation}
  s_{min} = \min_{i \in \cactive} s_i \; , \quad
  s_{max} = \max_{i \in \cactive} s_i \; .
\end{equation}
We recall that $m_e$ is the number of equality constraints,
and define $\ell := m_e + m_a$.

We express the structured ill-conditioning of $K_\gamma$ by
modifying the approach outlined in \cite[Theorem 3.2]{wright1998ill} to account for the additional
term $\gamma G^\top G$ arising from the equality constraints.
We show that the matrix $K_\gamma$ has two invariant subspaces
(in the sense defined in \cite[Chapter 5]{stewart1990matrix}),
associated respectively to the range of the transposed active Jacobian
(\emph{large space}) and to the null space of the active Jacobian (\emph{small space}).

\begin{theorem}[Properties of $K_\gamma$]
  \label{thm:cond}
  Suppose the condensed matrix is evaluated at a primal-dual
  point $(p, \nu)$ satisfying~\eqref{eq:centralitycond},
  for sufficiently small $\Xi$.
  Let $\lambda_1, \cdots, \lambda_n$ be the $n$ eigenvalues of
  $K_\gamma$, ordered as $|\lambda_1| \geq  \cdots \geq |\lambda_n|$.
  Let $\begin{bmatrix} Y & Z \end{bmatrix}$ be an orthogonal
  matrix, where $Z$ encodes the basis of the null-space of
  $A$. Let $\underline{\sigma} :=\min\left(\frac{1}{\Xi}, \gamma\right)$
  and $\overline{\sigma} := \max\left(\frac{1}{s_{min}}, \gamma\right)$.
  Then,
  \begin{enumerate}
    \item[(i)] The $\ell$ largest-magnitude eigenvalues of $K_\gamma$ are positive,
      with $\lambda_1 = \Theta(\overline{\sigma})$ and $\lambda_{\ell} = \Omega(\underline{\sigma})$.
    \item[(ii)] The $n-\ell$ smallest-magnitude eigenvalues of $K_\gamma$
      are $\Theta(1)$.
    \item[(iii)] If $0 < \ell < n$, then $\cond(K_\gamma) = \Theta(\overline{\sigma})$.
    \item[(iv)] There are orthonormal matrices $\widetilde{Y}$ and $\widetilde{Z}$ for
      simple invariant subspaces of $K_\gamma$ such that $Y - \widetilde{Y} = O(\underline{\sigma}^{-1})$
      and $Z - \widetilde{Z} = O(\underline{\sigma}^{-1})$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  We start the proof by setting apart the inactive constraints from the active constraints in $K_\gamma$:
  \begin{equation}
    K_\gamma = W + H_{\cinactive}^\top S_{\cinactive}^{-1} V_{\cinactive} H_{\cinactive}
    + A^\top D_\gamma A \, ,
    \quad
    \text{with} \quad D_\gamma = \begin{bmatrix} S_{\cactive}^{-1} V_{\cactive} & 0 \\ 0 & \gamma I \end{bmatrix} \; .
  \end{equation}
  Using Assumption~\ref{hyp:ipm}, Lipschitz
  continuity implies that the Hessian and the inactive Jacobian
  are bounded: $W = O(1)$, $H_{\cinactive} = O(1)$.
  Proposition~\ref{prop:cond:boundslack} implies that
  $s_{\cinactive} = \Theta(1)$ and $v_{\cinactive} = \Theta(\Xi)$. We deduce:
  \begin{equation}
    \label{eq:cond:inactiveblock}
    H_{\cinactive}^\top S_{\cinactive}^{-1} V_{\cinactive} H_{\cinactive} = O(\Xi) \; .
  \end{equation}
  Hence, for small enough $\Xi$,
  the condensed matrix $K_\gamma$ is dominated by the block of active constraints:
  \begin{equation}
    K_\gamma = A^\top D_\gamma A + O(1) \; .
  \end{equation}
  Sufficiently close to the optimum $p^\star$, the constraints qualification
  in Assumption~\ref{hyp:ipm} implies that $A = \Theta(1)$ and has rank $\ell$.
  The eigenvalues $\{\eta_i\}_{i =1,\cdots,n}$ of $A^\top D_\gamma A$
  satisfy $\eta_i > 0$ for $i = 1,\cdots,\ell$ and $\eta_i = 0$ for $i = \ell+1, \cdots, n$.
  As $s_{\cactive} = \Theta(\Xi)$ and $v_{\cactive} = \Theta(1)$
  (Proposition~\ref{prop:cond:boundslack}), the smallest diagonal
  element in $D_\gamma$ is $\Omega(\min\{\frac{1}{\Xi}, \gamma\})$
  and the largest diagonal element is $\Theta(\max\{\frac{1}{s_{min}}, \gamma\})$.
  Hence,
  \begin{equation}
    \eta_1 = \Theta(\overline{\sigma}) \; , \quad
    \eta_\ell = \Omega(\underline{\sigma}) \; .
  \end{equation}
  Using \cite[Lemma 3.1]{wright1998ill}, we deduce $\lambda_1 = \Theta(\overline{\sigma})$
  and $\lambda_\ell = \Omega(\underline{\sigma})$, proving the first result (i).

  Let $L_\gamma := A^\top D_\gamma A$.
  We have that
  \begin{equation}
    \begin{bmatrix}
      Z^\top \\ Y^\top
    \end{bmatrix}
    L_\gamma \begin{bmatrix}Z & Y \end{bmatrix}
    = \begin{bmatrix}
      L_1 & 0 \\
      0 & L_2
    \end{bmatrix} \; ,
  \end{equation}
  with $L_1 = 0$ and $L_2 = Y^\top L_\gamma Y$.
  The smallest eigenvalue of $L_2$ is $\Omega(\underline{\sigma})$
  and the matrix $E := K_\gamma - L_\gamma$ is $O(1)$.
  By applying \cite[Theorem 3.1, (ii)]{wright1998ill},
  the $n - \ell$ smallest eigenvalues in $K_\gamma$ differ by
  $\Omega(\underline{\sigma}^{-1})$ from those of the reduced Hessian $Z^\top K_\gamma Z$.
  In addition, \eqref{eq:cond:inactiveblock} implies
  that $Z^\top K_\gamma Z - Z^\top W Z = O(\Xi)$. Using SOSC,
  $Z^\top W Z$ is positive definite for small enough $\Xi$, implying
  all its eigenvalues are $\Theta(1)$. Using again \cite[Lemma 3.1]{wright1998ill},
  we get that the $n-\ell$ smallest eigenvalues in $K_\gamma$ are $\Theta(1)$,
  proving (ii). The results in (iii) can be obtained by combining
  (i) and (ii) (provided $0 < \ell < n$).
  Finally, point (iv) directly follows from \cite[Theorem 3.1 (i)]{wright1998ill}.
\end{proof}

\begin{corollary}
  \label{corr:cond:illstructured}
  The condensed matrix $K_\gamma$ can be decomposed as
  \begin{equation}
    \label{eq:cond:svd}
    K_\gamma = U \Sigma U^\top = \begin{bmatrix} U_L & U_S \end{bmatrix}
    \begin{bmatrix}
      \Sigma_L & 0 \\ 0 & \Sigma_S
    \end{bmatrix}
    \begin{bmatrix}
      U_L^\top \\ U_S^\top
    \end{bmatrix}
    \; ,
  \end{equation}
  with $\Sigma_L = \diag(\sigma_1, \cdots, \sigma_\ell) \in \mathbb{R}^{\ell \times \ell}$ and $\Sigma_S  = \diag(\sigma_{\ell+1}, \cdots, \sigma_n)\in \mathbb{R}^{(n-\ell) \times (n-\ell)}$
  two diagonal matrices, and $U_L \in \mathbb{R}^{n \times \ell}$,
  $U_S \in \mathbb{R}^{n \times (n - \ell)}$ two
  orthogonal matrices such that $U_L^\top U_S = 0$.
  The diagonal elements in $\Sigma_S$ and $\Sigma_L$ satisfy
  \begin{equation}
    \label{eq:cond:svddiag}
    \frac{\sigma_1}{\sigma_\ell} \ll \frac{\sigma_1}{\sigma_n} \; , \quad
    \frac{\sigma_{\ell +1}}{\sigma_{n}} \ll \frac{\sigma_1}{\sigma_n} \; .
  \end{equation}
  For suitably chosen basis $Y$ and $Z$,
  spanning respectively the row space and the null space of
  the active Jacobian $A$, we get
  \begin{equation}
    \label{eq:cond:invariantsubpsace}
    U_L - Y = O(\underline{\sigma}^{-1}) \; , \quad
    U_S - Z = O(\underline{\sigma}^{-1}) \; .
  \end{equation}
\end{corollary}
\begin{proof}
  Using the spectral theorem, we obtain the decomposition as \eqref{eq:cond:svd}.
  According to Theorem~\ref{thm:cond}, the $\ell$ largest eigenvalues of $K_\gamma$ are large
  and well separated from the $n - \ell$ smallest eigenvalues,
  establishing \eqref{eq:cond:svddiag}.
  Using Theorem \ref{thm:cond}, part (iv), we obtain the result
  in \eqref{eq:cond:invariantsubpsace}.
\end{proof}
Corollary~\ref{corr:cond:illstructured} gives us a deeper insight into the structure
of the condensed matrix $K_\gamma$.
Using equation~\eqref{eq:cond:invariantsubpsace}, we observe
we can assimilate the large space of $K_\gamma$ with $\rangespace(A^\top)$
and the small space with $\nullspace(A)$.
The decomposition~\eqref{eq:cond:svd} leads to the following relations
\begin{equation}
  \label{eq:cond:boundinvariantsubspace}
  \begin{aligned}
    & \| K_\gamma \| = \| \Sigma_L \| = \Theta(\overline{\sigma}) \; , &
    \Sigma_L^{-1} = O(\underline{\sigma}^{-1})  \;, \\
    & \| K_\gamma^{-1} \| = \| \Sigma_S^{-1} \| = \Theta(1) \, , &
  \Sigma_S = \Theta(1) \, .
  \end{aligned}
\end{equation}
The condition of $\Sigma_L$ depends on $\cond(A)$
and the ratio $\frac{s_{max}}{s_{min}} = O(\Xi \overline{\sigma})$.
The condition of $\Sigma_S$ reflects the condition of the reduced Hessian $Z^\top W Z$.

Three observations are due:
\begin{enumerate}
  \item Theorem~\ref{thm:cond} (iii) tells us that $\cond(K_\gamma) = \Theta(\overline{\sigma})$,
    meaning that if $\gamma \geq \frac{1}{s_{min}}$, then
    the conditioning $\cond(K_\gamma)$ increases linearly with $\gamma$, hence
    recovering a known result \cite{regev2023hykkt}.
  \item In early IPM iterations, the slacks are pushed away from the boundary
    and the number of active inequality constraints is $m_a = 0$. The ill-conditioning
    in $K_\gamma$ is caused only by $\gamma G^\top G$ and $\underline{\sigma} = \overline{\sigma} = \gamma$.
  \item In late IPM iterations, the active slacks are converging to $0$. We observe
    that if $\frac{1}{\Xi} \leq \gamma \leq \frac{1}{s_{min}}$ the parameter $\gamma$
    does not increase the ill-conditioning of the condensed matrix $K_\gamma$.
\end{enumerate}

\subsubsection{Numerical accuracy of the condensed matrix $K_\gamma$}
In floating-point arithmetic, the condensed matrix $K_\gamma$ is evaluated as
\begin{multline*}
  \widehat{K}_\gamma = W + \Delta W + (A + \Delta A)^\top (D_\gamma + \Delta D_\gamma) (A + \Delta A) \\
  + (H_{\cinactive} + \Delta H_{\cinactive})^\top S_{\cinactive}^{-1} V_{\cinactive} (H_{\cinactive} + \Delta H_{\cinactive}) \; ,
\end{multline*}
with $\Delta W = O(\epstol)$, $\Delta H_{\cinactive}  = O(\epstol)$, $ \Delta A  = \Theta(\epstol)$,
$\Delta D_\gamma = O(\epstol \overline{\sigma})$: most
of the errors arise because of the ill-conditioned diagonal terms in $D_\gamma$.
% We deduce that the perturbation $\Delta K_\gamma$ is bounded by $O(\epstol \overline{\sigma})$. % Sungho: Here, we should elaborate more; I do not see this immediately.
\begin{proposition}
  \label{prop:cond:boundcondensedmatrix}
  In floating-point arithmetic, the perturbation
  of the condensed matrix $K_\gamma$ satisfies
  $\Delta K_\gamma := \widehat{K_\gamma} - K_\gamma  = O(\epstol \overline{\sigma})$.
\end{proposition}
\begin{proof}
  As $A = \Theta(1)$, we have:
 $A^\top D_\gamma A = \Theta(\overline{\sigma})$ and
  $A^\top \Delta D_\gamma A = O(\epstol \overline{\sigma})$.
  Neglecting second-order terms, we get
  \begin{multline*}
    \Delta K_\gamma =
    \overbrace{\Delta W}^{O(\epstol)}
    + \overbrace{\Delta A^\top D_\gamma A}^{O(\overline{\sigma}\epstol)}
    + \overbrace{A^\top D_\gamma \Delta A}^{O(\overline{\sigma}\epstol)}
    + \overbrace{A^\top \Delta D_\gamma A}^{O(\overline{\sigma}\epstol)}  \\
    + \underbrace{\Delta H_{\cinactive} S_{\cinactive}^{-1} V_{\cinactive} H_{\cinactive}}_{O(\epstol)}
    + \underbrace{H_{\cinactive} S_{\cinactive}^{-1} V_{\cinactive} \Delta H_{\cinactive} }_{O(\epstol)}
      \; ,
  \end{multline*}
  where the terms in braces show the respective bounds on the errors.
  We deduce the error is dominated by the terms arising from the active Jacobian,
  all bounded by $O(\overline{\sigma} \epstol)$, hence concluding the proof.
\end{proof}

If it is large enough,
the unstructured perturbation $\Delta K_\gamma$
can impact the structured ill-conditioning in the perturbed
matrix $\widehat{K}_\gamma$.
We know that the smallest eigenvalue $\eta_\ell$ of $A^\top D_\gamma A$
is $\Omega(\underline{\sigma})$. As mentioned in
\cite[Section 3.4.2]{wright1998ill}, the perturbed matrix
$\widehat{K}_\gamma$ keeps the $p$ large eigenvalues
bounded below by $\underline{\sigma}$ if the perturbation
is itself much smaller than the eigenvalue $\eta_\ell$:
\begin{equation}
  \label{eq:cond:perturbationbound}
  \| \Delta K_\gamma \| \ll \eta_\ell = \Omega(\underline{\sigma})  \; .
\end{equation}
However, the bound given in Proposition~\ref{prop:cond:boundcondensedmatrix} is too loose
for \eqref{eq:cond:perturbationbound} to hold without any further assumption
(we have only $\underline{\sigma} \leq \overline{\sigma}$).
We note that for some constant $C > 0$, $\Delta K_\gamma \leq C \epstol \overline{\sigma}$,
implying $\Delta K_\gamma / \underline{\sigma} \leq C \epstol \overline{\sigma} / \underline{\sigma}$.
Hence, if we suppose in addition the ratio $\overline{\sigma}/\underline{\sigma}$ is close to $1$,
then $\|\Delta K_\gamma\| = O(\epstol \overline{\sigma})$ can instead be replaced by
$\| \Delta K_\gamma\|= O(\epstol \underline{\sigma})$, ensuring \eqref{eq:cond:perturbationbound} holds.

\subsubsection{Numerical solution of the condensed system}
We are interested in estimating the relative error
made when solving the system $K_\gamma x = b$ in floating
point arithmetic. We suppose $K_\gamma$ is factorized using
a backward-stable Cholesky decomposition. The computed
solution $\widehat{x}$ is solution of a perturbed system
$\widetilde{K}_\gamma \widehat{x} = b$, with $\widetilde{K}_\gamma
= K_\gamma + \Delta_s K_\gamma$ and $\Delta_s K_\gamma$ a symmetric matrix satisfying
\begin{equation}
  \label{eq:cond:backwardstable}
  \|\Delta_s K_\gamma\| \leq \epstol \varepsilon_n \|K_\gamma\| \;,
\end{equation}
for $\varepsilon_n$ a small constant depending on the dimension $n$.
We need the following additional assumptions to
ensure (a) the Cholesky factorization runs to completion
and (b) we can incorporate the backward-stable perturbation $\Delta_s K_\gamma$
in the generic perturbation $\Delta K_\gamma$ introduced in
Proposition~\ref{prop:cond:boundcondensedmatrix}.
\begin{assumption} Let $(p, v)$ be the current primal-dual iterate. We assume:
  \begin{itemize}
    \item[(a)] $(p, v)$ satisfies the centrality conditions~\eqref{eq:centralitycond}.
    \item[(b)] The parameter $\gamma$ satisfies $\gamma = \Theta(\Xi^{-1})$.
    \item[(c)] The duality measure is large enough relative to the precision $\epstol$: $\epstol \ll \Xi$.
    \item[(d)] The primal step $\widehat{x}$ is computed using a backward
      stable method satisfying~\eqref{eq:cond:backwardstable} for a small constant
      $\varepsilon_n$.
  \end{itemize}
  \label{hyp:cond:wellcond}
\end{assumption}
Condition (a) implies that
$s_{min} = \Theta(\Xi)$ and $s_{max} = \Theta(\Xi)$ (Proposition \ref{prop:cond:boundslack}).
Condition (b) supposes in addition $\gamma = \Theta(\Xi^{-1})$, making
the matrix $\Sigma_L$ well-conditioned with
$\underline{\sigma} = \Theta(\Xi^{-1})$,
$\overline{\sigma} = \Theta(\Xi^{-1})$ and $\overline{\sigma}/\underline{\sigma} = \Theta(1)$.
Condition (c) ensures that $\cond(K_\gamma) = \Theta(\overline{\sigma})$
satisfies $\cond(K_\gamma) \epstol \ll 1$
(implying the Cholesky factorization runs to completion).
Condition (d) tells us that the perturbation caused by the Cholesky
factorization is $\Delta_s K_\gamma = O(\epstol \| K_\gamma\|)$. As
\eqref{eq:cond:boundinvariantsubspace} implies $\|K_\gamma \| = \Theta(\Xi^{-1})$,
we can incorporate $\Delta_s K_\gamma$ in the perturbation
$\Delta K_\gamma$ given in Proposition~\ref{prop:cond:boundcondensedmatrix}.

We are now ready to analyze the perturbation bound for the condensed system.
We denote $x$ the solution of the linear system $K_\gamma x = b$
in exact arithmetic, and $\widehat{x}$ the solution of
the perturbed system $\widehat{K}_\gamma \widehat{x} = \widehat{b}$
in floating-point arithmetic. We are interested in bounding
the error $\Delta x = \widehat{x} - x$. We
recall that every vector $x \in \mathbb{R}^n$ decomposes as
\begin{equation}
  x = U_L x_L + U_S x_S = Y x_Y + Z x_Z \; .
\end{equation}

\paragraph{Impact of right-hand-side perturbation.}
Using \eqref{eq:cond:svd}, the inverse of
$K_\gamma$ satisfies
\begin{equation}
  \label{eq:cond:inversecondensed}
  K_\gamma^{-1}  = \begin{bmatrix} U_L & U_S \end{bmatrix}
  \begin{bmatrix}
    \Sigma_L^{-1} & 0 \\ 0 & \Sigma_S^{-1}
  \end{bmatrix}
  \begin{bmatrix}
    U_L^\top \\ U_S^\top
  \end{bmatrix}
  \; .
\end{equation}
Hence, if we solve the system for $\widehat{b} := b + \Delta b$,
$\Delta x = K_\gamma^{-1} \Delta b$ decomposes as
\begin{equation}
  \begin{bmatrix}
    \Delta x_L \\ \Delta x_S
  \end{bmatrix}
  =
  \begin{bmatrix}
    \Sigma_L^{-1} & 0 \\ 0 & \Sigma_S^{-1}
  \end{bmatrix}
  \begin{bmatrix}
    \Delta b_L \\ \Delta b_S
  \end{bmatrix}
  \; ,
\end{equation}
which in turn implies the following bounds:
\begin{equation}
  \label{eq:cond:rhserror}
     \| \Delta x_L \| \leq \| \Sigma_L^{-1} \| \| \Delta b_L \| \; ,\quad
    \| \Delta x_S \| \leq \| \Sigma_S^{-1} \| \| \Delta b_S \| \; .
\end{equation}
As $\Sigma_L^{-1} = O(\Xi)$ and $\Sigma_S^{-1} = \Theta(1)$,
we deduce that the error $\Delta x_L$ is smaller by a factor
of $\Xi$ than the error $\Delta x_S$. The total error
$\Delta x = U_L \Delta x_L + U_S \Delta x_S$ is bounded by
\begin{equation}
  \label{eq:cond:rhserrorfull}
  \| \Delta x \|
  \leq  \| \Sigma_L^{-1} \| \| \Delta b_L \| + \| \Sigma_S^{-1} \| \| \Delta b_S \| =
  O(\|\Delta b \|) \; .
\end{equation}

\paragraph{Impact of matrix perturbation.}
As $\|\Delta K_\gamma\| \ll \|K_\gamma\|$, we have that
\begin{equation}
  \label{eq:cond:invperturbed}
  \begin{aligned}
    (K_\gamma + \Delta K_\gamma)^{-1} &= (I + K_\gamma^{-1} \Delta K_\gamma)^{-1} K_\gamma^{-1} \; , \\
                                      &= K_\gamma^{-1} - K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1} + O(\|\Delta K_\gamma\|^2) \; .
  \end{aligned}
\end{equation}
We decompose $\Delta K_\gamma$ in two matrices
$\Gamma_L \in \mathbb{R}^{\ell \times n}$ and $\Gamma_S \in \mathbb{R}^{(n-\ell) \times n}$ such that
$\Delta K_\gamma = \begin{bmatrix}
  \Gamma_L \\ \Gamma _S
\end{bmatrix}$.
Using \eqref{eq:cond:inversecondensed} the first-order error is given by
\begin{equation}
  \label{eq:cond:inversecondensederror}
  K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1}  =
U_L \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}U_L^\top  +
  U_S \Sigma_S^{-1} \Gamma_S \Sigma_S^{-1}U_S^\top  \;.
\end{equation}
Using \eqref{eq:cond:boundinvariantsubspace} and $(\Gamma_L, \Gamma_S)= O( \Xi^{-1}\epstol)$,
we obtain $\Sigma_L^{-1} \Gamma_L \Sigma_L^{-1} = O(\Xi \epstol)$
and $\Sigma_S^{-1} \Gamma_S \Sigma_S^{-1} = O(\Xi^{-1} \epstol)$.
We deduce that the error made in the large space is $O(\Xi\epstol)$ whereas
the error in the small space is $O(\Xi^{-1}\epstol )$.

\subsection{Solution of the condensed KKT system}
We use the relations~\eqref{eq:cond:rhserror} and \eqref{eq:cond:inversecondensederror}
to bound the error made when solving the condensed KKT system~\eqref{eq:kkt:condensed}
in floating-point arithmetic.
In all this section, we assume that
the primal-dual iterate $(p,v)$ satisfies Assumption~\ref{hyp:cond:wellcond}.
Using \cite[Corollary 3.3]{wright2001effects}, the solution $(d_x, d_y)$ of the
condensed KKT system \eqref{eq:kkt:condensed} in exact arithmetic satisfies
$(d_x, d_y) = O(\Xi)$.
In \eqref{eq:kkt:condensed}, the RHS $\bar{r}_1$ and $\bar{r}_2$
evaluate in floating-point arithmetic as
\begin{equation}
  \label{eq:cond:condensedrhs}
  \left\{
  \begin{aligned}
    \bar{r}_1 &= - \widehat{r}_1 + \widehat{H}^\top\big(\widehat{D}_{s} \widehat{r}_{4} - \widehat{r}_{2} \big) \;, \\
     \bar{r}_2 &= -\widehat{r}_3 \; .
  \end{aligned}
  \right.
\end{equation}
Using basic floating-point arithmetic, we get
$\widehat{r}_1 = r_1 + O(\epstol)$,
$\widehat{r}_3 = r_3 + O(\epstol)$,
$\widehat{r}_4 = r_4 + O(\epstol)$.
The error in the right-hand-side $r_2$ is impacted by the term $\mu S^{-1}e$:
under Assumption~\ref{hyp:cond:wellcond}, it impacts differently
the active and inactive components:
$\widehat{r}_{2,\cactive}= r_{2,\cactive} + O(\epstol)$ and
$\widehat{r}_{2,\cinactive}= r_{2,\cinactive} + O(\Xi \epstol)$.
Similarly, the diagonal matrix $\widehat{D}_s$ retains full accuracy only
w.r.t. the inactive components: $\widehat{D}_{s,\cactive} = D_{s,\cactive} + O(\Xi^{-1} \epstol)$
and $\widehat{D}_{s,\cinactive} = D_{s,\cinactive} + O(\Xi \epstol)$.

\subsubsection{Solution with HyKKT}
We analyze the accuracy achieved when we solve the condensed system~\eqref{eq:kkt:condensed}
using HyKKT,
and show that the error remains reasonable even for large values of
the regularization parameter $\gamma$.

\paragraph{Initial right-hand-side.}
Let $\widehat{s}_\gamma := \bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2$.
The initial right-hand side in~\eqref{eq:kkt:schurcomplhykkt}
is evaluated as
$\widehat{r}_\gamma :=\widehat{G} \widehat{K}_\gamma^{-1} \widehat{s}_\gamma - \bar{r}_2$.
The following proposition shows that despite an expression involving the inverse
of the ill-conditioned condensed matrix $K_\gamma$, the error made in $r_\gamma$
is bounded only by the machine precision $\epstol$.

\begin{proposition}
In floating point arithmetic, the error in the right-hand-side $\Delta \widehat{r}_\gamma$ satisfies:
\begin{equation}
  \label{eq:cond:errorrgamma}
  \Delta \widehat{r}_\gamma = -\Delta \bar{r}_2 + \widehat{G} \widehat{K}_\gamma^{-1} \Delta s_\gamma = O(\epstol) \;.
\end{equation}
\end{proposition}
\begin{proof}
Using \eqref{eq:cond:condensedrhs}, we have
\begin{equation*}
  \label{eq:cond:boundderivationhykkt}
  \begin{aligned}
  \bar{r}_1 + \gamma \widehat{G}^\top \bar{r}_2 &=
- \widehat{r}_1 + \gamma \widehat{G}^\top \widehat{r}_3+ \widehat{H}^\top\big(\widehat{D}_{s} \widehat{r}_{4} - \widehat{r}_{2} \big) \\
&=  -
\underbrace{\widehat{r}_1}_{O(\epstol)} +
\underbrace{
\widehat{H}_{\cinactive}^\top\big(\widehat{D}_{s,\cinactive} \widehat{r}_{4,\cinactive} - \widehat{r}_{2,\cinactive} \big)}_{O(\Xi \epstol)}
+ \underbrace{\widehat{A}^\top \begin{bmatrix}
  \widehat{D}_{s,\cactive} \widehat{r}_{4,\cactive} - \widehat{r}_{2,\cactive}  \\
  \gamma \widehat{r}_3
\end{bmatrix}}_{O(\Xi^{-1}\epstol)} \; .
  \end{aligned}
\end{equation*}
The error decomposes as $\Delta s_\gamma = Y \Delta s_Y  + Z \Delta s_Z
= U_L \Delta s_L + U_S \Delta s_S$.
We have $\Delta s_Y = O(\Xi^{-1} \epstol)$ and $\Delta s_Z = O(\epstol)$.
Using \eqref{eq:cond:invariantsubpsace}, we deduce
$\Delta s_L = U_L^\top \Delta s_\gamma = O(\Xi^{-1} \epstol)$ and
$\Delta s_S = U_S^\top \Delta s_\gamma = O(\epstol)$.
Using \eqref{eq:cond:boundinvariantsubspace} and \eqref{eq:cond:inversecondensed},
the error in the large space $\Delta s_L$ annihilates in the backsolve:
\begin{equation}
  \label{eq:cond:boundhykkt1}
  K_\gamma^{-1} \Delta s_\gamma = U_L \Sigma_L^{-1} \Delta s_L + U_S \Sigma_S^{-1} \Delta s_S  = O(\epstol)
  \; .
\end{equation}
Finally, using \eqref{eq:cond:invperturbed}, we get
\begin{equation}
  \widehat{G} \widehat{K}_\gamma^{-1} \Delta s_\gamma \approx
  \widehat{G} (I - K_\gamma^{-1}\Delta K_\gamma) K_\gamma^{-1} \Delta s_\gamma \; .
\end{equation}
Using \eqref{eq:cond:boundhykkt1}, the first term is $\widehat{G} K_\gamma^{-1} \Delta s_\gamma = O(\epstol)$.
We have in addition
\begin{equation}
  G K_\gamma^{-1}\Delta K_\gamma (K_\gamma^{-1} \Delta s_\gamma)  =
  \big[ G U_L \Sigma_L^{-1} \Gamma_L + G U_S \Sigma_S^{-1} \Gamma_S \big] (K_\gamma^{-1} \Delta s_\gamma) \; .
\end{equation}
Using again \eqref{eq:cond:invariantsubpsace}:
$G U_L = G Y + O(\Xi)$ and $G U_S = O(\Xi)$. Hence
$G U_L \Sigma_L^{-1} \Gamma_L = O(1)$ and $G U_S \Sigma_S^{-1} \Gamma_S = O(1)$.
Using \eqref{eq:cond:rhserrorfull}, we have $K_\gamma^{-1} \Delta G^\top = O(\epstol)$,
implying $\Delta G K_\gamma^{-1} \Delta K_\gamma (K_\gamma^{-1} \Delta s_\gamma) = O(\Xi^{-1} \epstol^{2})$.
Assumption~\ref{hyp:cond:wellcond} implies that $\Xi^{-1} \epstol^2 \ll \epstol$,
proving \eqref{eq:cond:errorrgamma}.
\end{proof}

\paragraph{Schur-complement operator.}
The solution of the system~\eqref{eq:kkt:schurcomplhykkt}
involves the Schur complement $S_\gamma = G K_\gamma^{-1} G^\top$.
We show that the Schur complement
has a specific structure that limits the loss of accuracy
in the conjugate gradient algorithm.

\begin{proposition}
  Suppose the current primal-dual iterate $(p, v)$ satisfies Assumption~\ref{hyp:cond:wellcond}.
  In exact arithmetic,
  \begin{equation}
    S_\gamma = GY \, \Sigma_L^{-1} \, Y^\top G^\top + O(\Xi^2) \; .
  \end{equation}
\end{proposition}
\begin{proof}
  Using \eqref{eq:cond:inversecondensed}, we have
  \begin{equation}
    G K_\gamma^{-1} G^\top =
    G U_L \Sigma_L^{-1} U_L^\top G^\top + G U_S \Sigma_S^{-1} U_S^\top G^\top \;.
  \end{equation}
  Using \eqref{eq:cond:invariantsubpsace}, we have $G U_L = GY + O(\Xi)$,
  and $G = O(1)$, implying
  \begin{equation}
    G U_L \Sigma_L^{-1} U_L^\top G^\top = G Y  \Sigma_L^{-1} Y^\top G^\top + O(\Xi^2) \; .
  \end{equation}
  Using again \eqref{eq:cond:invariantsubpsace}, we have $G U_S = O(\Xi)$.
  Hence, $G U_S \Sigma_S^{-1} U_S^\top G^\top = O(\Xi^2)$,
  concluding the proof.
\end{proof}
We adapt the previous proposition to bound the error made when evaluating
$\widehat{S}_\gamma$ in floating-point arithmetic.
\begin{proposition}
  Suppose the current primal-dual iterate $(p, v)$ satisfies Assumption~\ref{hyp:cond:wellcond}.
  In floating-point arithmetic,
  \begin{equation}
    \label{eq:cond:errorSgamma}
    \widehat{S}_\gamma = S_\gamma + O(\epstol) \; .
  \end{equation}
\end{proposition}
\begin{proof}
  We denote $\widehat{G} = G + \Delta G$ (with $\Delta G = O(\epstol)$). Then
  \begin{equation}
    \begin{aligned}
      \widehat{S}_\gamma &= \widehat{G} \widehat{K}_\gamma^{-1} \widehat{G}^\top \; , \\
                         &\approx (G + \Delta G)\big(K_\gamma^{-1} - K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1}\big)(G + \Delta G)^\top \;, \\
                    &\approx S_\gamma - G \big(K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1} \big)G^\top
                    + K_\gamma^{-1} \Delta G^\top + \Delta G K_\gamma^{-1} \; .
    \end{aligned}
  \end{equation}
  The second line is given by \eqref{eq:cond:invperturbed},
  the third by neglecting the second-order errors.
  Using \eqref{eq:cond:rhserrorfull}, we get $K_\gamma^{-1} \Delta G^\top = O(\epstol)$
  and $\Delta G K_\gamma^{-1} = O(\epstol)$.
  Using \eqref{eq:cond:inversecondensederror}, we have
  \begin{equation*}
    G \big(K_\gamma^{-1}\Delta K_\gamma K_\gamma^{-1} \big)G^\top =
G U_L \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}U_L^\top G^\top  +
G U_S \Sigma_S^{-1} \Gamma_S \Sigma_S^{-1}U_S^\top  G^\top \;.
  \end{equation*}
  Using \eqref{eq:cond:invariantsubpsace}, we have $G U_S = O(\Xi)$.
  As $\Sigma_S^{-1} = \Theta(1)$ and $\Gamma_S = O(\Xi^{-1} \epstol)$, we
  get
  $G U_S \Sigma_S^{-1} \Gamma_S \Sigma_S^{-1}U_S^\top  G^\top = O(\Xi \epstol)$.
  Finally, as $\Sigma_L^{-1} = \Theta(\Xi)$ and $G U_L = GY + O(\Xi)$,
  we have
  \begin{equation}
    G U_L \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}U_L^\top G^\top =
    G Y \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}Y^\top G^\top + O(\Xi^2 \epstol) \; .
  \end{equation}
  We conclude the proof by using
  $G Y \Sigma_L^{-1} \Gamma_L \Sigma_L^{-1}Y^\top G^\top = O(\Xi \epstol)$.
\end{proof}
The two error bounds \eqref{eq:cond:errorrgamma} and
\eqref{eq:cond:errorSgamma} ensure that we can solve
\eqref{eq:kkt:schurcomplhykkt} using a conjugate gradient
algorithm, as the errors remain limited in floating-point
arithmetic.

\subsubsection{Solution with Lifted KKT system}
The equality relaxation strategy used in LiftedKKT
removes the equality constraints from the optimization problems, simplifying
the solution of the condensed KKT system to \eqref{eq:liftedkkt}.
The active Jacobian $A$ reduces to the active inequalities $A = H_{\cactive}$,
and we recover the original setting presented in \cite{wright1998ill}.
Using the same arguments than in \eqref{eq:cond:boundderivationhykkt},
the error in the right-hand-side is bounded by $O(\Xi^{-1} \epstol)$ and is in the
range space of the active Jacobian $A$. Using \eqref{eq:cond:inversecondensed},
we can show that the absolute error on $\widehat{d}_x$ is bounded by
$O(\Xi \epstol)$. That implies the descent direction $\widehat{d}_x$ retains
full relative precision close to optimality.
In other words, we can refine the solution returned by the Cholesky solver accurately using
Richardson iterations.

\subsubsection{Summary}
Numerically, the primal-dual step $(\widehat{d}_x, \widehat{d}_y)$
is computed only with an (absolute) precision $\varepsilon_{K}$,
greater than the machine precision $\epstol$ (for HyKKT, $\varepsilon_K$
is the absolute tolerance of the \CG algorithm, for LiftedKKT the
absolute tolerance of the iterative refinement algorithm).

The errors $\widehat{d}_x - d_x = O(\varepsilon_K)$ and
$\widehat{d}_y - d_y = O(\varepsilon_K)$ propagate further in $(\widehat{d}_s, \widehat{d}_z)$.
According to \eqref{eq:kkt:condensed}, we have $\widehat{d}_s = - \widehat{r}_4 - \widehat{H} \widehat{d}_x$.
By continuity, $\widehat{H} = H + O(\epstol)$ and $\widehat{r}_4 = r_4 + O(\epstol)$, implying
\begin{equation}
  \widehat{d}_s = d_s + O(\varepsilon_K) \; .
\end{equation}
Eventually, we obtain $\widehat{d}_z = - \widehat{r}_2 - \widehat{D}_s \widehat{d}_s$,
giving the following bounds for the errors in the inactive and active components:
\begin{equation}
  \begin{aligned}
     \widehat{d}_{z,\cactive} &= -\widehat{r}_{2,\cactive} - \widehat{D}_{s,\cactive} \widehat{d}_{s,\cactive}
    = d_{z,\cactive} + O(\varepsilon_K \Xi^{-1}) \;,\\
                              \widehat{d}_{z,\cinactive} &= -\widehat{r}_{2,\cinactive} - \widehat{D}_{s,\cinactive} \widehat{d}_{s,\cinactive}
                               = d_{z,\cinactive} + O(\varepsilon_K \Xi) \; .
  \end{aligned}
\end{equation}
Most of the error arises in the active components $\widehat{d}_{z,\cactive}$.
To limit the loss of accuracy, the algorithm has to decrease the absolute precision $\varepsilon_K$
as we are approaching to a local optimum.
The impact remains limited if we have only few active inequality constraints.


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
