\section{Solving KKT systems on the GPU}
The GPU has emerged as the new prominent landscape for numerical computing.
GPUs employ a SIMD formalism that yields excellent throughput for parallelizing small-scale operations.
However, their utility remains limited when computational algorithms require global communication.
Sparse factorization algorithms, which heavily rely on numerical pivoting, pose significant challenges for implementation on GPUs due to this limitation. Previous research has demonstrated that GPU-based linear solvers significantly lag behind their CPU counterparts \cite{tasseff2019exploring,swirydowicz2021linear}.
One emerging strategy to address this challenge is to utilize sparse factorization techniques that do not necessitate numerical pivoting \cite{regev2023hykkt,shin2023accelerating},
by leveraging the structure of the condensed KKT system \eqref{eq:kkt:condensed}.

\subsection{Golub \& Greif strategy}
\label{sec:kkt:golubgreif}
The Golub \& Greif~\cite{golub2003solving} strategy reformulates the KKT system
using an Augmented Lagrangian formulation.
It has been recently revisited in \cite{regev2023hykkt}
to solve the condensed KKT system~\eqref{eq:kkt:condensed} on the GPU.
The trick is to reformulate the condensed KKT system \eqref{eq:kkt:condensed} in the equivalent form
\begin{equation}
  \label{eq:kkt:hykkt}
  \begin{bmatrix}
    K_\gamma & G^\top \\
    G & 0
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\ d_y
  \end{bmatrix}
  =
  \begin{bmatrix}
    \bar{r}_1 + \gamma G^\top \bar{r}_2 \\
    \bar{r}_2
  \end{bmatrix} \; ,
\end{equation}
where we have introduced the regularized matrix $K_\gamma := K + \gamma G^\top G$.
If SOSC holds, the matrix $K_\gamma$ is positive definite for a large-enough parameter $\gamma$.
\begin{proposition}[\cite{regev2023hykkt}, Theorem 1, p.7]
  \label{prop:kkt:hykkt:pd}
  We suppose $G$ is full row rank. Then there exists $\gamma_{min}$
  such that, for all $\gamma > \gamma_{min}$, $K_\gamma$ is positive definite
  if and only if the reduced Hessian $Z^\top K Z$ is positive definite.
\end{proposition}
If $\gamma$ is large enough, we can prove that the conditioning
of $K_\gamma$ increases linearly with $\gamma$.
\begin{proposition}[\cite{regev2023hykkt}, Theorem 2, p.7]
  \label{prop:kkt:hykkt:cond}
  We suppose $G$ is full row rank. Then there exists $\gamma_{max}
  \geq \gamma_{min}$ such that for all $\gamma \geq \gamma_{max}$,
  $\cond(K_\gamma)$ increases linearly with $\gamma$.
\end{proposition}

The linear solver HyKKT~\cite{regev2023hykkt}
leverages the positive definiteness of $K_\gamma$ and solves
\eqref{eq:kkt:hykkt} using a hybrid direct-iterative method
that uses the following steps:
\begin{enumerate}
  \item Assemble $K_\gamma$ and factorize it using sparse Cholesky ;
  \item Solve the Schur complement of \eqref{eq:kkt:hykkt} using a conjugate gradient (CG)
    algorithm to recover the dual descent direction:
    \begin{equation}
      \label{eq:kkt:schurcomplhykkt}
      (G K_\gamma^{-1} G^\top) d_y = G K_\gamma^{-1} (\bar{r}_1 + \gamma G^\top \bar{r}_2) - \bar{r}_2 \; .
    \end{equation}
  \item Solve the system $K_\gamma d_x = \bar{r}_1 + \gamma G^\top \bar{r}_2 - G^\top d_y$
    to recover the primal descent direction.
\end{enumerate}
The method uses a sparse Cholesky factorization along with the conjugate gradient (CG) algorithm \cite{hestenes-stiefel-1952}.
The sparse Cholesky factorization has the advantage of being stable without
numerical pivoting, rendering the algorithm tractable on a GPU.
Each CG iteration requires the application of sparse triangular solves with the
factors of $K_\gamma$, operations that can be numerically demanding. For that reason,
HyKKT is efficient only if the CG solver converges in a small number of iterations.
Fortunately, the eigenvalues of the Schur-complement $S_\gamma := G K_\gamma^{-1} G^\top$
all converge to $\frac{1}{\gamma}$ as we increase the regularization parameter
$\gamma$ (\cite[Theorem 4]{regev2023hykkt}), meaning that $\lim_{\gamma \to \infty} \cond(S_\gamma) = 1$.
Because the convergence of the CG method depends on the number of distinct eigenvalues of $S_{\gamma}$, when $\gamma$ increases, the eigenvalues of $S_{\gamma}$ become more clustered, resulting in fewer iterations being required to solve \eqref{eq:kkt:schurcomplhykkt}.

\subsection{Lifted KKT System strategy}
\label{sec:kkt:sckkt}

We observe in \eqref{eq:kkt:condensed} that if all the constraints in the problem are inequalities, the system in \eqref{eq:kkt:condensed} becomes a $n \times n$ system which is guaranteed to be positive definite if the primal regularization parameter $\delta_x$ is adequately large. Furthermore, this parameter can be chosen dynamically using the inertia information of the system in \eqref{eq:kkt:condensed}. This motivates the Lifted KKT System strategy: we approximate the equalities with lifted inequalities.

In particular, we relax the equality constraints in \eqref{eq:problem} using a small relaxation parameter $\tau > 0$ (chosen based on the numerical tolerance of the optimization solver), and solve the relaxed problem
\begin{equation}
  \label{eq:problemrelaxation}
    \min_{x \in \mathbb{R}^n} \;  f(x)
\quad \text{subject to}\quad
     - \tau \leq g(x) \leq \tau \;,~  h(x) \leq 0  \; .
\end{equation}
The problem~\eqref{eq:problemrelaxation} has only inequality constraints. After introducing slack variables, the condensed KKT system
\eqref{eq:kkt:condensed} reduces to
\begin{equation}
  \label{eq:liftedkkt}
    K_k d_x = - r_1 - H_k^\top(D_s r_4 - r_2) \; .
\end{equation}
Using the standard inertia correction procedure, the parameter $\delta_x$ is set to a value high enough to ensure that  $K_k$ is positive definite. Therefore, $K_k$ can be factorized with Cholesky factorization, satisfying the key requirement for the implementation on the GPU. The relaxation causes error in the final solution, but the error is in the same order of the solver tolerance, and thus, do not significantly deteriorates the solution quality.

While this method can be implemented with small modification in the optimization solver, the presence of tight inequality in \eqref{eq:problemrelaxation} causes severe ill-conditioning throughout the IPM iterations. Thus,
choosing the solver tolerance appropriately is necessary to get a reliable convergence behavior.


\subsection{Discussion}
We have introduced two algorithms to solve
the KKT system on the GPU. On the contrary to classical CPU algorithms,
the two methods do not require computing a sparse \lblt factorization of the KKT
system by using alternate reformulation based on the condensed KKT
system~\eqref{eq:kkt:condensed}.

The first strategy uses an augmented Lagrangian formulation
of the KKT system. It requires a sparse Cholesky factorization (stable without numerical
pivoting, hence favorable for the GPU) and uses it to solve the Schur-complement system
using the CG algorithm. The performance of the method depends on a parameter $\gamma$, that
has to be tuned independently. The larger the $\gamma$, the faster is the convergence
in the CG algorithm, but the worst is the accuracy of the linear solve. Hence, a trade-off
has to be found. The second method uses an equality relaxation strategy to
solve the condensed KKT system~\eqref{eq:kkt:condensed} directly, parameterized
by a relaxation parameter $\tau$. It just requires
a sparse Cholesky solver to factorize~\eqref{eq:kkt:condensed}. However, the method
solves a relaxed problem, limiting the accuracy
of the solution.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
