\section{Solving KKT systems on the GPU}
The GPU has emerged as one of the new prominent landscape
for numerical computing. GPUs are using a SIMD formalism
that yields excellent throughput for parallelizing small-scale
operations, but their use remains limited as soon as the computational
pattern becomes more complex. With their heavy use of numerical
pivoting, sparse factorization algorithms are notoriously hard to implement
on a GPU for that exact reason. In particular, previous works have
shown that GPU-based linear solvers lag far behind their equivalent
on the CPU to factorize the system~\eqref{eq:kkt:augmented}
\cite{tasseff2019exploring,swirydowicz2021linear}. One strategy
has emerged to overcome this challenge:
use a sparse factorization that does not require numerical pivoting~\cite{regev2023hykkt,shin2023accelerating}.
Interestingly, the two methods are leveraging the condensed KKT system~\eqref{eq:kkt:condensed}.

\subsection{Golub \& Greif strategy}
\label{sec:kkt:golubgreif}
The Golub \& Greif~\cite{golub2003solving} strategy reformulates the KKT system
using an Augmented Lagrangian formulation.
It has been recently revisited in \cite{regev2023hykkt}
to solve the condensed KKT system~\eqref{eq:kkt:condensed} on the GPU.

The trick is to reformulate the condensed KKT system \eqref{eq:kkt:condensed} in the equivalent form
\begin{equation}
  \label{eq:kkt:hykkt}
  \begin{bmatrix}
    K_\gamma & G^\top \\
    G & 0
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\ d_y
  \end{bmatrix}
  =
  \begin{bmatrix}
    \hat{r}_1 + \gamma G^\top \hat{r}_2 \\
    \hat{r}_2
  \end{bmatrix}
\end{equation}
where we have introduced the regularized matrix $K_\gamma := K + \gamma G^\top G$.
If SOSC holds, the matrix $K_\gamma$ is positive definite for a large-enough parameter $\gamma$.
\begin{proposition}[\cite{regev2023hykkt}, Theorem 1, p.7]
  \label{prop:kkt:hykkt:pd}
  We suppose $G$ is full row rank. Then there exists $\gamma_{min}$
  such that, for all $\gamma > \gamma_{min}$, $K_\gamma$ is positive definite
  if and only if the reduced Hessian $Z^\top K Z$ is positive definite.
\end{proposition}
If $\gamma$ is getting too large, we can prove that the conditioning
of $K_\gamma$ increases linearly with $\gamma$ past a certain threshold.
\begin{proposition}[\cite{regev2023hykkt}, Theorem 2, p.7]
  \label{prop:kkt:hykkt:cond}
  We suppose $G$ is full row rank. Then there exists $\gamma_{max}
  \geq \gamma_{min}$ such that for all $\gamma \geq \gamma_{max}$,
  $\cond(K_\gamma)$ increases linearly with $\gamma$.
\end{proposition}

The linear solver HyKKT~\cite{regev2023hykkt}
leverages the positive definiteness of $K_\gamma$ and solves
\eqref{eq:kkt:hykkt} using a hybrid direct-iterative method:
\begin{enumerate}
  \item Assemble $K_\gamma$ and factorize it using sparse Cholesky ;
  \item Solve the Schur complement of \eqref{eq:kkt:hykkt} using a conjugate gradient (CG)
    algorithm to recover the dual descent direction:
    \begin{equation}
      \label{eq:kkt:schurcomplhykkt}
      (G K_\gamma^{-1} G^\top) d_y = G K_\gamma^{-1} (\hat{r}_1 + \gamma G^\top \hat{r}_2) - \hat{r}_2 \; .
    \end{equation}
  \item Solve the system $K_\gamma d_x = \hat{r}_1 + \gamma G^\top \hat{r}_2 - G^\top d_y$
    to get the primal descent direction.
\end{enumerate}
The method uses a sparse Cholesky factorization used in conjunction with a CG algorithm.
The sparse Cholesky factorization has the advantage of being stable without
numerical pivoting, rendering the algorithm tractable on a GPU.
Each CG iteration requires the application of a backsolve with the
factors of $K_\gamma$, an operation that can be numerically demanding. For that reason, the
hybrid method HyKKT requires a small number of iterations to be numerically efficient.
Fortunately, the eigenvalues of the Schur-complement $G K_\gamma^{-1} G^\top$
converge to a single value as we increase the regularization parameter
$\gamma$ (\cite[Theorem 4]{regev2023hykkt}). The larger the $\gamma$, the less iterations is required in the CG algorithm.


\subsection{A novel equality relaxation strategy (SS)}
\label{sec:kkt:sckkt}

TODO

\subsection{Discussion}
We have introduced two algorithms to solve
the KKT system on the GPU. On the contrary to classical CPU algorithms,
the two methods do not require computing a sparse LBL factorization of the KKT
system by using alternate reformulation based on the condensed KKT
system~\eqref{eq:kkt:condensed}.

The first strategy uses an augmented Lagrangian formulation
of the KKT system. It requires a sparse Cholesky factorization (stable without numerical
pivoting, hence favorable for the GPU) and uses it to solve the Schur-complement system
using a CG algorithm. The performance of the method depends on a parameter $\gamma$, that
has to be tuned independently. The larger the $\gamma$, the faster is the convergence
in the CG algorithm, but the worst is the accuracy of the linear solve. Hence, a trade-off
has to be found. The second method uses an equality relaxation strategy to
solve the condensed KKT system~\eqref{eq:kkt:condensed} directly. It just requires
a sparse Cholesky solver to factorize~\eqref{eq:kkt:condensed}. However, the method
solves a relaxation of the original problem~\eqref{eq:problem}, limiting the accuracy
of the solution.

