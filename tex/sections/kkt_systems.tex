\section{Solving KKT systems on the GPU}
The GPU has emerged as a new prominent computing hardware not only for graphics-related but also for general-purpose computing.
GPUs employ a SIMD formalism that yields excellent throughput for parallelizing small-scale operations.
However, their utility remains limited when computational algorithms require global communication.
Sparse factorization algorithms, which heavily rely on numerical pivoting, pose significant challenges for implementation on GPUs. Previous research has demonstrated that GPU-based linear solvers significantly lag behind their CPU counterparts \cite{tasseff2019exploring,swirydowicz2021linear}.
One emerging strategy is to utilize sparse factorization techniques that do not necessitate numerical pivoting \cite{regev2023hykkt,shin2023accelerating}
by leveraging the structure of the condensed KKT system \eqref{eq:kkt:condensed}.
We present two alternative methods to solve \eqref{eq:kkt:condensed}.
On the one hand, HyKKT is introduced in \S\ref{sec:kkt:golubgreif} and uses the hybrid
strategy of Golub \& Greif~\cite{golub2003solving,regev2023hykkt}.
On the other hand, LiftedKKT~\cite{shin2023accelerating} uses an equality relaxation strategy and
is presented in \S\ref{sec:kkt:sckkt}.

\subsection{Golub \& Greif strategy}
\label{sec:kkt:golubgreif}
The Golub \& Greif~\cite{golub2003solving} strategy reformulates the KKT system
using an Augmented Lagrangian formulation.
It has been recently revisited in \cite{regev2023hykkt}
to solve the condensed KKT system~\eqref{eq:kkt:condensed} on the GPU.
For a dual regularization $\delta_c = 0$,
the trick is to reformulate the condensed KKT system \eqref{eq:kkt:condensed} in an equivalent form
\begin{equation}
  \label{eq:kkt:hykkt}
  \begin{bmatrix}
    K_\gamma & G^\top \\
    G & 0
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\ d_y
  \end{bmatrix}
  =
  \begin{bmatrix}
    \bar{r}_1 + \gamma G^\top \bar{r}_2 \\
    \bar{r}_2
  \end{bmatrix} \; ,
\end{equation}
where we have introduced the regularized matrix $K_\gamma := K + \gamma G^\top G$.
We note by $Z$ a basis of the null-space of the Jacobian $G$.
Using a classical result from~\cite{debreu1952definite},
if $G$ is full row-rank then there
exists a threshold value $\underline{\gamma}$ such that
for all $\gamma > \underline{\gamma}$, the reduced Hessian $Z^\top K Z$
is positive definite if and only if $K_\gamma$ is positive definite.
Using the Sylvester's law of inertia stated in \eqref{eq:ipm:inertia}, we deduce
that for $\gamma > \underline{\gamma}$, $\inertia(K_2) = (n + m_i, m_e +m_i, 0)$
implies that $K_\gamma$ is positive definite.

If $\gamma$ is large enough, we can prove that the conditioning
of $K_\gamma$ increases linearly with $\gamma$.
\begin{proposition}[\cite{regev2023hykkt}, Theorem 2, p.7]
  \label{prop:kkt:hykkt:cond}
  We suppose $G$ is full row rank. Then there exists $\overline{\gamma}
  \geq \underline{\gamma}$ such that for all $\gamma \geq \overline{\gamma}$,
  $\cond(K_\gamma)$ increases linearly with $\gamma$.
\end{proposition}

The linear solver HyKKT~\cite{regev2023hykkt}
leverages the positive definiteness of $K_\gamma$ to solve
\eqref{eq:kkt:hykkt} using a hybrid direct-iterative method
that uses the following steps:
\begin{enumerate}
  \item Assemble $K_\gamma$ and factorize it using sparse Cholesky ;
  \item Solve the Schur complement of \eqref{eq:kkt:hykkt} using a conjugate gradient (\CG)
    algorithm to recover the dual descent direction:
    \begin{equation}
      \label{eq:kkt:schurcomplhykkt}
      (G K_\gamma^{-1} G^\top) d_y = G K_\gamma^{-1} (\bar{r}_1 + \gamma G^\top \bar{r}_2) - \bar{r}_2 \; .
    \end{equation}
  \item Solve the system $K_\gamma d_x = \bar{r}_1 + \gamma G^\top \bar{r}_2 - G^\top d_y$
    to recover the primal descent direction.
\end{enumerate}
The method uses a sparse Cholesky factorization along with the conjugate gradient (\CG) algorithm \cite{hestenes-stiefel-1952}.
The sparse Cholesky factorization has the advantage of being stable without
numerical pivoting, rendering the algorithm tractable on a GPU.
Each \CG iteration requires the application of sparse triangular solves with the
factors of $K_\gamma$, operations that can be numerically demanding. For that reason,
HyKKT is efficient only if the \CG solver converges in a small number of iterations.
Fortunately, the eigenvalues of the Schur-complement $S_\gamma := G K_\gamma^{-1} G^\top$
all converge to $\frac{1}{\gamma}$ as we increase the regularization parameter
$\gamma$ \cite[Theorem 4]{regev2023hykkt}, implying that $\lim_{\gamma \to \infty} \cond(S_\gamma) = 1$.
Because the convergence of the \CG method depends on the number of distinct eigenvalues of $S_{\gamma}$,
the larger the $\gamma$, the faster the convergence of
the \CG algorithm in \eqref{eq:kkt:schurcomplhykkt}.
\CR \cite{hestenes-stiefel-1952} and \CAR \cite{montoison-orban-saunders-2023} can also be used as an alternative to \CG.
Although we observe similar performance, these methods ensure a monotonic decrease in the residual norm of \eqref{eq:kkt:schurcomplhykkt} at each iteration.

\subsection{Equality relaxation strategy}
\label{sec:kkt:sckkt}

If the initial problem~\eqref{eq:problem} has only inequality constraints,
we keep only the $(1,1)$ block in the system \eqref{eq:kkt:condensed}.
Using the relation~\eqref{eq:ipm:inertia}, the system
is guaranteed to be positive definite if the primal regularization parameter $\delta_x$ is adequately large.
Furthermore, the parameter $\delta_x$ is chosen dynamically using the inertia information of the system in \eqref{eq:kkt:condensed}. This motivates the Lifted KKT System strategy: we approximate the equalities with lifted inequalities.
For a small relaxation parameter $\tau > 0$ (chosen based on the numerical tolerance of the optimization solver
$\varepsilon_{tol}$), we solve the relaxed problem
\begin{equation}
  \label{eq:problemrelaxation}
    \min_{x \in \mathbb{R}^n} \;  f(x)
\quad \text{subject to}\quad
     - \tau \leq g(x) \leq \tau \;,~  h(x) \leq 0  \; .
\end{equation}
The problem~\eqref{eq:problemrelaxation} has only inequality constraints. After introducing slack variables, the condensed KKT system
\eqref{eq:kkt:condensed} reduces to
\begin{equation}
  \label{eq:liftedkkt}
    K_\tau \,d_x = - r_1 - H_\tau^\top(D_H r_4 - C r_2) \; .
\end{equation}
Using the standard inertia correction procedure, the parameter $\delta_x$ is set to a value high enough to ensure that  $K_\tau$ is positive definite. Therefore, $K_\tau$ can be factorized with a Cholesky decomposition, satisfying the key requirement of stable pivoting for the implementation on the GPU. The relaxation causes error in the final solution, but the error is in the same order of the solver tolerance, and thus, do not significantly deteriorates the solution quality for small $\varepsilon_{tol}$.

While this method can be implemented with small modification in the optimization solver, the presence of tight inequality in \eqref{eq:problemrelaxation} causes severe ill-conditioning throughout the IPM iterations. Thus,
using an accurate iterative refinement algorithm is necessary to get a reliable convergence behavior.

\subsection{Discussion}
We have introduced two algorithms to solve
KKT systems on the GPU. On the contrary to classical implementations,
the two methods do not require computing a sparse \lblt factorization of the KKT
system and use instead alternate reformulations based on the condensed KKT
system~\eqref{eq:kkt:condensed}. Both strategies
rely on a Cholesky factorization: HyKKT factorizes a positive
definite matrix $K_\gamma$ obtained with an Augmented Lagrangian strategy
whereas Lifted KKT factorizes a positive definite matrix $K_\tau$
after using an equality relaxation strategy.
We will see in the next section that the ill-conditioned matrices $K_\gamma$ and $K_\tau$
have a specific structure that limits the loss of accuracy in IPM.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
