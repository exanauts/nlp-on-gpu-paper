\section{Introduction}
Graphical Processing Units (GPUs) have become the most popular parallel computing architecture
for scientific computing, driven by its success in machine learning.
GPUs offer massive parallel computing capability for applications
that can exploit coarse-grain parallelism and high-memory bandwidth.
As such, the library CUDA has revolutionized High Performance Computing (HPC)
by bringing to a general audience parallel capabilities that were previously
exclusive to expensive supercomputers.
In the post-Moore law era, GPUs
are getting more power-efficient than CPUs to run parallel workloads, as they require
fewer transistors to process multiple tasks in parallel.

Despite their tremendous impacts on machine learning applications, the
adoption of GPUs in the mathematical programming community has
remained limited. Most of the optimization solvers have been developed
in the 1990s and have been heavily optimized for CPU architectures.
The use of GPUs has been hindered by the poor performance of
sparse matrix factorization routines, an algorithm difficult to parallelize
on SIMD architectures. However, the landscape has changed in
recent years thanks to a few breakthroughs.
(1) The performance of sparse matrix operations has been improved
in the CUDA library, mostly due to the novel tensor-cores used
in recent GPUs~\cite{markidis2018nvidia}.
(2) There is an increased interest in solving optimization
problems in batch, for problems sharing the same
structure but with different parameters~\cite{amos2017optnet,pineda2022theseus}.
(3) GPUs are offering unmatched performance for automatic differentiation,
both for machine learning~\cite{jax2018github} and scientific computing applications \cite{enzyme2021}.
Most engineering problems are formulated with a limited
number of common patterns repeated throughout the model: once
identified, we can evaluate them in parallel in a SIMD formalism
and achieve near speed-of-light performance~\cite{shin2023accelerating}.
(4) With the emergence of new exascale architectures, 
GPUs have become the primary driver to achieve performance in
supercomputing.

\subsection{Current state-of-the-art}
For all the reasons listed before, there is an increasing interest
to solve optimization problems on the GPU.

\paragraph{GPU for mathematical programming.}
The machine learning community has been a strong advocate for porting
mathematical optimization on the GPU. One of the most promising
applications is embedding mathematical programs inside neural networks,
a task that requires batching the solution of the optimization model
for the training algorithm to be
efficient~\cite{amos2017optnet,pineda2022theseus}.  This has led to
the development of prototype code solving thousands of (small)
optimization problems in parallel on the GPU.  However, it is not
trivial to adapt such code to solve large-scale optimization problems,
as the previous prototypes are reliant on dense linear solvers to
compute the descent direction.

For this reason, practitioners often resort to using first-order
methods on GPUs, leveraging level-1 and level-2 BLAS operations that
are more amenable to parallel computation.
First-order algorithms depend mostly on (sparse) matrix-vector operations, that run
very efficiently on modern GPUs. Hence, we can counterbalance
the relative inaccuracy of the first-order method by running more
iterations of the algorithm.
A recent breakthrough~\cite{lu2023cupdlp,lu2023cupdlp2} demonstrates
that a first-order algorithm can surpass the performance of Gurobi, a
commercial solver, in tackling large-scale linear programs. This
performance gain is made possible by executing the first-order
iterations solely on the GPU through an optimized codebase, thereby
solving the LP problems to $10^{-8}$ accuracy.


\paragraph{GPU for nonlinear programming.}
The success of first-order algorithms in classical mathematical programming
relies on the convexity of the problem. Thus, their approaches are nontrivial to replicate
in nonlinear programming. Most engineering problems embed complex
physical equations that are likely to break any convex structure in the problem.
Previous experiments on the OPF problem have shown that even a simple
algorithm as ADMM has trouble converging as soon as the convergence
tolerance is set below $10^{-3}$~\cite{kim2021leveraging}.

Thus, second-order methods continue to be a competitive option, particularly
for scenarios that require higher levels of accuracy and robust convergence.
Second-order algorithms require solving a Newton step at each
iteration, an operation relying on non-trivial sparse linear algebra operations.
The previous generation of GPU-accelerated sparse linear
solvers were lagging behind their CPU equivalents, as illustrated in
subsequent surveys~\cite{tasseff2019exploring,swirydowicz2021linear}.
Fortunately, sparse solvers on GPUs are getting increasingly better with the newest
generations of GPUs. In particular, NVIDIA has released in November 2023
a new sparse direct solver that implements different sparse factorization routines: {\tt cuDSS}. Our
preliminary benchmark has shown {\tt cuDSS} is significantly
faster than the sparse solvers in the previous generations.
Furthermore, variants of interior point methods have been proposed
that does not require the use of numerical pivoting.
As these algorithms do not require expensive numerical pivoting
operations, parallelized sparse solvers can be effectively exploited
within the solution algorithms.
Coupled with a GPU-accelerated automatic differentiation library and a
sparse Cholesky solver, these nonlinear programming solvers can solve
Optimal Power Flow (OPF) problems 10x faster than state-of-the-art
methods~\cite{shin2023accelerating}.

An alternative thread of research is investigating the solution of the Newton steps
on the GPU using iterative linear algebra methods.
Iterative methods often require non-trivial reformulation of the Newton step to avoid
ill-conditioned matrices, which has limited their use inside interior-point
algorithms. New results are giving promising outlooks for convex problems~\cite{ghannad2022linear},
but nonconvex problems often require an Augmented Lagrangian reformulation
to be tractable~\cite{cao2016augmented,regev2023hykkt}. In particular,
\cite{regev2023hykkt} presents an interesting use of the Golub and Greif
hybrid method~\cite{golub2003solving} to solve the KKT systems arising in
the interior-point methods, with promising results on the GPU.
The null-space method, also known as the reduced Hessian strategy,
is also a good candidate for solving KKT systems on the GPU.
The null-space method reduces the KKT system down to
a medium-sized dense matrix, which then can be factorized efficiently on the GPU.
Our previous research has shown that the method plays nicely with the interior-point
methods if the number of degrees of freedom in the problem is relatively small~\cite{pacaud2022condensed}.

\subsection{Contributions}
In this article, we assess the current capabilities of modern GPUs
to solve large-scale nonconvex nonlinear programs to optimality.
We focus on the two condensed-space methods
introduced respectively in~\cite{regev2023hykkt,shin2023accelerating}.
We re-use classical results from~\cite{wright1998ill} to show
that for both methods, the condensed matrix exhibits
structured ill-conditioning that limits the loss of accuracy in
the descent direction (provided the interior-point algorithm satisfies
some standard assumptions).
We implement both algorithms inside the GPU-accelerated solver MadNLP,
and leverage the GPU-accelerated automatic differentiation
backend ExaModels~\cite{shin2023accelerating}.
The interior-point algorithm runs entirely on the GPU, from
the evaluation of the model (using ExaModels) to the solution of
the KKT system (using a condensed-space method running on the GPU).
We add a wrapper to the sparse linear solver {\tt cuDSS} to solve
the condensed KKT systems with a cutting-edge sparse linear solver.
We assess the strengths
and weaknesses of both methods, in terms of accuracy and runtime.
We move beyond the classical OPF instances we used in our previous works
and include large-scale problems coming from the COPS
non-linear benchmark~\cite{dolan2004benchmarking}.
We compare the performance we obtain on the GPU with a state-of-the-art
method running on the CPU: our results show that the condensed-space
methods can solve large-scale OPF instances 10x faster using the GPU,
but the performance is more mitigated on the COPS benchmark.

\subsection{Notations}
By defaut, the norm $\|\cdot\|$ refers to the 2-norm.
We define the conditioning of a matrix $A$ as
$\cond(A) = \| A \| \|A^{-1} \|$.
For any real number $x$, we note $\widehat{x}$ its floating
point representation.
We note $\epstol$ the smallest positive number such that
$\widehat{x} \leq (1 + \tau) x$ for $|\tau| < \epstol$.
In double precision, $\epstol = 1.1 \times 10^{-16}$.
We use the following notations to proceed to our error analysis.
For $p \in \mathbb{N}$ and a positive variable $h$:
\begin{itemize}
  \item We write $x = O(h^p)$ if there exists a constant $b > 0$
    such that $\| x \| \leq b h^p$;
  \item We write $x = \Omega(h^p)$ if there exists a constant $a > 0$
    such that $\| x \| \geq a h^p$;
  \item We write $x = \Theta(h^p)$ if there exists two constants $0 < a < b$
    such that $a h^p \leq \| x \| \leq b h^p$.
\end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
