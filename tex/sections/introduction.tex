\section{Introduction}
Graphics processing units (GPUs) are driving the advancement of scientific computing, their most remarkable success being the capabilities to train and utilize large artificial intelligence (AI) models.
GPUs offer two practical advantages: (1) massive parallel computing capability for applications that can exploit coarse-grain parallelism and high-memory bandwidth and (2) power efficiency due to requiring fewer transistors to process multiple tasks in parallel utilizing ``single instruction, multiple data'' (SIMD) parallelism.

While GPUs have made significant strides in enhancing machine learning applications, their adoption in the mathematical programming community has been relatively limited.
This limitation stems primarily from the fact that most optimization solvers were developed in the 1990s and are heavily optimized for CPU architectures.
Additionally, the utilization of GPUs has been impeded by the challenges associated with sparse matrix factorization routines, which are inherently difficult to parallelize on SIMD architectures. Nevertheless, recent years have witnessed notable advancements that are reshaping this landscape.
\begin{enumerate}
  \item \textbf{Improved sparse matrix operations}: The performance of sparse matrix operations has seen substantial improvements in the CUDA library, largely attributed to the integration of novel tensor cores in recent GPUs~\cite{markidis2018nvidia}.
  \item \textbf{Interest in batch optimization}: There is a growing interest in solving parametric optimization problems in batch mode, for problems sharing the same structure but with different parameters~\cite{amos2017optnet,pineda2022theseus}.
  \item \textbf{Advancements in automatic differentiation}: GPUs offer unparalleled performance for automatic differentiation, benefiting both machine learning~\cite{jax2018github} and scientific computing applications \cite{enzyme2021}. Engineering problems often exhibit recurring patterns throughout the model. Once these patterns are identified, they can be evaluated in parallel within a SIMD framework, enabling near speed-of-light performance~\cite{shin2023accelerating}.
  \item \textbf{Role in exascale computing}: With the emergence of new exascale supercomputers (e.g., Frontier and Aurora), the capabilities to run on GPUs have become central for supercomputing.
\end{enumerate}

\subsection{Solving optimization problems on GPU: current state-of-the-art}
For all the reasons listed before, there is an increasing interest to solve optimization problems on GPUs.
We now summarize the previous work on solving classical---large-scale, sparse, constrained---mathematical programs on GPUs.

\paragraph{GPU for mathematical programming.}
The factorization of sparse matrices encountered within second-order optimization algorithms has been considered to be challenging  on GPUs.
For this reason, practitioners often has resorted to using first-order
methods on GPUs, leveraging level-1 and level-2 BLAS operations that
are more amenable to parallel computations.
First-order algorithms depend mostly on (sparse) matrix-vector operations that run
very efficiently on modern GPUs. Hence, we can counterbalance
the relative inaccuracy of the first-order method by running more
iterations of the algorithm.
A recent breakthrough~\cite{lu2023cupdlp,lu2023cupdlp2} based on the primal-dual hybrid gradient method has demonstrated
that a first-order algorithm can surpass the performance of Gurobi, a
commercial solver, in tackling large-scale linear programs. This
performance gain is made possible by executing the first-order
iterations solely on the GPU through an optimized codebase.

\paragraph{GPU for batched optimization solvers.}
The machine learning community has been a strong advocate for porting
mathematical optimization on the GPU. One of the most promising
applications is embedding mathematical programs inside neural networks,
a task that requires batching the solution of the optimization model
for the training algorithm to be
efficient~\cite{amos2017optnet,pineda2022theseus}.  This has led to
the development of prototype code solving thousands of (small)
optimization problems in parallel on the GPU.
Furthermore, batched optimization solvers can be leveraged
in decomposition algorithms, when the subproblems share the same structure~\cite{kimLeveragingGPUBatching2021}.
However, it is not trivial to adapt such code to solve large-scale optimization problems,
as the previous prototypes are reliant on dense linear solvers to
compute the descent directions.

\paragraph{GPU for nonlinear programming.}
The success of first-order algorithms in classical mathematical programming
relies on the convexity of the problem. Thus, this approach is nontrivial to replicate
in nonlinear programming: Most engineering problems encode complex
physical equations that are likely to break any convex structure in the problem.
Previous experiments on the alternating current (AC) optimal power flow (OPF) problem have shown that even a simple
algorithm as the alternating direction method of multipliers (ADMM) has trouble converging as soon as the convergence
tolerance is set below $10^{-3}$~\cite{kimLeveragingGPUBatching2021}.

Thus, second-order methods remain a competitive option, particularly
for scenarios that demand higher levels of accuracy and robust convergence.
Second-order algorithms solve a Newton step at each
iteration, an operation relying on non-trivial sparse linear algebra operations.
The previous generation of GPU-accelerated sparse linear
solvers were lagging behind their CPU equivalents, as illustrated in
subsequent surveys~\cite{swirydowicz2021linear,tasseff2019exploring}.
Fortunately, sparse solvers on GPUs are becoming increasingly better: NVIDIA has released in November 2023
the {\tt cuDSS} sparse direct solver that implements different sparse factorization routines with remarkably improved performance.
Our benchmark results indicate that {\tt cuDSS} is significantly faster than the previous sparse solvers using NVIDIA GPUs (e.g., published in \cite{shin2023accelerating}).
% SS: removed due to irrelevance.
% The new NVIDIA Grace CPU architecture could also be a game changer in the future of sparse linear solvers, thanks to fast communication between the CPU and GPU.
Furthermore, variants of interior point methods have been proposed
that does not depend on numerical pivoting in the linear solves,
opening the door to parallelized sparse solvers.
Coupled with a GPU-accelerated automatic differentiation library and a
sparse Cholesky solver, these nonlinear programming solvers can solve
optimal power flow (OPF) problems 10x faster than state-of-the-art
methods~\cite{shin2023accelerating}.

There exist a few alternatives to sparse linear solvers for solving the KKT systems on the GPU.
On the one hand, iterative and Krylov methods depend only on matrix-vector products to solve linear systems.
They often require non-trivial reformulation or
specialized preconditioning of the KKT systems to mitigate the
inherent ill-conditioning of the KKT matrices, which has limited their
use within the interior-point methods
\cite{curtisNoteImplementationInteriorpoint2012,rodriguezScalablePreconditioningBlockstructured2020}.
New results are giving promising outlooks for convex problems~\cite{ghannad2022linear},
but nonconvex problems often need an Augmented Lagrangian reformulation
to be tractable~\cite{cao2016augmented,regev2023hykkt}. In particular,
\cite{regev2023hykkt} presents an interesting use of the Golub and Greif
hybrid method~\cite{golub2003solving} to solve the KKT systems arising in
the interior-point methods, with promising results on the GPU.
On the other hand, null-space methods (also known as reduced Hessian methods)
reduce the KKT system down to a dense matrix, a setting also favorable for GPUs.
Our previous research has shown that the method plays nicely with the interior-point
methods if the number of degrees of freedom in the problem remains relatively small~\cite{pacaud2022condensed}.


\subsection{Contributions}
In this article, we assess the current capabilities of modern GPUs
to solve large-scale nonconvex nonlinear programs to optimality.
We focus on the two condensed-space methods
introduced respectively in~\cite{regev2023hykkt,shin2023accelerating}.
We re-use classical results from~\cite{wright1998ill} to show
that for both methods, the condensed matrix exhibits
structured ill-conditioning that limits the loss of accuracy in
the descent direction (provided the interior-point algorithm satisfies
some standard assumptions).
We implement both algorithms inside the GPU-accelerated solver MadNLP,
and leverage the GPU-accelerated automatic differentiation
backend ExaModels~\cite{shin2023accelerating}.
The interior-point algorithm runs entirely on the GPU, from
the evaluation of the model (using ExaModels) to the solution of
the KKT system (using a condensed-space method running on the GPU).
We use CUDSS.jl \cite{Montoison_CUDSS}, a Julia interface to the NVIDIA library {\tt cuDSS},
to solve the condensed KKT systems. We evaluate the strengths
and weaknesses of both methods, in terms of accuracy and runtime.
Extending beyond the classical OPF instances examined in our previous work,
we incorporate large-scale problems sourced from the COPS nonlinear benchmark~\cite{dolan2004benchmarking}.
Our assessment involves comparing the performance achieved on the GPU with that of a state-of-the-art method executed on the CPU.
The findings reveal that the condensed-space IPM enables a remarkable ten-time acceleration in solving large-scale OPF instances when utilizing the GPU.
However, performance outcomes on the COPS benchmark exhibit more variability.

\subsection{Notations}
By default, the norm $\|\cdot\|$ refers to the 2-norm.
We define the conditioning of a matrix $A$ as
$\cond(A) = \| A \| \|A^{-1} \|$. % Sungho: I wonder if subscript 2 is necessary in that it is clear from the context that we're connsidering 2 norm.
For any real number $x$, we denote by $\widehat{x}$ its floating
point representation.
We denote $\epstol$ as the smallest positive number such that
$\widehat{x} \leq (1 + \tau) x$ for $|\tau| < \epstol$.
In double precision, $\epstol = 1.1 \times 10^{-16}$.
We use the following notations to proceed with our error analysis.
For $p \in \mathbb{N}$ and a positive variable $h$:
\begin{itemize}
  \item We write $x = O(h^p)$ if there exists a constant $b > 0$
    such that $\| x \| \leq b h^p$;
  \item We write $x = \Omega(h^p)$ if there exists a constant $a > 0$
    such that $\| x \| \geq a h^p$;
  \item We write $x = \Theta(h^p)$ if there exists two constants $0 < a < b$
    such that $a h^p \leq \| x \| \leq b h^p$.
\end{itemize}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main.tex"
%%% End:
