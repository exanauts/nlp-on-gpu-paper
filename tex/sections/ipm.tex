\section{Primal-dual interior-point method}
The interior-point method (IPM) is among the most popular algorithms
to solve nonlinear programs. The basis of the algorithm is to
reformulate the Karush-Kuhn-Tucker (KKT) conditions of the nonlinear program as a smooth
system of nonlinear equations using a homotopy method~\cite{nocedal_numerical_2006}.
In a standard implementation, the
resulting system is solved iteratively with a Newton method (used in conjunction
with a line-search method for globalization). In this section, we
give a brief description of a nonlinear program in \S\ref{sec:ipm:problem}
and detail the Newton step solved at each IPM iteration in \S\ref{sec:ipm:kkt}.

\subsection{Problem's formulation and KKT conditions}
\label{sec:ipm:problem}
We are interested in solving the following nonlinear program:
\begin{equation}
  \label{eq:problem}
    \min_{x \in \mathbb{R}^n} \;  f(x)
\quad \text{subject to}\quad
\left\{
  \begin{aligned}
    & g(x) = 0 \; , ~ h(x) \leq 0 \; , \\
      & x \geq 0  \; ,
  \end{aligned}
\right.
\end{equation}
with $f:\mathbb{R}^n \to \mathbb{R}$ a real-valued function
encoding the objective, $g: \mathbb{R}^n \to \mathbb{R}^{m_e}$
encoding the equality constraints, and $h: \mathbb{R}^{n} \to
\mathbb{R}^{m_i}$ encoding the inequality constraints.
In addition, the variable $x$ is subject to simple bounds $x \geq 0$.
In what follows, we suppose that the functions $f, g, h$ are smooth
and twice differentiable.

We reformulate \eqref{eq:problem} using slack variables $s \geq 0$
into the equivalent formulation
\begin{equation}
  \label{eq:slack_problem}
    \min_{x \in \mathbb{R}^n, s \in \mathbb{R}^{m_i}} \;  f(x)
    \quad \text{subject to} \quad
    \left\{
  \begin{aligned}
    & g(x) = 0 \; , ~ h(x) + s = 0 \; , \\
      & x \geq 0  \; , ~ s \geq 0  \; .
  \end{aligned}
  \right.
\end{equation}
In~\eqref{eq:slack_problem}, the inequality constraints
are directly encoded inside the variable bounds.

We note by $y \in \mathbb{R}^{m_e}$ the multipliers associated
to the equality constraints and $z \in \mathbb{R}^{m_i}$ the multipliers
associated to the inequality constraints. Similarly, we note
by $(u, v) \in \mathbb{R}^{n + m_i}$ the multipliers associated
respectively to $x \geq 0$ and $s \geq 0$.
Using the multipliers $(y, z, v, w)$, we define the Lagrangian of \eqref{eq:slack_problem} as
\begin{equation}
  \label{eq:lagrangian}
  L(x, s; y, z, u, v) = f(x) + y^\top g(x) + z^\top \big(h(x) +s\big)
  - u^\top x - v^\top s \; .
\end{equation}
The KKT conditions of \eqref{eq:slack_problem} are:
\begin{subequations}
  \label{eq:kktconditions}
    \begin{align}
      & \nabla f(x) + \nabla g(x)^\top y + \nabla h(x)^\top z - u = 0 \\
      & z - v = 0 \\
      & g(x) = 0 \\
      & h(x) + s = 0 \\
      \label{eq:kktconditions:compx}
      & 0 \leq x \perp u \geq 0 \\
      \label{eq:kktconditions:comps}
      & 0 \leq s \perp v \geq 0
    \end{align}
\end{subequations}
The notation $x \perp u$ is a shorthand for the complementarity
condition $x_i u_i = 0$ (for all $i=1\cdots, n$).

The set of active constraints at a point $x$ is denoted by
\begin{equation}
  \mathcal{B}(x) := \{ i = 1, \cdots, m_i \; | \; h_i(x) = 0 \} \; .
\end{equation}
The inactive set is defined as the complement $\mathcal{N}(x) := \{1, \cdots, m_i \} \setminus \mathcal{B}(x)$.
We note $m_a$ the number of active constraints.
The active Jacobian is defined as $A(x) := \begin{bmatrix} \nabla g(x) \\ \nabla h_{\mathcal{B}}(x) \end{bmatrix} \in \mathbb{R}^{(m_e + m_a) \times n}$.

\begin{assumption}
  \label{hyp:ipm}
  Let $w^\star = (x^\star, s^\star, y^\star, z^\star, u^\star, v^\star)$ be a primal-dual solution
  satisfying the KKT conditions~\eqref{eq:kktconditions}. Let the following hold:
  \begin{itemize}
    \item Regularity: The Hessian $\nabla^2_{x x} L(\cdot)$ is Lipschitz continuous
      near $w^\star$;
    \item LICQ: the active Jacobian $A(x^\star)$ is full row-rank;
    \item SCS: for every $i \in \mathcal{B}(x^\star)$, $z_i^\star > 0$.
    \item SOSC: for every $v \in \nullspace\big(A(x^\star)\big)$,
      $v^\top \nabla_{x x}^2 L(w^\star) v > 0$.
  \end{itemize}
\end{assumption}


\subsection{Solving the KKT conditions with the interior-point method}
\label{sec:ipm:kkt}
The interior-point method aims at finding a stationary point
associated to the KKT conditions~\eqref{eq:kktconditions}.
The complementarity constraints \eqref{eq:kktconditions:compx}-\eqref{eq:kktconditions:comps}
render the KKT conditions non-smooth, complicating the solution of
the whole system~\eqref{eq:kktconditions}.
Instead, IPM uses a homotopy continuation method to solve a simplified
version of \eqref{eq:kktconditions}, parameterized by a barrier
parameter $\mu > 0$~\cite[Chapter 19]{nocedal_numerical_2006}.
For positive $(x, s, u, v) > 0$, we solve the system
$F_\mu(x, s, y, z, u, v) = 0$, with
\begin{equation}
  \label{eq:kkt_ipm}
  F_\mu(x, s, y, z, u, v) =
  \begin{bmatrix}
       \nabla f(x) + \nabla g(x)^\top y + \nabla h(x)^\top z - u  \\
       z - v  \\
       g(x)  \\
       h(x) + s  \\
       X u - \mu e  \\
       S v - \mu e
  \end{bmatrix}
   \; .
\end{equation}
We introduce in \eqref{eq:kkt_ipm} the diagonal matrices $X = \diag(x_1, \cdots, x_n)$
and $S = \diag(s_1, \cdots, s_{m_i})$.
As we drive the barrier parameter $\mu$ to $0$, we recover the original
KKT conditions~\eqref{eq:kktconditions}.

We note that at a fixed parameter $\mu$, the function $F_\mu(\cdot)$
is smooth. Hence, the system \eqref{eq:kkt_ipm} can be solved iteratively
using a regular Newton method. For a given primal-dual variable
$w_k := (x_k, s_k, y_k, z_k, u_k, v_k)$, the Newton step writes out
$w_{k+1} = w_k + \alpha_k d_k$, with $d_k$ a descent
direction being solution of the linear system
\begin{equation}
  \label{eq:newton_step}
  \nabla_w F(w_k) d_k = -F(w_k) \; .
\end{equation}
The step $\alpha_k$ is computed using a line-search algorithm, in a way
that ensures that the bounded variables remain positive
at the next primal-dual iterate (so that $(x_{k+1}, s_{k+1}, u_{k+1}, v_{k+1}) > 0$).
Once the iterates are sufficiently closed to the central path,
the IPM decreases the barrier term $\mu$ to find a solution closer to
the original KKT conditions~\eqref{eq:kktconditions}.

In IPM, the bulk of the workload is the computation of the Newton
step \eqref{eq:newton_step}, which involves (i) assembling the Jacobian
$\nabla_w F_\mu(w_k)$ and (ii) solving the linear system to compute
the descent direction $d_k$.
The Newton step~\eqref{eq:newton_step} expands as the $6 \times 6$
\emph{unreduced KKT system}:
\begin{equation}
  \label{eq:kkt:unreduced}
  \tag{$K_3$}
  \begin{bmatrix}
    W & 0 & G^\top & H^\top & -I & 0 \\
    0 & 0 & 0 & I & 0 & -I \\
    G & 0 & 0 & 0 & 0 & 0 \\
    H & I & 0 & 0 & 0 & 0 \\
    U & 0 & 0 & 0 & X & 0 \\
    0 & V & 0 & 0 & 0 & S
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\
    d_s \\
    d_y \\
    d_z \\
    d_u \\
    d_v
  \end{bmatrix}
  % = -F_\mu(w_k) \; .
  = - \begin{bmatrix}
    \nabla_x L(w_k) \\
       % \nabla f(x_k) + \nabla g(x_k)^\top y_k + \nabla h(x_k)^\top z_k - v_k  \\
       z_k - v_k  \\
       g(x_k)  \\
       h(x_k) + s_k  \\
       X_k u_k - \mu e  \\
       S_k v_k - \mu e
  \end{bmatrix} \; ,
\end{equation}
where we have introduced the Hessian $W = \nabla^2_{x x} L(w_k)$ and
the two Jacobians $G = \nabla g(x_k)$, $H = \nabla h(x_k)$.
In addition, we define $U := \diag(u_1, \cdots, u_n)$
and $V = \diag(v_1, \cdots, v_{m_i})$.

\paragraph{Augmented KKT system.}
The system~\eqref{eq:kkt:unreduced} is not symmetric.
It is usual to remove the blocks associated
to the bound multipliers $(u, v)$ and solve instead the equivalent
$4 \times 4$ symmetric system, called the \emph{augmented KKT system}:
\begin{equation}
  \label{eq:kkt:augmented}
  \tag{$K_2$}
  \begin{bmatrix}
    W + D_x & 0 & G^\top & H^\top \\
    0 & D_s & 0& I \\
    G & 0 & 0 & 0 \\
    H & I & 0 & 0
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\
    d_s \\
    d_y \\
    d_z
  \end{bmatrix}
  = - \begin{bmatrix}
    r_1 \\ r_2 \\ r_3 \\ r_4
       % \nabla f(x_k) + \nabla g(x_k)^\top y_k + \nabla h(x_k)^\top z_k   \\
       % z_k - w_k  \\
       % g(x_k)  \\
       % h(x_k) + s_k
  \end{bmatrix} \; ,
\end{equation}
with the diagonal matrices $D_x = X^{-1} U$ and $D_s = S^{-1} V$.
The right-hand-sides are given respectively by
$r_1 = \nabla f(x_k) + \nabla g(x_k)^\top y_k + \nabla h(x_k)^\top z_k + \mu X^{-1} e$,
$r_2 = z_k + \mu S^{-1} e$,
$r_3 = g(x_k)$,
$r_4 = h(x_k) + s_k$.
Once \eqref{eq:kkt:augmented} solved, we recover the
update on the bound multipliers with
$d_u = - X^{-1}(U d_x + X u_k - \mu e)$,
$d_v = - S^{-1}(V d_s + S v_k - \mu e)$.

The system \eqref{eq:kkt:augmented} is usually factorized using
an inertia-revealing LBL factorization.
Unfortunately, the block diagonal term
$D_x$ and $D_s$ are usually poorly conditioned, preventing
a solution with a Krylov iterative method.

\paragraph{Condensed KKT system.}
The $4 \times 4$ KKT system \eqref{eq:kkt:augmented} can be further
reduced down to a $2 \times 2$ system by eliminating the two blocks
$(d_s, d_z)$ associated to the inequality constraints.
The resulting system is called the \emph{condensed KKT system}:
\begin{equation}
  \label{eq:kkt:condensed}
  \tag{$K_1$}
  \begin{bmatrix}
    K & G^\top \\
    G & 0
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\ d_y
  \end{bmatrix}
  =
  -
  \begin{bmatrix}
    r_1 + H^\top(D_s r_4 - r_2) \\ r_3
  \end{bmatrix}
  =:
  \begin{bmatrix}
    \bar{r}_1 \\ \bar{r}_2
  \end{bmatrix}
   \; ,
\end{equation}
where we have introduced the \emph{condensed matrix} $K := W + D_x + H^\top D_s H$.
Using the solution of the system~\eqref{eq:kkt:condensed},
we recover the updates on the slacks and inequality multipliers with
$d_s = -r_4 - Hd_x$ and $d_z = -r_2 - D_s d_s$.

\paragraph{Iterative refinement.}
Compared to \eqref{eq:kkt:unreduced},
the diagonal matrices $D_x$ and $D_s$ in \eqref{eq:kkt:augmented} introduce
an additional ill-conditioning in \eqref{eq:kkt:augmented}, amplified
in the condensed form~\eqref{eq:kkt:condensed}. For that reason, it
is recommended to refine the solution returned by the direct sparse linear
solver by using Richardson iterations on the original system~\eqref{eq:kkt:unreduced}
(see \cite[Section 3.10]{wachter2006implementation}).


\subsection{Discussion}
We have obtained three different formulations for the KKT system
appearing at each IPM iteration.
The original formulation \eqref{eq:kkt:unreduced} is not symmetric, but
has a better conditioning than the two alternatives \eqref{eq:kkt:augmented}
and \eqref{eq:kkt:condensed}.
The second formulation~\eqref{eq:kkt:augmented} is
used by default in state-of-the-art nonlinear solvers~\cite{wachter2006implementation,waltz2006interior}.
The system~\eqref{eq:kkt:augmented} is usually factorized using a LBL factorization: for sparse matrices, the Duff and Reid
multifrontal algorithm~\cite{duff1983multifrontal} is the favored method (as implemented in the linear solvers HSL
MA27 and MA57~\cite{duff2004ma57}).
On its end, the condensed KKT system~\eqref{eq:kkt:condensed} is often discarded,
as its conditioning is higher
than \eqref{eq:kkt:augmented} (implying less accurate solutions)
and the term $H^\top D_s H$ can lead to a dense condensed matrix if one column
of the Jacobian is dense. In addition,
the condensed matrix $K$ can potentially lead to additional fill-in
in the direct factorization~\cite[Section 19.3, p.571]{nocedal_numerical_2006}.
For that reason, Knitro~\cite{waltz2006interior} is the only solver
that supports computing the descent direction with \eqref{eq:kkt:condensed}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
