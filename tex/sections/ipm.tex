\section{Primal-dual interior-point method}

The interior-point method (IPM) is among the most popular algorithms
to solve nonlinear programs. The basis of the algorithm is to
reformulate the Karush-Kuhn-Tucker (KKT) conditions of the nonlinear program as a smooth
system of nonlinear equations using a homotopy method~\cite{nocedal_numerical_2006}.
In a standard implementation, the
resulting system is solved iteratively with a Newton method (used in conjunction
with a line-search method for globalization). In Section \ref{sec:ipm:problem}, we
give a brief description of a nonlinear program.
We detail in Section \ref{sec:ipm:kkt} the Newton step computation within each IPM iteration.

\subsection{Problem formulation and KKT conditions}
\label{sec:ipm:problem}
We are interested in solving the following nonlinear program:
\begin{equation}
  \label{eq:problem}
    \min_{x \in \mathbb{R}^n} \;  f(x)
\quad \text{subject to}\quad
\left\{
  \begin{aligned}
    & g(x) = 0 \; , ~ h(x) \leq 0 \; , \\
      & x \geq 0  \; ,
  \end{aligned}
\right.
\end{equation}
with $f:\mathbb{R}^n \to \mathbb{R}$ a real-valued function
encoding the objective, $g: \mathbb{R}^n \to \mathbb{R}^{m_e}$
encoding the equality constraints, and $h: \mathbb{R}^{n} \to
\mathbb{R}^{m_i}$ encoding the inequality constraints.
In addition, the variable $x$ is subject to simple bounds $x \geq 0$.
In what follows, we suppose that the functions $f, g, h$ are smooth
and twice differentiable.

We reformulate \eqref{eq:problem} using non-negative slack variables $s \geq 0$
into the equivalent formulation
\begin{equation}
  \label{eq:slack_problem}
    \min_{x \in \mathbb{R}^n, ~ s \in \mathbb{R}^{m_i}} \;  f(x)
    \quad \text{subject to} \quad
    \left\{
  \begin{aligned}
    & g(x) = 0 \; , ~ h(x) + s = 0 \; , \\
      & x \geq 0  \; , ~ s \geq 0  \; .
  \end{aligned}
  \right.
\end{equation}
In~\eqref{eq:slack_problem}, the inequality constraints
are encoded inside the variable bounds.

We denote by $y \in \mathbb{R}^{m_e}$ the multipliers associated
to the equality constraints and $z \in \mathbb{R}^{m_i}$ the multipliers
associated to the inequality constraints. Similarly, we denote
by $(u, v) \in \mathbb{R}^{n + m_i}$ the multipliers associated
respectively to $x \geq 0$ and $s \geq 0$.
Using the multipliers $(y, z, u, v)$, we define the Lagrangian of \eqref{eq:slack_problem} as
\begin{equation}
  \label{eq:lagrangian}
  L(x, s, y, z, u, v) = f(x) + y^\top g(x) + z^\top \big(h(x) +s\big)
  - u^\top x - v^\top s \; .
\end{equation}
The KKT conditions of \eqref{eq:slack_problem} are:
\begin{subequations}
  \label{eq:kktconditions}
    \begin{align}
      & \nabla f(x) + \nabla g(x)^\top y + \nabla h(x)^\top z - u = 0 \; , \\
      & z - v = 0 \; , \\
      & g(x) = 0 \; , \\
      & h(x) + s = 0 \; , \\
      \label{eq:kktconditions:compx}
      & 0 \leq x \perp u \geq 0 \; , \\
      \label{eq:kktconditions:comps}
      & 0 \leq s \perp v \geq 0 \; .
    \end{align}
\end{subequations}
The notation $x \perp u$ is a shorthand for the complementarity
condition $x_i u_i = 0$ (for all $i=1\cdots, n$).

The set of active constraints at a point $x$ is denoted by
\begin{equation}
  \mathcal{B}(x) := \{ i \in\{ 1, \cdots, m_i\} \; | \; h_i(x) = 0 \} \; .
\end{equation}
The inactive set is defined as the complement $\mathcal{N}(x) := \{1, \cdots, m_i \} \setminus \mathcal{B}(x)$.
We note $m_a$ the number of active constraints.
The active Jacobian is defined as $A(x) := \begin{bmatrix} \nabla g(x) \\ \nabla h_{\mathcal{B}}(x) \end{bmatrix} \in \mathbb{R}^{(m_e + m_a) \times n}$.


\subsection{Solving the KKT conditions with the interior-point method}
\label{sec:ipm:kkt}
The interior-point method aims at finding a stationary point
satisfying the KKT conditions~\eqref{eq:kktconditions}.
The complementarity constraints \eqref{eq:kktconditions:compx}-\eqref{eq:kktconditions:comps}
render the KKT conditions non-smooth, complicating the solution of
the whole system~\eqref{eq:kktconditions}.
IPM uses a homotopy continuation method to solve a simplified
version of \eqref{eq:kktconditions}, parameterized by a barrier
parameter $\mu > 0$~\cite[Chapter 19]{nocedal_numerical_2006}.
For positive $(x, s, u, v) > 0$, we solve the system
\begin{equation}
  \label{eq:kkt_ipm}
  F_\mu(x, s, y, z, u, v) =
  \begin{bmatrix}
       \nabla f(x) + \nabla g(x)^\top y + \nabla h(x)^\top z - u  \\
       z - v  \\
       g(x)  \\
       h(x) + s  \\
       X u - \mu e  \\
       S v - \mu e
  \end{bmatrix} = 0
   \; .
\end{equation}
We introduce in \eqref{eq:kkt_ipm} the diagonal matrices $X = \diag(x_1, \cdots, x_n)$
and $S = \diag(s_1, \cdots, s_{m_i})$.
As we drive the barrier parameter $\mu$ to $0$, the solution of the
system $F_\mu(x, s, y, z, u, v) = 0$ tends to the solution of the
KKT conditions~\eqref{eq:kktconditions}.

We note that at a fixed parameter $\mu$, the function $F_\mu(\cdot)$
is smooth. Hence, the system \eqref{eq:kkt_ipm} can be solved iteratively
using a regular Newton method. For a primal-dual iterate
$w_k := (x_k, s_k, y_k, z_k, u_k, v_k)$, the next iterate is computed as
$w_{k+1} = w_k + \alpha_k d_k$, where $d_k$ is a descent
direction computed by solving the linear system
\begin{equation}
  \label{eq:newton_step}
  \nabla_w F_{\mu}(w_k) d_k = -F_{\mu}(w_k) \; .
\end{equation}
The step $\alpha_k$ is computed using a line-search algorithm, in a way
that ensures that the bounded variables remain positive
at the next primal-dual iterate: $(x_{k+1}, s_{k+1}, u_{k+1}, v_{k+1}) > 0$.
Once the iterates are sufficiently close to the central path,
the IPM decreases the barrier parameter $\mu$ to find a solution closer to
the original KKT conditions~\eqref{eq:kktconditions}.

In IPM, the bulk of the workload is the computation of the Newton
step \eqref{eq:newton_step}, which involves assembling the Jacobian
$\nabla_w F_\mu(w_k)$ and solving the linear system to compute
the descent direction $d_k$.
By writing out all the blocks, the system in~\eqref{eq:newton_step} expands as the $6 \times 6$
\emph{unreduced KKT system}:
\begin{equation}
  \label{eq:kkt:unreduced}
  \setlength\arraycolsep{5pt}
  \tag{$K_3$}
  \begin{bmatrix}
    W_k & 0 & G_k^\top           & H_k^\top           & -I           & \phantom{-}0 \\
    0 & 0 & 0\phantom{^\top} & I\phantom{^\top} & \phantom{-}0 & -I           \\
    G_k & 0 & 0\phantom{^\top} & 0\phantom{^\top} & \phantom{-}0 & \phantom{-}0 \\
    H_k & I & 0\phantom{^\top} & 0\phantom{^\top} & \phantom{-}0 & \phantom{-}0 \\
    U_k & 0 & 0\phantom{^\top} & 0\phantom{^\top} & \phantom{-}X_k & \phantom{-}0 \\
    0 & V_k & 0\phantom{^\top} & 0\phantom{^\top} & \phantom{-}0 & \phantom{-}S_k
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\
    d_s \\
    d_y \\
    d_z \\
    d_u \\
    d_v
  \end{bmatrix}
  % = -F_\mu(w_k) \; .
  = - \begin{bmatrix}
    \nabla_x L(w_k) \\
       % \nabla f(x_k) + \nabla g(x_k)^\top y_k + \nabla h(x_k)^\top z_k - v_k  \\
       z_k - v_k  \\
       g(x_k)  \\
       h(x_k) + s_k  \\
       X_k u_k - \mu e  \\
       S_k v_k - \mu e
  \end{bmatrix} \; ,
\end{equation}
where we have introduced the Hessian $W_k = \nabla^2_{x x} L(w_k)$ and
the two Jacobians $G_k = \nabla g(x_k)$, $H_k = \nabla h(x_k)$.
In addition, we define $X_k$, $S_k$, $U_k$ and $V_k$ the diagonal matrices built respectively
from the vectors $x_k$, $s_k$, $u_k$ and $v_k$.
Note that~\eqref{eq:kkt:unreduced} can be symmetrized by performing simple block row and column operations.
In what follows, we will omit the index $k$ to alleviate the notations.

\paragraph{Augmented KKT system.}
It is usual to remove in \eqref{eq:kkt:unreduced} the blocks associated
to the bound multipliers $(u, v)$ and solve instead the regularized
$4 \times 4$ symmetric system, called the \emph{augmented KKT system}:
\begin{equation}
  \label{eq:kkt:augmented}
  \tag{$K_2$}
  \setlength\arraycolsep{3pt}
  \begin{bmatrix}
    W + D_x + \delta_w I & 0   & \phantom{-}G^\top           & \phantom{-}H^\top           \\
      0       & D_s + \delta_w I  & \phantom{-}0\phantom{^\top} & \phantom{-}I\phantom{^\top} \\
      G       & 0   & -\delta_c I  & \phantom{-}0\phantom{^\top} \\
    H       & I   & \phantom{-}0\phantom{^\top} & -\delta_c I
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\
    d_s \\
    d_y \\
    d_z
  \end{bmatrix}
  = - \begin{bmatrix}
    r_1 \\ r_2 \\ r_3 \\ r_4
       % \nabla f(x_k) + \nabla g(x_k)^\top y_k + \nabla h(x_k)^\top z_k   \\
       % z_k - w_k  \\
       % g(x_k)  \\
       % h(x_k) + s_k
  \end{bmatrix} \; ,
\end{equation}
with the diagonal matrices $D_x := X^{-1} U$ and $D_s := S^{-1} V$.
The vectors forming the right-hand-sides are given respectively by
$r_1 := \nabla f(x) + \nabla g(x)^\top y + \nabla h(x)^\top z + \mu X^{-1} e$,
$r_2 := z + \mu S^{-1} e$,
$r_3 := g(x)$,
$r_4 := h(x) + s$.
Once \eqref{eq:kkt:augmented} solved, we recover the updates on bound multipliers with
$d_u = - X^{-1}(U d_x + X u - \mu e)$ and
$d_v = - S^{-1}(V d_s + S v - \mu e)$.

Note that we have added additional regularization terms $\delta_w \geq 0 $
and $\delta_c \geq 0$ in \eqref{eq:kkt:augmented}, to ensure the
matrix is invertible.
The augmented matrix \eqref{eq:kkt:augmented} is non-singular
if and only if the Jacobian $J = \begin{bmatrix} G & 0 \\ H & I \end{bmatrix}$
is full row-rank and the matrix $\begin{bmatrix} W + D_x & 0 \\ 0 & D_s \end{bmatrix}$
projected onto the null-space of the Jacobian $J$ is positive-definite~\cite{benzi2005numerical}.
The condition is satisfied if the inertia of the matrix is $(n + m_i, m_i + m_e, 0)$
(here the inertia is defined as the respective numbers of positive, negative and
null eigenvalues).
We use the inertia-controlling method introduced in \cite{wachter2006implementation}
to regularize the augmented matrix by adding multiple of the identity
on the diagonal of \eqref{eq:kkt:augmented} if the inertia is not $(n+m_i, m_e+m_i, 0)$.

As a consequence, the system \eqref{eq:kkt:augmented} is usually factorized using
an inertia-revealing \lblt factorization~\cite{duff1983multifrontal}.
Krylov methods are often not competitive when solving~\eqref{eq:kkt:augmented},
as the block diagonal terms $D_x$ and $D_s$ are getting increasingly
ill-conditioned near the solution. Their use in IPM has been limited in
linear and convex quadratic programming \cite{gondzio-2012} (when paired
with a suitable preconditioner). We also refer to \cite{cao-seth-laird-2016}
for an efficient implementation of a preconditioned conjugate gradient
on GPU, for solving the Newton step arising in an Augmented-Lagrangian Interior-Point
approach.

\paragraph{Condensed KKT system.}
The $4 \times 4$ KKT system \eqref{eq:kkt:augmented} can be further
reduced down to a $2 \times 2$ system by eliminating the two blocks
$(d_s, d_z)$ associated to the inequality constraints.
The resulting system is called the \emph{condensed KKT system}:
\begin{equation}
  \label{eq:kkt:condensed}
  \tag{$K_1$}
  \setlength\arraycolsep{3pt}
  \begin{bmatrix}
    K & G^\top \\
    G & -\delta_c I \phantom{^\top}
  \end{bmatrix}
  \begin{bmatrix}
    d_x \\ d_y
  \end{bmatrix}
  =
  -
  \begin{bmatrix}
    r_1 + H^\top(D_H r_4 - C r_2) \\ r_3
  \end{bmatrix}
  =:
  \begin{bmatrix}
    \bar{r}_1 \\ \bar{r}_2
  \end{bmatrix}
   \; ,
\end{equation}
where we have introduced the \emph{condensed matrix} $K := W + D_x + \delta_w I  + H^\top D_H H$
and the two diagonal matrices
\begin{equation}
  C := \big(I + \delta_c(D_s + \delta_w I)\big)^{-1} \; , \quad
  D_H := (D_s + \delta_w I) C \; .
\end{equation}
Using the solution of the system~\eqref{eq:kkt:condensed},
we recover the updates on the slacks and inequality multipliers with
$d_z = -C r_2 + D_H(H d_x + r_4)$ and $d_s = -(D_s + \delta_w I)^{-1}(r_2 - d_z)$.
Using Sylvester's law of inertia, we can prove that
\begin{equation}
  \label{eq:ipm:inertia}
  \inertia(K_2) = (n+m_i, m_e+m_i, 0) \iff
  \inertia(K_1) = (n, m_e, 0) \;.
\end{equation}

\paragraph{Iterative refinement.}
Compared to \eqref{eq:kkt:unreduced},
the diagonal matrices $D_x$ and $D_s$ introduce
an additional ill-conditioning in \eqref{eq:kkt:augmented}, amplified
in the condensed form~\eqref{eq:kkt:condensed}:
the elements in the diagonal tend to infinity if a variable converges to its bound,
and to $0$ if the variable is inactive.
To address the numerical error arising from such ill-conditioning, most of the
implementation of IPM employs Richardson iterations on the original system~\eqref{eq:kkt:unreduced} to refine the solution returned by the direct sparse linear solver (see \cite[Section 3.10]{wachter2006implementation}).

\subsection{Discussion}

We have obtained three different formulations of the KKT systems
appearing at each IPM iteration.  The original formulation
\eqref{eq:kkt:unreduced} has a better
conditioning than the two alternatives \eqref{eq:kkt:augmented} and
\eqref{eq:kkt:condensed} but has a much larger size.
The second formulation~\eqref{eq:kkt:augmented} is
used by default in state-of-the-art nonlinear solvers~\cite{wachter2006implementation,waltz2006interior}.
The system~\eqref{eq:kkt:augmented} is usually factorized using a \lblt factorization: for sparse matrices, the Duff and Reid
multifrontal algorithm~\cite{duff1983multifrontal} is the favored method (as implemented in the HSL linear solvers MA27 and MA57~\cite{duff2004ma57}).
The condensed KKT system~\eqref{eq:kkt:condensed} is often discarded,
as its conditioning is higher
than \eqref{eq:kkt:augmented} (implying less accurate solutions).
Additionally, condensation may result in increased fill-in within the condensed system \eqref{eq:kkt:condensed}~\cite[Section 19.3, p.571]{nocedal_numerical_2006}.
In the worst cases \eqref{eq:kkt:condensed} itself may become fully dense if an inequality row is completely dense (fortunately, a case rarer than in the normal equations used in linear programming).
Consequently, condensation methods are not commonly utilized in practical optimization settings.
To the best our knowledge, Knitro~\cite{waltz2006interior} is the only solver that supports computing the descent direction with \eqref{eq:kkt:condensed}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t %%% TeX-master: "../main"
%%% End:
