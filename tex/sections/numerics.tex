\section{Numerical results}
We compare the performance of LiftedKKT and HyKKT with HSL MA27 and MA57 on two benchmarks.
First, we detail in \S\ref{sec:num:pprof} the respective performance of the two hybrid solvers
on a large-scale OPF instance. Then, we present in \S\ref{sec:num:opf}
the results reported on the PGLIB OPF benchmark, complemented in \S\ref{sec:num:cops} by
the COPS benchmark.

\subsection{Implementation}
All our implementation uses the Julia language \cite{bezanson-edelman-karpinski-shah-2017}.

\paragraph{IPM solver.}
We have implemented the two condensed-space methods in our nonlinear IPM solver MadNLP~\cite{shin2021graph}.
This implementation utilizes the abstraction {\tt AbstractKKTSystem}
in MadNLP to represent the various formulations of the KKT linear systems.
MadNLP can deport most of the IPM algorithm to the GPU, except for basic IPM operations used for the coordination (e.g., the filter line-search algorithm).
In particular, any operation that involves the manipulation of array entries is performed by GPU kernels without transferring data to host memory.
We refer to \cite{shin2023accelerating} for a detailed description of the GPU implementation in MadNLP.

\paragraph{Evaluation of the nonlinear models.}
We use the ExaModels.jl modeling tool~\cite{shin2023accelerating} to implement all the nonlinear programs utilized in our benchmark.
ExaModels.jl harnesses the sparsity structure and provides custom derivative kernels for repetitive algebraic subexpressions of the constraints and objective functions to compute first and second-order derivatives on the GPU in parallel~\cite{bischof1991exploiting,enzyme2021}.
This approach caters to the SIMD architecture of the GPU by assigning each expression to multiple threads responsible for computing derivatives for different values.

\paragraph{Linear solvers.}
We solve the KKT systems assembled within MadNLP using various sparse linear solvers, chosen based on the KKT formulation (\ref{eq:kkt:condensed}, \ref{eq:kkt:augmented}, \ref{eq:kkt:unreduced}) and the device (CPU, GPU) being utilized. We utilize the following solvers:
\begin{itemize}
  \item {\tt HSL MA27/MA57}: Implement the \lblt factorization on the CPU~\cite{duff1983multifrontal}.
    It solves the augmented KKT system~\eqref{eq:kkt:augmented}.
    This solver serves as the reference when running on the CPU.
  \item {\tt CHOLMOD}: Implements the Cholesky factorization on the CPU % Sungho: Maybe LDLFactorizations.jl instead, once the results are updated.
    (using the AMD ordering \cite{amestoy-david-duff-2004} by default).
    It factorizes the condensed matrix $K_\gamma$ appearing in \eqref{eq:kkt:hykkt}.
    This solver is used to assess the performance of the hybrid solvers when running on the CPU.
  \item {\tt cuDSS}: Implement \llt, \ldlt and \lu decompositions on an NVIDIA GPU.
    We use the \ldlt factorization to factorize the condensed matrices $K_\gamma$ on GPU.
    This newly released solver is the contender in our benchmark.
  \item {\tt Krylov.jl}: Contains the \CG method
    used in the Golub \& Greif strategy to solve \eqref{eq:kkt:schurcomplhykkt} on both CPU and GPU architectures.
\end{itemize}
CHOLMOD \cite{chen-davis-hager-rajamanickam-2008} is shipped with Julia.
For the HSL linear solvers, we utilize libHSL \cite{fowkes-lister-montoison-orban-2024} with the Julia interface HSL.jl \cite{montoison-orban-hsl-2021}.
HSL MA57 and CHOLMOD are both compiled with OpenBLAS \cite{openblas}, a multithreaded version of BLAS and LAPACK.
The Julia package Krylov.jl~\cite{montoison2023krylov} contains a collection of Krylov methods with a polymorphic implementation that can be used on both CPU and GPU architectures.
% Should we provide the version of the linear solvers?

\subsection{Performance analysis on a large-scale instance}
\label{sec:num:pprof}
We evaluate the performance of each KKT solver on a large-scale OPF instance, taken from
the PGLIB benchmark~\cite{babaeinejadsarookolaee2019power}: {\tt 78484epigrids}.
Our formulation with ExaModels has
a total of 674,562 variables, 661,017 equality constraints and 378,045
inequality constraints.
Our previous work has pointed out that as soon as the OPF model is
evaluated on the GPU using ExaModels, the KKT solver becomes the bottleneck
in the numerical implementation~\cite{shin2023accelerating}.

\subsubsection{Individual performance of the linear solvers}
Subsequently, we evaluate the individual performance of the cuDSS solver when factorizing the matrix $K_{\gamma}$ at the first IPM iteration (here with $\gamma = 10^7$).
We compare the times to perform the symbolic analysis,
the factorization and the triangular solves with those reported in CHOLMOD.

The results are displayed in Table~\ref{tab:linsol:time}.
We benchmark the three decompositions implemented in cuDSS (\llt, \ldlt, \lu), and assess the accuracy of the solution by computing the infinity norm of the residual.
% The accuracy of the solution depends of on more factors than the backsolve.
% The main error comes from the factorization.
We observe that the analysis phase is four times slower for cuDSS compared to CHOLMOD.
Fortunately, this operation only needs to be computed once in IPM, meaning that its cost is amortized if we run many IPM iterations.
The factorization is about twenty times faster in cuDSS, with a time almost independent of the algorithm being used.
The backward and forward sweeps are ten times faster:
the triangular solves are harder to parallelize on a GPU.
% The triangular solves don't parallelize well on a GPU.
In terms of accuracy, the quality of the solution remains on par with CHOLMOD, except for the \ldlt decomposition which lags behind by at least two orders of magnitude.

% \begin{table}[!ht]
%   \centering
%   \resizebox{\textwidth}{!}{
%   \begin{tabular}{|lrrrr|}
%   \hline
%   linear solver & analysis (s) & factorization (s) & backsolve (s) & accuracy \\
%   \hline
%     cholmod     & 0.798 &$6.34\times 10^{-1}$&$5.50\times 10^{-2}$&$3.60\times 10^{-13}$\\
%     cudss-\llt  & 3.50  &$4.77\times 10^{-2}$&$2.08\times 10^{-2}$&$2.64\times 10^{-13}$\\
%     cudss-\lu   & 3.48  &$4.97\times 10^{-2}$&$1.91\times 10^{-2}$&$2.58\times 10^{-13}$\\
%     cudss-\ldlt & 3.49  &$5.80\times 10^{-2}$&$1.88\times 10^{-2}$&$5.44\times 10^{-11}$\\
%   \hline
%   \end{tabular}
%   }
%   \caption{Comparing the performance of cuDSS with CHOLMOD.
%     The matrix $K_\gamma$ is symmetric positive definite, with
%     a size $n = 674,562$. The matrix is extremely sparse, with only $7,342,680$ non-zero entries ($0.002$\%).
%     \label{tab:linsol:time}
%     (A30 GPU)
%   }
% \end{table}

\begin{table}[!ht]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{|lrrrr|}
  \hline
  linear solver & analysis (s) & factorization (s) & backsolve (s) & accuracy \\
  \hline
    CHOLMOD     & 1.18  & $8.57 \times 10^{-1}$ & $1.27 \times 10^{-1}$ & $3.60\times 10^{-13}$\\
    cuDSS-\llt   & 4.52  & $3.75 \times 10^{-2}$ & $1.32 \times 10^{-2}$ & $2.64\times 10^{-13}$\\ % Sungho: not Cholesky? it would be better to use consistent term. I'd recommend Cholesky instead of LL^T.
    cuDSS-\lu   & 4.50  & $3.72 \times 10^{-2}$ & $1.49 \times 10^{-2}$ & $2.58\times 10^{-13}$\\
    cuDSS-\ldlt & 4.50  & $4.07 \times 10^{-2}$ & $1.55 \times 10^{-2}$ & $7.62\times 10^{-11}$\\
  \hline
  \end{tabular}
  }
  \caption{Comparing the performance of cuDSS with CHOLMOD.
    The matrix $K_\gamma$ is symmetric positive definite, with
    a size $n = 674,562$. The matrix is extremely sparse, with only $7,342,680$ non-zero entries ($0.002$\%).
    \label{tab:linsol:time}
    (A100 GPU)
  }
\end{table}

\subsubsection{Tuning the Golub \& Greif strategy}
\label{sec:num:tuninghykkt}
In Figure~\ref{fig:hybrid:gamma} we depict the evolution of the number
of \CG iterations and relative accuracy as we increase the parameter $\gamma$
from $10^4$ to $10^8$ in HyKKT.

On the algorithmic side, we observe that the higher the regularization $\gamma$,
the faster the \CG algorithm: we decrease the total number of iterations
spent in \CG by a factor of 10. However, we have to pay a price in term
of accuracy: for $\gamma > 10^8$ the solution returned by the linear solver
is not accurate enough and the IPM algorithm has to proceed to more
primal-dual regularization, leading to an increase in the total number of iterations.

On the numerical side, the table in Figure~\ref{fig:hybrid:gamma} compares
the time spent in the IPM solver on the CPU (using CHOLMOD) and on the GPU
(using the solver {\tt cuDSS}). Overall {\tt cuDSS} is
faster than CHOLMOD, leading to a 4x-8x speed-up in the total IPM solution time.
We note also that the assembly of the condensed matrix $K_\gamma$ parallelizes well
on the GPU, with a reduction in the assembly time from $\approx 8s$ on the CPU to $\approx 0.2s$ on the GPU.

\begin{figure}[!ht]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{|r|rrrr >{\bfseries}r|rrrr >{\bfseries}r|}
  \hline
  & \multicolumn{5}{c|}{\bf CHOLMOD (CPU)} & \multicolumn{5}{c|}{\bf cuDSS-\ldlt (CUDA)} \\
  \hline
  $\gamma$ & \# it & cond. (s) & \CG (s) & linsol (s) & IPM (s) & \# it & cond. (s) & \CG (s) & linsol (s) & IPM (s) \\
  \hline
  $10^4$ & 96 & 8.15 & 463.86 & 536.83 & 575.06 & 96 & 0.17 & 113.27 & 114.52 & 124.00 \\
  $10^5$ & 96 & 8.33 & 163.35 & 235.61 & 273.36 & 96 & 0.17 & 53.37 & 54.62 & 64.39 \\
  $10^6$ & 96 & 8.22 & 68.69 & 139.86 & 177.24 & 96 & 0.17 & 14.53 & 15.78 & 25.39 \\
  $10^7$ & 96 & 8.24 & 35.12 & 107.17 & 144.78 & 96 & 0.17 & 7.95 & 9.20 & 18.41 \\
  $10^8$ & 96 & 7.89 & 21.68 & 93.85 & 131.33 & 96 & 0.17 & 5.36 & 6.62 & 15.90 \\
  \hline
  \end{tabular}
  }
  \includegraphics[width=\textwidth]{../figures/hybrid-gamma.pdf}
  \caption{
    Above: Decomposition of IPM solution time across
    (a) condensation time (cond.), (b) \CG time, (c) total time
    spent in the linear solver (linsol.) and (d) total time spent in
    IPM solver (IPM).
    Below: Impact of $\gamma$ on the total number of \CG iterations
    and the norm of the relative residual at each IPM iteration.
    The peak observed in the norm of the relative residual corresponds
    to the primal-dual regularization performed inside the IPM algorithm,
    applied when the matrix $K_\gamma$ is not positive definite.
    \label{fig:hybrid:gamma}
    (A30 GPU)
  }
\end{figure}


\subsubsection{Tuning the equality relaxation strategy}
We now analyze the numerical performance of LiftedKKT (\S\ref{sec:kkt:sckkt}).
The method solves the KKT system~\eqref{eq:liftedkkt} using a direct solver.
The parameter $\tau$ used in the equality relaxation~\eqref{eq:problemrelaxation}
is set equal to the IPM tolerance $\varepsilon_{tol}$ (in practice, it does not
make sense to set a parameter $\tau$ below IPM tolerance as the
inequality constraints are satisfied only up to a tolerance $\pm \varepsilon_{tol}$
in IPM).

We compare in Table~\ref{tab:sckkt:performance} the performance obtained by LiftedKKT
as we decrease the IPM tolerance $\varepsilon_{tol}$.
We display both the runtimes on the CPU (using LDLFactorizations) and on the GPU (using {\tt cuDSS}-\ldlt).
The slacks associated with the relaxed equality constraints are converging to a value below $2 \tau$,
leading to highly ill-conditioned terms in the diagonal matrices $\Sigma_s$.
As a consequence, the conditioning of the matrix $K_\tau$ in \eqref{eq:liftedkkt} can increase
above $10^{18}$, leading to a nearly singular linear system.
We observe {\tt cuDSS}-\ldlt is more stable: the factorization
succeeds, and the loss of accuracy caused by the ill-conditioning is tamed by the multiple
Richardson iterations that reduces the relative accuracy in the residual down to an acceptable level.
As a result, {\tt cuDSS} can solve
the problem to optimality in $\approx 20s$, a time comparable with HyKKT (see Figure~\ref{fig:hybrid:gamma}).

\begin{table}[!ht]
  \centering
  \resizebox{.7\textwidth}{!}{
  \begin{tabular}{|l|rr|rr|r|}
    \hline
    & \multicolumn{2}{c|}{\bf CHOLMOD (CPU)} & \multicolumn{2}{c|}{\bf cuDSS-\ldlt (CUDA)}& \\
    \hline
    $\varepsilon_{tol}$ & \#it & time (s) & \#it & time (s) & accuracy \\
    \hline
    $10^{-4}$ &220& 358.8& 114 & 19.9& $1.2 \times 10^{-2}$\\
    $10^{-5}$ &120&  683.1& 113 & 30.4&$1.2 \times 10^{-3}$ \\
    $10^{-6}$ &109&  328.7& 109 & 25.0&$1.2 \times 10^{-4}$  \\
    $10^{-7}$ &108&  272.9& 104 & 20.1&$1.2 \times 10^{-5}$ \\
    $10^{-8}$ & - &  - & 105 & 20.3&$1.2 \times 10^{-6}$  \\
    \hline
  \end{tabular}
  }
  \label{tab:sckkt:performance}
  \caption{Performance of the equality-relaxation
    strategy as we decrease the IPM tolerance $\varepsilon_{tol}$.
    The table displays the wall time on the CPU (using CHOLMOD)
    and on the GPU (using cuDSS-\ldlt). (A30 GPU)
  }
\end{table}

\subsubsection{Breakdown of the time spent in one IPM iteration}
We decompose the time spent in a single
IPM iteration for all the available KKT solvers (HSL MA27, LiftedKKT, and HyKKT).
When solving the KKT system, the time can be decomposed into: (1) assembling the
KKT system, (2) factorizing the KKT system, and (3) computing the descent direction with a triangular solve.
As depicted in Figure~\ref{fig:timebreakdown}, we observe
that constructing the KKT system represents only a fraction of the computation time, compared
to the factorization and the triangular solve. Using {\tt cuDSS}-\ldlt, we observe speedups of
30x and 15x in the factorization compared to MA27 and CHOLMOD running on the CPU.
Once the KKT system is factorized, computing the descent direction with LiftedKKT is faster than with HyKKT
(0.04s compared to 0.13s) as HyKKT has to run a \CG algorithm to solve the Schur complement
system~\eqref{eq:kkt:schurcomplhykkt}.

\begin{figure}[!ht]
  \centering
  \resizebox{.7\textwidth}{!}{
    \begin{tabular}{|l|rrrr|}
      \hline
       & build (s) & factorize (s) & backsolve (s) & accuracy \\
      \hline
      hsl        & $3.15\times 10^{-2}$&$1.22 \times 10^{-0} $&$3.58\times 10^{-1}$&$5.52\times 10^{-7}$\\
    sckkt-cpu  & $8.71\times 10^{-2}$&$6.08\times 10^{-1}$&$2.32\times 10^{-1}$&$3.73\times 10^{-9}$\\
    hckkt-cpu  & $7.97\times 10^{-2}$&$6.02\times 10^{-1}$&$7.30\times 10^{-1}$&$3.38\times 10^{-3}$\\
    sckkt-cuda & $2.09\times 10^{-3}$&$4.37\times 10^{-2}$&$3.53\times 10^{-2}$&$4.86\times 10^{-9}$\\
    hckkt-cuda & $1.86\times 10^{-3}$&$3.38\times 10^{-2}$&$1.35\times 10^{-1}$&$3.91\times 10^{-3}$\\
      \hline
    \end{tabular}
  }
  \includegraphics[width=.7\textwidth]{../figures/breakdown.pdf}
  \caption{Breakdown of the time spent in one IPM iteration
    for different linear solvers, when solving {\tt 78484epigrids} (A30 GPU)
  \label{fig:timebreakdown}}
\end{figure}



\subsection{Benchmark on OPF instances}
\label{sec:num:opf}
We run a benchmark on difficult OPF instances taken
from the PGLIB benchmark~\cite{babaeinejadsarookolaee2019power}.
We compare our two condensed-space methods with HSL MA27 running
on the CPU. The results are displayed in Table~\ref{tab:opf:benchmark},
for an IPM tolerance set to $10^{-6}$. The table displays the
time spent in the initialization, the time spent in the linear solver and the total
solving time.
We complement the table with a Dolan \& Moré performance profile~\cite{dolan2002benchmarking} displayed
in Figure~\ref{fig:opf:pprof}.

Overall, the performance of HSL MA27 on the CPU is consistent with what was reported
in \cite{babaeinejadsarookolaee2019power}: We observe that HSL MA57 is slower
than HSL MA27, as the OPF instances are super-sparse.
Hence, the block elimination algorithm implemented in HSL MA57 is not beneficial there
\footnote{Personal communication with Iain Duff.}.

On the GPU, LiftedKKT is faster than HyKKT on small and medium instances: indeed, the algorithm
does not have to run a \CG algorithm at each IPM iteration, limiting the number
of backsolves to the number of Richardson iterations in the iterative refinement
algorithm.
Regarding HyKKT, we set $\gamma = 10^7$ following the analysis in \S\ref{sec:num:tuninghykkt}.
The algorithm is significantly faster than HSL MA27, only specific
instances when $\gamma$ is not high enough to reduce significantly the number
of \CG iterations. For example, compared
to the other instances, the solution of {\tt 8387\_pegase} is impaired
by a larger total number of \CG iterations leading to a 4x slowdown compared to LiftedKKT.
Nevertheless, the performance of HyKKT is better on the largest instances,
with almost an 8x speed-up compared to the reference HSL MA27.

\begin{table}[!ht]
  \centering
  \resizebox{\textwidth}{!}{
    \begin{tabular}{|l|rrr >{\bfseries}r|rrr >{\bfseries}r|rrr >{\bfseries}r|}
      \hline
      & \multicolumn{4}{c|}{\bf HSL MA27} &
      \multicolumn{4}{c|}{\bf LiftedKKT+cuDSS} &
      \multicolumn{4}{c|}{\bf HyKKT+cuDSS} \\
      \hline
      Case & it & init & lin & total & it & init & lin & total & it & init & lin & total \\
      \hline
        89\_pegase & 32 & 0.00 & 0.02 & 0.03 & 31 & 0.02 & 0.12 & 0.23 & 32 & 0.02 & 0.07 & 0.19 \\
        179\_goc & 45 & 0.00 & 0.03 & 0.05 & 38 & 0.03 & 0.19 & 0.32 & 45 & 0.03 & 0.07 & 0.23 \\
        500\_goc & 39 & 0.01 & 0.10 & 0.14 & 39 & 0.04 & 0.10 & 0.24 & 39 & 0.04 & 0.08 & 0.24 \\
        793\_goc & 35 & 0.01 & 0.12 & 0.18 & 47 & 0.05 & 0.27 & 0.46 & 35 & 0.04 & 0.10 & 0.27 \\
        1354\_pegase & 49 & 0.02 & 0.35 & 0.52 & 71 & 0.08 & 0.66 & 1.05 & 49 & 0.08 & 0.18 & 0.47 \\
              \hline
        2000\_goc & 42 & 0.03 & 0.66 & 0.93 & 48 & 0.12 & 0.43 & 0.78 & 42 & 0.14 & 0.15 & 0.48 \\
        2312\_goc & 43 & 0.02 & 0.59 & 0.82 & 63 & 0.12 & 0.45 & 0.90 & 43 & 0.40 & 0.23 & 0.82 \\
        2742\_goc & 125 & 0.04 & 3.76 & 7.31 & 168 & 0.15 & 2.38 & 3.91 & - & - & - & - \\
        2869\_pegase & 55 & 0.04 & 1.09 & 1.52 & 57 & 0.15 & 0.33 & 0.80 & 55 & 0.15 & 0.30 & 0.71 \\
        3022\_goc & 55 & 0.03 & 0.98 & 1.39 & 53 & 0.13 & 0.30 & 0.74 & 55 & 0.14 & 0.26 & 0.66 \\
              \hline
        3970\_goc & 48 & 0.05 & 1.95 & 2.53 & 48 & 0.22 & 0.49 & 1.00 & 48 & 0.21 & 0.27 & 0.77 \\
        4020\_goc & 59 & 0.06 & 3.90 & 4.60 & 64 & 0.22 & 0.77 & 1.60 & 59 & 0.23 & 0.46 & 1.09 \\
        4601\_goc & 71 & 0.09 & 3.09 & 4.16 & 67 & 0.21 & 0.60 & 1.22 & 71 & 0.23 & 0.44 & 1.12 \\
        4619\_goc & 49 & 0.07 & 3.21 & 3.91 & 52 & 0.26 & 0.70 & 1.34 & 49 & 0.28 & 0.37 & 1.00 \\
        4837\_goc & 59 & 0.08 & 2.49 & 3.33 & 56 & 0.22 & 0.44 & 1.10 & 59 & 0.24 & 0.43 & 1.03 \\
              \hline
        4917\_goc & 63 & 0.07 & 1.97 & 2.72 & 69 & 0.20 & 0.79 & 1.51 & 63 & 0.21 & 0.40 & 0.96 \\
        5658\_epigrids & 51 & 0.31 & 2.80 & 3.86 & 48 & 0.27 & 0.55 & 1.23 & 51 & 0.29 & 0.42 & 1.06 \\
        7336\_epigrids & 50 & 0.13 & 3.60 & 4.91 & 54 & 0.36 & 1.12 & 2.08 & 50 & 0.35 & 0.46 & 1.20 \\
        8387\_pegase & 74 & 0.14 & 5.31 & 7.62 & 76 & 0.46 & 0.70 & 2.29 & 75 & 0.47 & 11.17 & 12.34 \\
        9241\_pegase & 74 & 0.15 & 6.11 & 8.60 & 107 & 0.50 & 1.18 & 3.42 & 71 & 0.51 & 1.46 & 2.72 \\
              \hline
        9591\_goc & 67 & 0.20 & 11.14 & 13.37 & 75 & 0.49 & 1.99 & 3.74 & 67 & 0.50 & 1.02 & 2.24 \\
        10000\_goc & 82 & 0.15 & 6.00 & 8.16 & 64 & 0.39 & 1.12 & 2.21 & 82 & 0.38 & 0.96 & 1.97 \\
        10192\_epigrids & 54 & 0.41 & 7.79 & 10.08 & 68 & 0.57 & 1.54 & 3.14 & 54 & 0.55 & 0.93 & 2.07 \\
        10480\_goc & 71 & 0.24 & 12.04 & 14.74 & 66 & 0.60 & 1.57 & 3.51 & 71 & 0.60 & 1.45 & 2.91 \\
        13659\_pegase & 63 & 0.45 & 7.21 & 10.14 & 68 & 0.69 & 1.09 & 3.03 & 62 & 0.68 & 1.37 & 2.77 \\
              \hline
        19402\_goc & 69 & 0.63 & 31.71 & 36.92 & - & - & - & - & 69 & 1.55 & 2.52 & 5.38 \\
        20758\_epigrids & 51 & 0.63 & 14.27 & 18.21 & 52 & 1.07 & 1.48 & 4.18 & 51 & 1.13 & 2.17 & 4.13 \\
        30000\_goc & 183 & 0.65 & 63.02 & 75.95 & 139 & 0.98 & 4.50 & 8.46 & 228 & 0.99 & 7.73 & 13.71 \\
        78484\_epigrids & 102 & 2.57 & 179.29 & 207.79 & 110 & 4.84 & 9.90 & 26.01 & 102 & 5.01 & 15.27 & 25.70 \\
      \hline
    \end{tabular}
  }
  \caption{OPF benchmark, solved with a tolerance {\tt tol=1e-6}. (A30 GPU) \label{tab:opf:benchmark}}
\end{table}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.6\textwidth]{../figures/pprof-cuda.pdf}
  \caption{Performance profile for the PGLIB OPF benchmark, solved
    with a tolerance {\tt tol=1e-6}.
  \label{fig:opf:pprof}}
\end{figure}


\subsection{Benchmark on COPS instances}
\label{sec:num:cops}
We have observed in the previous section that both LiftedKKT
and HyKKT outperforms HSL MA27 when running on the GPU.
However, the OPF instances are specific nonlinear problems.
For that reason, we complement our analysis by looking
at the performance of LiftedKKT and HyKKT on the COPS benchmark,
which gathers generic nonlinear programs~\cite{dolan2004benchmarking}.
We look at the performance we get on the particular COPS instances used in
the Mittelmann benchmark, widely used to benchmark nonlinear optimization
solvers~\cite{mittelmann2002benchmark}.
The results of the COPS benchmark are displayed in Table~\ref{tab:cops:benchmark}.
HSL MA57 gives better results than HSL MA27 on the COPS benchmark, and we have
decided to use HSL MA57 as the reference running on the CPU.

As expected, the results are different than on the OPF benchmark.
We observe that LiftedKKT+cuDSS and HyKKT+cuDSS outperform HSL MA57 on the dense instance {\tt elec}
(20x speed-up) and {\tt bearing}  --- an instance whose sparsity pattern
is similar to the OPF. In the other instances, LiftedKKT and HyKKT
on par with HSL MA57 and sometimes even slightly slower ({\tt rocket} and {\tt pinene}).

To illustrate the heterogeneity of the COPS instances compared to the
previous OPF problems, we display in Figure~\ref{fig:cops:nnz} the sparsity pattern of the
condensed matrices $K_\gamma$ \eqref{eq:kkt:hykkt} for one OPF instance and for multiple
COPS instances. We observe that some instances ({\tt bearing}) have a sparsity pattern
similar to the OPF instance on the left, whereas some are fully dense ({\tt elec}).
On the opposite, the optimal control instances ({\tt marine}, {\tt pinene}) are
highly sparse and have highly structured Cholesky lower triangular factors.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.9\textwidth]{../figures/sparsity_pattern.png}
  \caption{Sparsity patterns for one OPF instance and for various
    COPS problems. The first row displays the sparsity pattern
    of $K_\gamma$, after AMD reordering. The second row displays
    the sparsity pattern of the triangular factor computed by CHOLMOD.
    \label{fig:cops:nnz}
  }
\end{figure}

\begin{table}[!ht]
  \centering
  \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{|l|rrrr|}
      \hline
      & $n$ & $m$ & $nnzj$ & $nnzh$ \\
      \hline
      bearing\_400 & 161604 & 1608 & 1608 & 1128816 \\
      camshape\_6400 & 6400 & 19204 & 63999 & 57589 \\
      elec\_400 & 1200 & 400 & 1200 & 6225600 \\
      gasoil\_3200 & 83203 & 83200 & 435190 & 155002 \\
      marine\_1600 & 51215 & 51192 & 185576 & 105800 \\
      pinene\_3200 & 160005 & 160000 & 751980 & 288880 \\
      robot\_1600 & 14411 & 9613 & 70414 & 155200 \\
      rocket\_12800 & 51205 & 38404 & 345604 & 1318400 \\
      steering\_12800 & 64006 & 51208 & 281608 & 307200 \\
      \hline
      bearing\_800 & 643204 & 3208 & 3208 & 4497616 \\
      camshape\_12800 & 12800 & 38404 & 127999 & 115189 \\
      elec\_800 & 2400 & 800 & 2400 & 24931200 \\
      gasoil\_12800 & 332803 & 332800 & 1740790 & 615802 \\
      marine\_12800 & 409615 & 409592 & 1484776 & 845000 \\
      pinene\_12800 & 640005 & 640000 & 3007980 & 1152880 \\
      robot\_12800 & 115211 & 76813 & 563214 & 1241600 \\
      rocket\_51200 & 204805 & 153604 & 1382404 & 5273600 \\
      steering\_51200 & 256006 & 204808 & 1126408 & 1228800 \\
      \hline
    \end{tabular}
  }
    \caption{Selected instances in the COPS benchmark}
  \end{table}


\begin{table}[!ht]
  \centering
  \resizebox{\textwidth}{!}{
    \begin{tabular}{|l|rrr >{\bfseries}r|rrr >{\bfseries}r|rrr >{\bfseries}r|}
    \hline
    & \multicolumn{4}{c|}{\bf HSL MA57} &
      \multicolumn{4}{c|}{\bf LiftedKKT+cuDSS} &
      \multicolumn{4}{c|}{\bf HyKKT+cuDSS} \\
    \hline
    & it & init & lin & total & it & init & lin & total & it & init & lin & total \\
    \hline
    bearing\_400 & 17 & 0.14 & 3.42 & 4.10 & 14 & 0.62 & 0.08 & 0.93 & 14 & 0.61 & 0.94 & 1.82 \\
    camshape\_6400 & 38 & 0.02 & 0.18 & 0.29 & 35 & 0.04 & 0.03 & 0.15 & 38 & 0.04 & 0.04 & 0.20 \\
    elec\_400 & 185 & 0.54 & 24.64 & 33.02 & 114 & 0.37 & 3.65 & 6.17 & 100 & 0.43 & 0.57 & 3.43 \\
    gasoil\_3200 & 37 & 0.36 & 4.81 & 5.81 & 22 & 0.46 & 0.43 & 1.56 & 20 & 0.46 & 0.32 & 1.33 \\
    marine\_1600 & 13 & 0.05 & 0.41 & 0.50 & 26 & 0.29 & 0.51 & 1.08 & 13 & 0.32 & 0.14 & 0.60 \\
    pinene\_3200 & 12 & 0.11 & 1.32 & 1.60 & 21 & 0.75 & 0.21 & 1.50 & 12 & 0.78 & 1.52 & 2.65 \\
    robot\_1600 & 34 & 0.04 & 0.33 & 0.45 & 35 & 0.16 & 0.11 & 0.73 & 34 & 0.16 & 0.10 & 0.81 \\
    rocket\_12800 & 23 & 0.12 & 1.73 & 2.16 & 37 & 0.28 & 0.12 & 2.11 & 24 & 0.23 & 2.17 & 3.62 \\
    steering\_12800 & 19 & 0.25 & 1.49 & 1.93 & 17 & 0.40 & 0.20 & 1.62 & 20 & 0.38 & 0.12 & 1.91 \\
    \hline
    bearing\_800 & 13 & 0.94 & 14.59 & 16.86 & 14 & 2.65 & 0.24 & 3.60 & 12 & 3.04 & 2.63 & 6.50 \\
    camshape\_12800 & 34 & 0.02 & 0.34 & 0.54 & 33 & 0.04 & 0.02 & 0.14 & 34 & 0.05 & 0.03 & 0.17 \\
    elec\_800 & 354 & 2.36 & 337.41 & 409.57 & 354 & 1.60 & 27.21 & 59.54 & 189 & 2.00 & 2.46 & 19.01 \\
    gasoil\_12800 & 20 & 1.78 & 11.15 & 13.65 & 17 & 1.87 & 1.31 & 5.16 & 22 & 1.90 & 1.89 & 6.24 \\
    marine\_12800 & 11 & 0.36 & 3.51 & 4.46 & 148 & 2.28 & 38.01 & 55.87 & 11 & 2.35 & 1.00 & 4.02 \\
    pinene\_12800 & 10 & 0.48 & 7.15 & 8.45 & 21 & 3.51 & 1.29 & 7.11 & 12 & 3.99 & 5.29 & 10.67 \\
    robot\_12800 & 35 & 0.54 & 4.63 & 5.91 & 33 & 1.05 & 0.37 & 4.48 & 35 & 1.06 & 0.41 & 4.92 \\
    rocket\_51200 & 31 & 1.21 & 6.24 & 9.51 & 38 & 0.89 & 0.47 & 9.63 & 30 & 0.90 & 4.09 & 11.99 \\
    steering\_51200 & 27 & 1.40 & 9.74 & 13.00 & 15 & 1.73 & 0.23 & 5.76 & 24 & 1.73 & 0.85 & 10.81 \\
    \hline
    \end{tabular}
  }
  \caption{COPS benchmark , solved with a tolerance {\tt tol=1e-6}\label{tab:cops:benchmark} (A30 GPU)}
\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
